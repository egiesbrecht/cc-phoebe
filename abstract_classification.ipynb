{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "import datasets\n",
    "from datasets import DatasetDict, load_dataset, Dataset\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5Tokenizer,\n",
    "    T5ForSequenceClassification,\n",
    "    T5Config,\n",
    "    BertTokenizer, \n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from load_set import *\n",
    "from epoch_stats import EpochStats#, print_stat_tuples\n",
    "import model_training\n",
    "from model_training import (\n",
    "    BatchBuffer, \n",
    "    train_bern_model, \n",
    "    mask_tokens, \n",
    "    preprocess_with_given_labels, \n",
    "    num_parameters, \n",
    "    num_trainable_parameters, \n",
    "    preprocess_for_causallm, \n",
    "    preprocess_for_multiple_choice,\n",
    "    preprocess_for_seq2seq_swag,\n",
    "    preprocess_with_given_labels_train_test_wrap\n",
    ")\n",
    "import coin_i3C_modeling as ci3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1\n",
    "CHECKPOINT_PATH = None#\"hyp_cls/BERT-base_all-labels/\" \n",
    "SHUFFLE_CUSTOM_DATALOADER = True\n",
    "LEARNING_RATE = 1e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30_522\n",
    "MAX_POSITION_EMBEDDINGS = 512#24**2\n",
    "HIDDEN_SIZE = 1024\n",
    "IS_HF_MODEL = False\n",
    "GENERIC_OUTPUT_CLASS = True\n",
    "DOC_PAD_TOKENS = False\n",
    "\n",
    "NUM_LABELS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    tokenizer = transformers.DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    model = transformers.DebertaForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/deberta-base\",\n",
    "        num_labels=NUM_LABELS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num layers: 4\n",
      "gamma schema: [0.96875, 0.9875984191894531, 0.995078444480896, 0.998046875]\n",
      "layer 0 num experts: 1\n",
      "layer 1 num experts: 1\n",
      "layer 2 num experts: 1\n",
      "layer 3 num experts: 1\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    rffn_base_model_path = \"tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x2_1dec-none_no-revert_chunkwise_group-exp_congen-head_B10_multi-query-2_switch-ii_mpe576_no-cross-att/\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    #tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = ci3C.COINForSequenceClassification(\n",
    "        config=ci3C.COINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "            forward_method=\"parallel\",\n",
    "            apply_decay=True,\n",
    "            num_decay_parts=1,\n",
    "            hidden_retention_act=\"relu\",\n",
    "            hidden_pos_offset=False,\n",
    "            rope_dim=32,\n",
    "            num_query_heads=1,\n",
    "\n",
    "            decoder_output=\"none\",\n",
    "            revert_decoder=False,\n",
    "            decoder_schema=[0] * 4,\n",
    "            cross_encoder_schema=[0] * 4,\n",
    "            experts_schema=None,#[2, 2],\n",
    "            block_io_schema=None,#[[1024, 1024*4, 1024]],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all labels: datasets/wordpiece_abstracts_train_all_labels.csv  30 labels\n",
    "# 2 labels: datasets/wordpiece_abstracts_train_side_label_1.csv  20 labels\n",
    "# main label onyl: datasets/wordpiece_abstracts_train.csv        10 labels\n",
    "\n",
    "DS_TRAIN_PATH = \"datasets/wordpiece_abstracts_train_all_labels.csv\"\n",
    "DS_TEST_PATH = \"datasets/wordpiece_abstracts_test_all_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files\n",
      "    datasets/wordpiece_abstracts_train_all_labels.csv\n",
      "loading files\n",
      "    datasets/wordpiece_abstracts_test_all_labels.csv\n",
      "{'a0': 0, 'a1': 1, 'a2': 2, 'a3': 3, 'a4': 4, 'a5': 5, 'a6': 6, 'a7': 7, 'a8': 8, 'a9': 9, 'b0': 10, 'b1': 11, 'b2': 12, 'b3': 13, 'b4': 14, 'b5': 15, 'b6': 16, 'b7': 17, 'b8': 18, 'b9': 19, 'c0': 20, 'c1': 21, 'c2': 22, 'c3': 23, 'c4': 24, 'c5': 25, 'c6': 26, 'c7': 27, 'c8': 28, 'c9': 29}\n",
      "{0: 'a0', 1: 'a1', 2: 'a2', 3: 'a3', 4: 'a4', 5: 'a5', 6: 'a6', 7: 'a7', 8: 'a8', 9: 'a9', 10: 'b0', 11: 'b1', 12: 'b2', 13: 'b3', 14: 'b4', 15: 'b5', 16: 'b6', 17: 'b7', 18: 'b8', 19: 'b9', 20: 'c0', 21: 'c1', 22: 'c2', 23: 'c3', 24: 'c4', 25: 'c5', 26: 'c6', 27: 'c7', 28: 'c8', 29: 'c9'}\n",
      "['a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": load_set([DS_TRAIN_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"]),\n",
    "    \"test\": load_set([DS_TEST_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"]),\n",
    "})\n",
    "labels = [label for label in dataset['train'].features.keys() if label not in [\"text\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 862/862 [00:01<00:00, 663.62 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 92/92 [00:00<00:00, 131.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = preprocess_with_given_labels_train_test_wrap(dataset, tokenizer, labels, label2id, MAX_POSITION_EMBEDDINGS, False, remove_columns=dataset[\"train\"].column_names, \n",
    "                                                               default_teacher_forcing=False, teacher_forcing_prefix=None, doc_pad_tokens=DOC_PAD_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n"
     ]
    }
   ],
   "source": [
    "batch_schema = list(encoded_dataset[\"train\"].features.keys())\n",
    "print(batch_schema)\n",
    "train_dataloader = BatchBuffer(encoded_dataset[\"train\"], TRAIN_BATCH_SIZE)\n",
    "if SHUFFLE_CUSTOM_DATALOADER:\n",
    "    train_dataloader.shuffle()\n",
    "test_dataloader = BatchBuffer(encoded_dataset[\"test\"], TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_dataloader) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "warmup_steps = math.ceil(total_steps * 0.05)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECKPOINT_PATH is not None:\n",
    "    try:\n",
    "        os.mkdir(CHECKPOINT_PATH)\n",
    "    except OSError as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_HF_MODEL:\n",
    "    forward_args = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"]\n",
    "else:\n",
    "    forward_args = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:02, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:03, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:03, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:04, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:04, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:05, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:06, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:06, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:07, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:07, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:08, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:08, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:09, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:09, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:10, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:10, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:11, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:11, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:12, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:12, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:13, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:14, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:14, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:15, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:15, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:16, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:16, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:17, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:17, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:18, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:18, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:19, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:19, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:20, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:21, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:21, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:22, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:22, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:23, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:23, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:24, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:24, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:25, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:25, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:26, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:26, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:27, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.33310673779131644\n",
      "    accuracy: 0.07308584686774942\n",
      "    per class f1: 0.4394057830542758\n",
      "    f1_micro: 0.4394057830542758\n",
      "    f1_macro: 0.8759474091260583\n",
      "    recall_micro: 0.4394057830542758\n",
      "    recall_macro: 0.937973704563028\n",
      "    precision_micro: 0.4394057830542758\n",
      "    precision_macro: 0.937973704563028\n",
      "  Training epoch took: 0:00:27\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.21977134413369323\n",
      "    accuracy: 0.21739130434782608\n",
      "    per class f1: 0.6316511387163561\n",
      "    f1_micro: 0.6316511387163561\n",
      "    f1_macro: 0.9159420289855069\n",
      "    recall_micro: 0.6316511387163561\n",
      "    recall_macro: 0.9579710144927539\n",
      "    precision_micro: 0.6316511387163561\n",
      "    precision_macro: 0.9579710144927539\n",
      "  Testing took: 0:00:02\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:02, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:03, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:03, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:04, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:04, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:05, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:05, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:06, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:06, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:07, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:07, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:08, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:08, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:09, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:10, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:10, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:11, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:11, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:12, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:12, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:13, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:13, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:14, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:14, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:15, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:15, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:16, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:16, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:17, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:17, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:18, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:18, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:19, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:19, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:20, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:20, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:21, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:21, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:22, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:23, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:23, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:24, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:24, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:25, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:25, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:26, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:26, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.1800577932160581\n",
      "    accuracy: 0.3433874709976798\n",
      "    per class f1: 0.7150599883488515\n",
      "    f1_micro: 0.7150599883488515\n",
      "    f1_macro: 0.9353441608661965\n",
      "    recall_micro: 0.7150599883488515\n",
      "    recall_macro: 0.9676720804330998\n",
      "    precision_micro: 0.7150599883488515\n",
      "    precision_macro: 0.9676720804330998\n",
      "  Training epoch took: 0:00:27\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.19254684739786646\n",
      "    accuracy: 0.21739130434782608\n",
      "    per class f1: 0.6701604554865422\n",
      "    f1_micro: 0.6701604554865422\n",
      "    f1_macro: 0.9246376811594196\n",
      "    recall_micro: 0.6701604554865422\n",
      "    recall_macro: 0.9623188405797106\n",
      "    precision_micro: 0.6701604554865422\n",
      "    precision_macro: 0.9623188405797106\n",
      "  Testing took: 0:00:02\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:02, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:03, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:03, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:04, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:04, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:05, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:05, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:06, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:06, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:07, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:07, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:08, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:08, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:09, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:09, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:10, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:10, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:11, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:11, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:12, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:12, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:13, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:13, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:14, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:15, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:15, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:16, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:16, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:17, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:17, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:18, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:18, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:19, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:19, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:20, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:20, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:21, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:21, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:22, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:22, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:23, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:24, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:24, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:25, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:25, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:26, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:26, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.12746461095794032\n",
      "    accuracy: 0.5638051044083526\n",
      "    per class f1: 0.8461067000800179\n",
      "    f1_micro: 0.8461067000800179\n",
      "    f1_macro: 0.9641144624903286\n",
      "    recall_micro: 0.8461067000800179\n",
      "    recall_macro: 0.9820572312451656\n",
      "    precision_micro: 0.8461067000800179\n",
      "    precision_macro: 0.9820572312451656\n",
      "  Training epoch took: 0:00:27\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.1925408593378961\n",
      "    accuracy: 0.22826086956521738\n",
      "    per class f1: 0.6681677018633538\n",
      "    f1_micro: 0.6681677018633538\n",
      "    f1_macro: 0.9246376811594198\n",
      "    recall_micro: 0.6681677018633538\n",
      "    recall_macro: 0.9623188405797106\n",
      "    precision_micro: 0.6681677018633538\n",
      "    precision_macro: 0.9623188405797106\n",
      "  Testing took: 0:00:02\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:02, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:03, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:03, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:04, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:04, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:05, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:05, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:06, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:06, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:07, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:07, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:08, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:08, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:09, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:10, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:10, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:11, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:11, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:12, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:12, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:13, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:13, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:14, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:14, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:15, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:15, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:16, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:16, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:17, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:17, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:18, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:19, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:19, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:20, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:20, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:21, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:21, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:22, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:22, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:23, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:23, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:24, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:24, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:25, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:25, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:26, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:26, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.08840244955852143\n",
      "    accuracy: 0.7679814385150812\n",
      "    per class f1: 0.9264127854615088\n",
      "    f1_micro: 0.9264127854615088\n",
      "    f1_macro: 0.982366589327144\n",
      "    recall_micro: 0.9264127854615088\n",
      "    recall_macro: 0.9911832946635728\n",
      "    precision_micro: 0.9264127854615088\n",
      "    precision_macro: 0.9911832946635728\n",
      "  Training epoch took: 0:00:27\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.20029831217075494\n",
      "    accuracy: 0.20652173913043478\n",
      "    per class f1: 0.6361024844720496\n",
      "    f1_micro: 0.6361024844720496\n",
      "    f1_macro: 0.9173913043478253\n",
      "    recall_micro: 0.6361024844720496\n",
      "    recall_macro: 0.9586956521739134\n",
      "    precision_micro: 0.6361024844720496\n",
      "    precision_macro: 0.9586956521739134\n",
      "  Testing took: 0:00:02\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:02, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:03, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:03, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:04, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:04, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:05, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:06, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:06, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:07, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:07, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:08, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:08, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:09, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:10, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:10, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:11, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:11, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:12, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:12, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:13, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:13, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:14, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:15, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:15, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:16, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:16, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:17, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:17, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:18, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:19, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:19, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:20, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:20, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:21, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:21, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:22, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:23, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:23, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:24, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:24, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:25, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:25, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:26, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:27, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:27, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:28, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:28, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.061825086596570995\n",
      "    accuracy: 0.8979118329466357\n",
      "    per class f1: 0.9716941958531284\n",
      "    f1_micro: 0.9716941958531284\n",
      "    f1_macro: 0.9927300850734722\n",
      "    recall_micro: 0.9716941958531284\n",
      "    recall_macro: 0.9963650425367356\n",
      "    precision_micro: 0.9716941958531284\n",
      "    precision_macro: 0.9963650425367356\n",
      "  Training epoch took: 0:00:29\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.21176629357100668\n",
      "    accuracy: 0.20652173913043478\n",
      "    per class f1: 0.6420807453416147\n",
      "    f1_micro: 0.6420807453416147\n",
      "    f1_macro: 0.9181159420289847\n",
      "    recall_micro: 0.6420807453416147\n",
      "    recall_macro: 0.9590579710144932\n",
      "    precision_micro: 0.6420807453416147\n",
      "    precision_macro: 0.9590579710144932\n",
      "  Testing took: 0:00:02\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:02, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:03, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:03, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:04, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:05, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:05, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:06, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:06, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:07, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:07, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:08, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:09, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:09, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:10, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:10, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:11, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:11, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:12, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:13, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:13, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:14, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:14, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:15, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:15, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:16, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:16, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:17, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:18, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:18, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:19, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:19, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:20, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:20, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:21, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:22, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:22, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:23, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:23, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:24, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:24, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:25, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:26, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:26, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:27, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:27, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:28, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:28, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.043729746879538706\n",
      "    accuracy: 0.9547563805104409\n",
      "    per class f1: 0.9868973962361433\n",
      "    f1_micro: 0.9868973962361433\n",
      "    f1_macro: 0.996597061098221\n",
      "    recall_micro: 0.9868973962361433\n",
      "    recall_macro: 0.9982985305491104\n",
      "    precision_micro: 0.9868973962361433\n",
      "    precision_macro: 0.9982985305491104\n",
      "  Training epoch took: 0:00:29\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.22161579209521576\n",
      "    accuracy: 0.1956521739130435\n",
      "    per class f1: 0.6378105590062111\n",
      "    f1_micro: 0.6378105590062111\n",
      "    f1_macro: 0.9173913043478253\n",
      "    recall_micro: 0.6378105590062111\n",
      "    recall_macro: 0.9586956521739134\n",
      "    precision_micro: 0.6378105590062111\n",
      "    precision_macro: 0.9586956521739134\n",
      "  Testing took: 0:00:02\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:02, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:03, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:03, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:04, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:04, Remaining:  0:00:43.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stats = train_bern_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    device,\n",
    "    id2label=id2label,\n",
    "    batch_schema=batch_schema,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    print_status=True,\n",
    "    is_hf_model=IS_HF_MODEL,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    one_label_only=False,\n",
    "    generic_output_class=True,\n",
    "    calc_metrics=True,\n",
    "    per_class_f1=True,\n",
    "\n",
    "    forward_args=forward_args\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
