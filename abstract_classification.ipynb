{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "import datasets\n",
    "from datasets import DatasetDict, load_dataset, Dataset\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5Tokenizer,\n",
    "    T5ForSequenceClassification,\n",
    "    T5Config,\n",
    "    BertTokenizer, \n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from load_set import *\n",
    "from epoch_stats import EpochStats#, print_stat_tuples\n",
    "import model_training\n",
    "from model_training import (\n",
    "    BatchBuffer, \n",
    "    train_bern_model, \n",
    "    mask_tokens, \n",
    "    preprocess_with_given_labels, \n",
    "    num_parameters, \n",
    "    num_trainable_parameters, \n",
    "    preprocess_for_causallm, \n",
    "    preprocess_for_multiple_choice,\n",
    "    preprocess_for_seq2seq_swag,\n",
    "    preprocess_with_given_labels_train_test_wrap\n",
    ")\n",
    "import coin_i3C_modeling as ci3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1\n",
    "CHECKPOINT_PATH = \"hyp_cls/BiomedBERT-base_all-labels/\" \n",
    "SHUFFLE_CUSTOM_DATALOADER = True\n",
    "LEARNING_RATE = 1e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30_522\n",
    "MAX_POSITION_EMBEDDINGS = 512\n",
    "HIDDEN_SIZE = 1024\n",
    "IS_HF_MODEL = False\n",
    "GENERIC_OUTPUT_CLASS = True\n",
    "DOC_PAD_TOKENS = False\n",
    "\n",
    "NUM_LABELS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model (turn if condition to True) and run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    IS_HF_MODEL = True\n",
    "    tokenizer = transformers.DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    model = transformers.DebertaForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/deberta-base\",\n",
    "        num_labels=NUM_LABELS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"tmp_models/COIN-i3C_default_tokenizer/wordpiece_tokenizer/\")\n",
    "    model = ci3C.COINForSequenceClassification(\n",
    "        config=ci3C.COINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "            forward_method=\"parallel\",\n",
    "            apply_decay=True,\n",
    "            num_decay_parts=1,\n",
    "            hidden_retention_act=\"relu\",\n",
    "            hidden_pos_offset=False,\n",
    "            rope_dim=32,\n",
    "            num_query_heads=1,\n",
    "\n",
    "            decoder_output=\"none\",\n",
    "            revert_decoder=False,\n",
    "            decoder_schema=[1] * 8,\n",
    "            cross_encoder_schema=[0] * 8,\n",
    "            experts_schema=None,#[2, 2],\n",
    "            block_io_schema=None,#[[1024, 1024*4, 1024]],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    tokenizer = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "    model = transformers.RobertaForSequenceClassification.from_pretrained(\n",
    "        \"roberta-base\",\n",
    "        num_labels=NUM_LABELS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")\n",
    "    model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\",\n",
    "        num_labels=NUM_LABELS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=NUM_LABELS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all labels: datasets/wordpiece_abstracts_train_all_labels.csv  30 labels\n",
    "# 2 labels: datasets/wordpiece_abstracts_train_side_label_1.csv  20 labels\n",
    "# main label only: datasets/wordpiece_abstracts_train.csv        10 labels\n",
    "\n",
    "DS_TRAIN_PATH = \"datasets/wordpiece_abstracts_train_all_labels.csv\"\n",
    "DS_TEST_PATH = \"datasets/wordpiece_abstracts_test_all_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files\n",
      "    datasets/wordpiece_abstracts_train_all_labels.csv\n",
      "loading files\n",
      "    datasets/wordpiece_abstracts_test_all_labels.csv\n",
      "{'a0': 0, 'a1': 1, 'a2': 2, 'a3': 3, 'a4': 4, 'a5': 5, 'a6': 6, 'a7': 7, 'a8': 8, 'a9': 9, 'b0': 10, 'b1': 11, 'b2': 12, 'b3': 13, 'b4': 14, 'b5': 15, 'b6': 16, 'b7': 17, 'b8': 18, 'b9': 19, 'c0': 20, 'c1': 21, 'c2': 22, 'c3': 23, 'c4': 24, 'c5': 25, 'c6': 26, 'c7': 27, 'c8': 28, 'c9': 29}\n",
      "{0: 'a0', 1: 'a1', 2: 'a2', 3: 'a3', 4: 'a4', 5: 'a5', 6: 'a6', 7: 'a7', 8: 'a8', 9: 'a9', 10: 'b0', 11: 'b1', 12: 'b2', 13: 'b3', 14: 'b4', 15: 'b5', 16: 'b6', 17: 'b7', 18: 'b8', 19: 'b9', 20: 'c0', 21: 'c1', 22: 'c2', 23: 'c3', 24: 'c4', 25: 'c5', 26: 'c6', 27: 'c7', 28: 'c8', 29: 'c9'}\n",
      "['a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'b0', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": load_set([DS_TRAIN_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"]),\n",
    "    \"test\": load_set([DS_TEST_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"]),\n",
    "})\n",
    "labels = [label for label in dataset['train'].features.keys() if label not in [\"text\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 862/862 [00:01<00:00, 685.93 examples/s] \n",
      "Map (num_proc=4): 100%|██████████| 92/92 [00:00<00:00, 133.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = preprocess_with_given_labels_train_test_wrap(dataset, tokenizer, labels, label2id, MAX_POSITION_EMBEDDINGS, False, remove_columns=dataset[\"train\"].column_names, \n",
    "                                                               default_teacher_forcing=False, teacher_forcing_prefix=None, doc_pad_tokens=DOC_PAD_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n"
     ]
    }
   ],
   "source": [
    "batch_schema = list(encoded_dataset[\"train\"].features.keys())\n",
    "print(batch_schema)\n",
    "train_dataloader = BatchBuffer(encoded_dataset[\"train\"], TRAIN_BATCH_SIZE)\n",
    "if SHUFFLE_CUSTOM_DATALOADER:\n",
    "    train_dataloader.shuffle()\n",
    "test_dataloader = BatchBuffer(encoded_dataset[\"test\"], TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_dataloader) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "warmup_steps = math.ceil(total_steps * 0.05)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'hyp_cls/BiomedBERT-base_all-labels/'\n"
     ]
    }
   ],
   "source": [
    "if CHECKPOINT_PATH is not None:\n",
    "    try:\n",
    "        os.mkdir(CHECKPOINT_PATH)\n",
    "    except OSError as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_HF_MODEL:\n",
    "    forward_args = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"]\n",
    "else:\n",
    "    forward_args = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:02, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:04, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:05, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:06, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:07, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:08, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:09, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:10, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:12, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:13, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:14, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:15, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:16, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:17, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:18, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:20, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:21, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:22, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:23, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:24, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:25, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:26, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:28, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:29, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:30, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:31, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:32, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:33, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:34, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:36, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:37, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:38, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:39, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:40, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:41, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:42, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:43, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:45, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:46, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:47, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:48, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:49, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:50, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:51, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:53, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:54, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:55, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:56, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:57, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.3660045792878089\n",
      "    accuracy: 0.02320185614849188\n",
      "    per class f1: 0.38487392988553126\n",
      "    f1_micro: 0.38487392988553126\n",
      "    f1_macro: 0.8658159319412181\n",
      "    recall_micro: 0.38487392988553126\n",
      "    recall_macro: 0.932907965970604\n",
      "    precision_micro: 0.38487392988553126\n",
      "    precision_macro: 0.932907965970604\n",
      "  Training epoch took: 0:00:58\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.25107956256555475\n",
      "    accuracy: 0.07608695652173914\n",
      "    per class f1: 0.5383540372670806\n",
      "    f1_micro: 0.5383540372670806\n",
      "    f1_macro: 0.8978260869565206\n",
      "    recall_micro: 0.5383540372670806\n",
      "    recall_macro: 0.9489130434782618\n",
      "    precision_micro: 0.5383540372670806\n",
      "    precision_macro: 0.9489130434782618\n",
      "  Testing took: 0:00:02\n",
      "[Errno 17] File exists: 'hyp_cls/BiomedBERT-base_all-labels//epoch_0/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:02, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:03, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:05, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:06, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:07, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:08, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:09, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:10, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:11, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:13, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:14, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:15, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:16, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:17, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:18, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:19, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:21, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:22, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:23, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:24, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:25, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:26, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:27, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:29, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:30, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:31, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:32, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:33, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:34, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:35, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:37, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:38, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:39, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:40, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:41, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:42, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:44, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:45, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:46, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:47, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:48, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:49, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:50, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:52, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:53, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:54, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:55, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:56, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:57, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.21729092888551355\n",
      "    accuracy: 0.18561484918793503\n",
      "    per class f1: 0.6147064362551606\n",
      "    f1_micro: 0.6147064362551606\n",
      "    f1_macro: 0.9143851508120591\n",
      "    recall_micro: 0.6147064362551606\n",
      "    recall_macro: 0.9571925754060284\n",
      "    precision_micro: 0.6147064362551606\n",
      "    precision_macro: 0.9571925754060284\n",
      "  Training epoch took: 0:00:58\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.2065940654796103\n",
      "    accuracy: 0.18478260869565216\n",
      "    per class f1: 0.6354554865424428\n",
      "    f1_micro: 0.6354554865424428\n",
      "    f1_macro: 0.915942028985507\n",
      "    recall_micro: 0.6354554865424428\n",
      "    recall_macro: 0.9579710144927541\n",
      "    precision_micro: 0.6354554865424428\n",
      "    precision_macro: 0.9579710144927541\n",
      "  Testing took: 0:00:02\n",
      "[Errno 17] File exists: 'hyp_cls/BiomedBERT-base_all-labels//epoch_1/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:02, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:03, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:05, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:06, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:07, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:08, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:09, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:10, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:12, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:13, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:14, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:15, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:16, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:17, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:18, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:20, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:21, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:22, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:23, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:24, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:25, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:27, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:28, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:29, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:30, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:31, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:32, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:34, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:35, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:36, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:37, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:38, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:39, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:41, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:42, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:43, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:44, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:45, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:46, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:47, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:49, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:50, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:51, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:52, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:53, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:54, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:55, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:57, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:58, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.17812869724171482\n",
      "    accuracy: 0.2888631090487239\n",
      "    per class f1: 0.7118186338023924\n",
      "    f1_micro: 0.7118186338023924\n",
      "    f1_macro: 0.9346481051817404\n",
      "    recall_micro: 0.7118186338023924\n",
      "    recall_macro: 0.9673240525908724\n",
      "    precision_micro: 0.7118186338023924\n",
      "    precision_macro: 0.9673240525908724\n",
      "  Training epoch took: 0:00:59\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.19314488999383606\n",
      "    accuracy: 0.2391304347826087\n",
      "    per class f1: 0.6664337474120081\n",
      "    f1_micro: 0.6664337474120081\n",
      "    f1_macro: 0.9224637681159417\n",
      "    recall_micro: 0.6664337474120081\n",
      "    recall_macro: 0.9612318840579713\n",
      "    precision_micro: 0.6664337474120081\n",
      "    precision_macro: 0.9612318840579713\n",
      "  Testing took: 0:00:02\n",
      "[Errno 17] File exists: 'hyp_cls/BiomedBERT-base_all-labels//epoch_2/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:02, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:03, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:05, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:06, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:07, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:08, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:09, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:10, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:11, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:13, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:14, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:15, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:16, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:17, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:18, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:19, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:21, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:22, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:23, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:24, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:25, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:26, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:28, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:29, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:30, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:31, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:32, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:33, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:35, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:36, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:37, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:38, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:39, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:41, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:42, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:43, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:44, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:45, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:47, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:48, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:49, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:50, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:51, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:53, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:54, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:55, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:56, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:57, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:58, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.1532052850303199\n",
      "    accuracy: 0.4013921113689095\n",
      "    per class f1: 0.7751477737266597\n",
      "    f1_micro: 0.7751477737266597\n",
      "    f1_macro: 0.9481825212683609\n",
      "    recall_micro: 0.7751477737266597\n",
      "    recall_macro: 0.9740912606341839\n",
      "    precision_micro: 0.7751477737266597\n",
      "    precision_macro: 0.9740912606341839\n",
      "  Training epoch took: 0:00:59\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.1874664272384151\n",
      "    accuracy: 0.2826086956521739\n",
      "    per class f1: 0.673938923395445\n",
      "    f1_micro: 0.673938923395445\n",
      "    f1_macro: 0.9239130434782604\n",
      "    recall_micro: 0.673938923395445\n",
      "    recall_macro: 0.9619565217391307\n",
      "    precision_micro: 0.673938923395445\n",
      "    precision_macro: 0.9619565217391307\n",
      "  Testing took: 0:00:02\n",
      "[Errno 17] File exists: 'hyp_cls/BiomedBERT-base_all-labels//epoch_3/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:02, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:03, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:04, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:05, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:07, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:08, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:09, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:10, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:11, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:12, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:13, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:14, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:16, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:17, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:18, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:19, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:20, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:21, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:22, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:23, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:25, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:26, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:27, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:28, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:29, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:30, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:31, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:33, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:34, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:35, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:36, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:37, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:38, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:39, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:41, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:42, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:43, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:44, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:45, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:46, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:47, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:49, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:50, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:51, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:52, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:53, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:54, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:55, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:57, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.13574695814046533\n",
      "    accuracy: 0.4814385150812065\n",
      "    per class f1: 0.8209645343056009\n",
      "    f1_micro: 0.8209645343056009\n",
      "    f1_macro: 0.9578499613302328\n",
      "    recall_micro: 0.8209645343056009\n",
      "    recall_macro: 0.9789249806651202\n",
      "    precision_micro: 0.8209645343056009\n",
      "    precision_macro: 0.9789249806651202\n",
      "  Training epoch took: 0:00:57\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.1818496307115192\n",
      "    accuracy: 0.30434782608695654\n",
      "    per class f1: 0.6945134575569358\n",
      "    f1_micro: 0.6945134575569358\n",
      "    f1_macro: 0.9289855072463762\n",
      "    recall_micro: 0.6945134575569358\n",
      "    recall_macro: 0.9644927536231888\n",
      "    precision_micro: 0.6945134575569358\n",
      "    precision_macro: 0.9644927536231888\n",
      "  Testing took: 0:00:02\n",
      "[Errno 17] File exists: 'hyp_cls/BiomedBERT-base_all-labels//epoch_4/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:02, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:03, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:05, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:06, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:07, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:08, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:09, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:10, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:11, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:12, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:14, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:15, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:16, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:17, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:18, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:19, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:20, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:22, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:23, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:24, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:25, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:26, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:27, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:28, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:29, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:31, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:32, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:33, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:34, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:35, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:36, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:37, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:39, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:40, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:41, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:42, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:43, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:44, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:45, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:46, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:48, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:49, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:50, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:51, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:52, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:53, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:54, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:56, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:57, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.12083357651642332\n",
      "    accuracy: 0.5591647331786543\n",
      "    per class f1: 0.8523855559238382\n",
      "    f1_micro: 0.8523855559238382\n",
      "    f1_macro: 0.9657385924207206\n",
      "    recall_micro: 0.8523855559238382\n",
      "    recall_macro: 0.9828692962103648\n",
      "    precision_micro: 0.8523855559238382\n",
      "    precision_macro: 0.9828692962103648\n",
      "  Training epoch took: 0:00:57\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.18135235237929484\n",
      "    accuracy: 0.2826086956521739\n",
      "    per class f1: 0.6854554865424429\n",
      "    f1_micro: 0.6854554865424429\n",
      "    f1_macro: 0.9275362318840576\n",
      "    recall_micro: 0.6854554865424429\n",
      "    recall_macro: 0.9637681159420295\n",
      "    precision_micro: 0.6854554865424429\n",
      "    precision_macro: 0.9637681159420295\n",
      "  Testing took: 0:00:02\n",
      "[Errno 17] File exists: 'hyp_cls/BiomedBERT-base_all-labels//epoch_5/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:02, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:03, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:05, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:06, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:07, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:08, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:09, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:10, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:11, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:12, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:14, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:15, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:16, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:17, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:18, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:19, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:20, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:21, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:23, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:24, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:25, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:26, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:27, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:28, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:29, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:31, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:32, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:33, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:34, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:35, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:36, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:37, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:38, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:40, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:41, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:42, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:43, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:44, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:45, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:46, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:48, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:49, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:50, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:51, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:52, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:53, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:54, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:55, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:57, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.10883959270408701\n",
      "    accuracy: 0.6299303944315545\n",
      "    per class f1: 0.8826795883930444\n",
      "    f1_micro: 0.8826795883930444\n",
      "    f1_macro: 0.9716937354988339\n",
      "    recall_micro: 0.8826795883930444\n",
      "    recall_macro: 0.9858468677494211\n",
      "    precision_micro: 0.8826795883930444\n",
      "    precision_macro: 0.9858468677494211\n",
      "  Training epoch took: 0:00:57\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.18107206196240758\n",
      "    accuracy: 0.2826086956521739\n",
      "    per class f1: 0.6787525879917183\n",
      "    f1_micro: 0.6787525879917183\n",
      "    f1_macro: 0.9275362318840574\n",
      "    recall_micro: 0.6787525879917183\n",
      "    recall_macro: 0.9637681159420294\n",
      "    precision_micro: 0.6787525879917183\n",
      "    precision_macro: 0.9637681159420294\n",
      "  Testing took: 0:00:02\n",
      "model dumped\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    17  of    862.    Elapsed:  0:00:01, Remaining:  0:00:50.\n",
      "  Batch    34  of    862.    Elapsed:  0:00:02, Remaining:  0:00:49.\n",
      "  Batch    51  of    862.    Elapsed:  0:00:03, Remaining:  0:00:48.\n",
      "  Batch    68  of    862.    Elapsed:  0:00:05, Remaining:  0:00:47.\n",
      "  Batch    85  of    862.    Elapsed:  0:00:06, Remaining:  0:00:46.\n",
      "  Batch   102  of    862.    Elapsed:  0:00:07, Remaining:  0:00:45.\n",
      "  Batch   119  of    862.    Elapsed:  0:00:08, Remaining:  0:00:44.\n",
      "  Batch   136  of    862.    Elapsed:  0:00:09, Remaining:  0:00:43.\n",
      "  Batch   153  of    862.    Elapsed:  0:00:10, Remaining:  0:00:42.\n",
      "  Batch   170  of    862.    Elapsed:  0:00:11, Remaining:  0:00:41.\n",
      "  Batch   187  of    862.    Elapsed:  0:00:13, Remaining:  0:00:40.\n",
      "  Batch   204  of    862.    Elapsed:  0:00:14, Remaining:  0:00:39.\n",
      "  Batch   221  of    862.    Elapsed:  0:00:15, Remaining:  0:00:38.\n",
      "  Batch   238  of    862.    Elapsed:  0:00:16, Remaining:  0:00:37.\n",
      "  Batch   255  of    862.    Elapsed:  0:00:17, Remaining:  0:00:36.\n",
      "  Batch   272  of    862.    Elapsed:  0:00:18, Remaining:  0:00:35.\n",
      "  Batch   289  of    862.    Elapsed:  0:00:19, Remaining:  0:00:34.\n",
      "  Batch   306  of    862.    Elapsed:  0:00:20, Remaining:  0:00:33.\n",
      "  Batch   323  of    862.    Elapsed:  0:00:22, Remaining:  0:00:32.\n",
      "  Batch   340  of    862.    Elapsed:  0:00:23, Remaining:  0:00:31.\n",
      "  Batch   357  of    862.    Elapsed:  0:00:24, Remaining:  0:00:30.\n",
      "  Batch   374  of    862.    Elapsed:  0:00:25, Remaining:  0:00:29.\n",
      "  Batch   391  of    862.    Elapsed:  0:00:26, Remaining:  0:00:28.\n",
      "  Batch   408  of    862.    Elapsed:  0:00:27, Remaining:  0:00:27.\n",
      "  Batch   425  of    862.    Elapsed:  0:00:28, Remaining:  0:00:26.\n",
      "  Batch   442  of    862.    Elapsed:  0:00:30, Remaining:  0:00:25.\n",
      "  Batch   459  of    862.    Elapsed:  0:00:31, Remaining:  0:00:24.\n",
      "  Batch   476  of    862.    Elapsed:  0:00:32, Remaining:  0:00:23.\n",
      "  Batch   493  of    862.    Elapsed:  0:00:33, Remaining:  0:00:22.\n",
      "  Batch   510  of    862.    Elapsed:  0:00:34, Remaining:  0:00:21.\n",
      "  Batch   527  of    862.    Elapsed:  0:00:35, Remaining:  0:00:20.\n",
      "  Batch   544  of    862.    Elapsed:  0:00:36, Remaining:  0:00:19.\n",
      "  Batch   561  of    862.    Elapsed:  0:00:37, Remaining:  0:00:18.\n",
      "  Batch   578  of    862.    Elapsed:  0:00:39, Remaining:  0:00:17.\n",
      "  Batch   595  of    862.    Elapsed:  0:00:40, Remaining:  0:00:16.\n",
      "  Batch   612  of    862.    Elapsed:  0:00:41, Remaining:  0:00:15.\n",
      "  Batch   629  of    862.    Elapsed:  0:00:42, Remaining:  0:00:14.\n",
      "  Batch   646  of    862.    Elapsed:  0:00:43, Remaining:  0:00:13.\n",
      "  Batch   663  of    862.    Elapsed:  0:00:44, Remaining:  0:00:12.\n",
      "  Batch   680  of    862.    Elapsed:  0:00:45, Remaining:  0:00:11.\n",
      "  Batch   697  of    862.    Elapsed:  0:00:47, Remaining:  0:00:10.\n",
      "  Batch   714  of    862.    Elapsed:  0:00:48, Remaining:  0:00:09.\n",
      "  Batch   731  of    862.    Elapsed:  0:00:49, Remaining:  0:00:08.\n",
      "  Batch   748  of    862.    Elapsed:  0:00:50, Remaining:  0:00:07.\n",
      "  Batch   765  of    862.    Elapsed:  0:00:51, Remaining:  0:00:06.\n",
      "  Batch   782  of    862.    Elapsed:  0:00:52, Remaining:  0:00:05.\n",
      "  Batch   799  of    862.    Elapsed:  0:00:53, Remaining:  0:00:04.\n",
      "  Batch   816  of    862.    Elapsed:  0:00:54, Remaining:  0:00:03.\n",
      "  Batch   833  of    862.    Elapsed:  0:00:56, Remaining:  0:00:02.\n",
      "  Batch   850  of    862.    Elapsed:  0:00:57, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.09858817791268057\n",
      "    accuracy: 0.7064965197215777\n",
      "    per class f1: 0.9113384842793186\n",
      "    f1_micro: 0.9113384842793186\n",
      "    f1_macro: 0.977958236658928\n",
      "    recall_micro: 0.9113384842793186\n",
      "    recall_macro: 0.9889791183294673\n",
      "    precision_micro: 0.9113384842793186\n",
      "    precision_macro: 0.9889791183294673\n",
      "  Training epoch took: 0:00:58\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.18395435429461623\n",
      "    accuracy: 0.29347826086956524\n",
      "    per class f1: 0.6767598343685299\n",
      "    f1_micro: 0.6767598343685299\n",
      "    f1_macro: 0.9275362318840574\n",
      "    recall_micro: 0.6767598343685299\n",
      "    recall_macro: 0.9637681159420294\n",
      "    precision_micro: 0.6767598343685299\n",
      "    precision_macro: 0.9637681159420294\n",
      "  Testing took: 0:00:02\n",
      "model dumped\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stats = train_bern_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    device,\n",
    "    id2label=id2label,\n",
    "    batch_schema=batch_schema,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    print_status=True,\n",
    "    is_hf_model=IS_HF_MODEL,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    one_label_only=False,\n",
    "    generic_output_class=True,\n",
    "    calc_metrics=True,\n",
    "    per_class_f1=True,\n",
    "\n",
    "    forward_args=forward_args\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
