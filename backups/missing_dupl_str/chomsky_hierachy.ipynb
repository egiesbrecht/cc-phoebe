{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from load_set import *\n",
    "import model_training\n",
    "from model_training import train_bern_model\n",
    "import coin_i3C_modeling as ci3C\n",
    "from rnn_modeling import *\n",
    "import ntm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "MAX_POSITION_EMBEDDINGS = 20\n",
    "TEST_POSITION_EMBEDDINGS = 20\n",
    "VOCAB_SIZE = MAX_POSITION_EMBEDDINGS\n",
    "NUM_LABELS = 2\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_TRAIN_SAMPLES = 3_500_000 #256 * 380\n",
    "NUM_WARMUP_STEPS = 2_000_000\n",
    "SAMPLE_METHOD = \"static-warmup\"\n",
    "NUM_TEST_SAMPLES = 1000\n",
    "LEARNING_RATE = 1e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500000, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TRAIN_SAMPLES, NUM_TEST_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket-sort, duplicate-string, parity-check, missing-duplicate-string\n",
    "#TASK = \"missing-duplicate-string\"\n",
    "TASK = \"parity-check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"parity-check\":\n",
    "    VOCAB_SIZE = 2\n",
    "elif TASK == \"missing-duplicate-string\":\n",
    "    VOCAB_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = sLSTMForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.0,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            intermediate_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = COINForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            intermediate_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "            layer_norm_eps=1e-12\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = RNNForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            num_hidden_layers=2,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = StackRNNForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.5,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # B=50\n",
    "    # T_train=20\n",
    "    # T_test= >500\n",
    "    class LSTMForParityCheck(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.n_layers = config.num_hidden_layers\n",
    "            #self.lstm = nn.LSTM(2, config.hidden_size, batch_first=True, num_layers=self.n_layers, dropout=config.hidden_dropout_prob)\n",
    "            self.lstm = nn.LSTM(config.hidden_size, config.hidden_size, batch_first=True, num_layers=self.n_layers, dropout=config.hidden_dropout_prob)\n",
    "            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "            self.classifier = nn.Sequential(\n",
    "                #nn.Dropout(config.hidden_dropout_prob, \n",
    "                nn.Linear(config.hidden_size, config.num_labels)\n",
    "            )\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            logits = F.one_hot(input_ids.long(), self.config.vocab_size).float()\n",
    "            logits = self.embeddings(logits)\n",
    "            B, T, C = logits.shape\n",
    "            hidden = (torch.randn(self.n_layers, B, C, device=logits.device), torch.randn(self.n_layers, B, C, device=logits.device))\n",
    "            logits, hidden = self.lstm(logits)\n",
    "            r_logits = self.classifier(logits[:, -1])\n",
    "            loss = self.loss_fn(r_logits, labels)\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=r_logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "    model = LSTMForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.5,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    class BertForBucketSort(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.bert = transformers.BertModel(config)\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            emb = self.embeddings(F.one_hot(input_ids, self.config.vocab_size).float())\n",
    "            B, T, C = emb.shape\n",
    "            logits = self.bert(inputs_embeds=emb, attention_mask=attention_mask).last_hidden_state\n",
    "            logits = self.lm_head(logits)\n",
    "            loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "\n",
    "    model = BertForBucketSort(\n",
    "        config=transformers.BertConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_attention_heads=1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    class BertForParityCheck(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            #self.bert = transformers.BertModel(config)\n",
    "            self.bert = transformers.BertForSequenceClassification(config)\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            #print(attention_mask)\n",
    "            emb = F.one_hot(input_ids, self.config.vocab_size).float()\n",
    "            emb = self.embeddings(emb)\n",
    "            logits = self.bert(inputs_embeds=emb, attention_mask=attention_mask).logits\n",
    "            #logits = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            #logits = self.classifier(logits[:, 0, :])\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "\n",
    "    model = BertForParityCheck(\n",
    "        config=transformers.BertConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_attention_heads=1,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    class BertForBucketSort(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.bert = transformers.BertModel(config)\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            emb = self.embeddings(F.one_hot(input_ids, self.config.vocab_size).float())\n",
    "            B, T, C = emb.shape\n",
    "            logits = self.bert(inputs_embeds=emb, attention_mask=attention_mask).last_hidden_state\n",
    "            logits = self.lm_head(logits)\n",
    "            loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "\n",
    "    model = BertForBucketSort(\n",
    "        config=transformers.BertConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_hidden_layers=5,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_attention_heads=1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample schema: tensor([ 1,  1,  1,  ..., 20, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "if TASK == \"bucket-sort\":\n",
    "    gen_fn = generate_bucket_sort_set\n",
    "elif TASK == \"duplicate-string\":\n",
    "    gen_fn = generate_duplicate_string_set\n",
    "elif TASK == \"parity-check\":\n",
    "    gen_fn = generate_parity_check_set\n",
    "elif TASK == \"missing-duplicate-string\":\n",
    "    gen_fn = generate_missing_duplicate_string_set\n",
    "\n",
    "#train_buf = generate_uniform_batches(gen_fn, B=BATCH_SIZE, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE, num_samples=NUM_TRAIN_SAMPLES)\n",
    "#test_buf = generate_uniform_batches(gen_fn, B=BATCH_SIZE, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE, num_samples=NUM_TEST_SAMPLES)\n",
    "train_buf, test_buf = generate_ch_batches(\n",
    "    generate_set_fn=gen_fn, \n",
    "    B=BATCH_SIZE, \n",
    "    T_train=MAX_POSITION_EMBEDDINGS, \n",
    "    T_test=TEST_POSITION_EMBEDDINGS, \n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    num_train_samples=NUM_TRAIN_SAMPLES, \n",
    "    num_test_samples=NUM_TEST_SAMPLES,\n",
    "    sample_method=SAMPLE_METHOD,\n",
    "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_buf, test_buf = generate_static_parity_check_set(BATCH_SIZE, MAX_POSITION_EMBEDDINGS, TEST_POSITION_EMBEDDINGS, NUM_TRAIN_SAMPLES, NUM_TEST_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SCHEMA = [\"input_ids\", \"decoder_input_ids\", \"attention_mask\", \"labels\"]\n",
    "#BATCH_SCHEMA = [\"input_ids\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n",
      "{0: 0, 1: 1}\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "labels = np.unique(train_buf[0][\"labels\"]).tolist()\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")# if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_buf) / BATCH_SIZE * EPOCHS\n",
    "warmup_steps = math.ceil(total_steps * 0.05)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'decoder_input_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SCHEMA = list(train_buf[0].keys())\n",
    "BATCH_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch 1,400  of  70,000.    Elapsed:  0:00:03, Remaining:  0:02:27.\n",
      "  Batch 2,800  of  70,000.    Elapsed:  0:00:07, Remaining:  0:02:24.\n",
      "  Batch 4,200  of  70,000.    Elapsed:  0:00:10, Remaining:  0:02:21.\n",
      "  Batch 5,600  of  70,000.    Elapsed:  0:00:15, Remaining:  0:03:50.\n",
      "  Batch 7,000  of  70,000.    Elapsed:  0:00:19, Remaining:  0:03:45.\n",
      "  Batch 8,400  of  70,000.    Elapsed:  0:00:24, Remaining:  0:03:40.\n",
      "  Batch 9,800  of  70,000.    Elapsed:  0:00:30, Remaining:  0:04:18.\n",
      "  Batch 11,200  of  70,000.    Elapsed:  0:00:36, Remaining:  0:04:12.\n",
      "  Batch 12,600  of  70,000.    Elapsed:  0:00:42, Remaining:  0:04:06.\n",
      "  Batch 14,000  of  70,000.    Elapsed:  0:00:49, Remaining:  0:04:40.\n",
      "  Batch 15,400  of  70,000.    Elapsed:  0:00:56, Remaining:  0:04:33.\n",
      "  Batch 16,800  of  70,000.    Elapsed:  0:01:03, Remaining:  0:04:26.\n",
      "  Batch 18,200  of  70,000.    Elapsed:  0:01:11, Remaining:  0:04:56.\n",
      "  Batch 19,600  of  70,000.    Elapsed:  0:01:20, Remaining:  0:04:48.\n",
      "  Batch 21,000  of  70,000.    Elapsed:  0:01:28, Remaining:  0:04:40.\n",
      "  Batch 22,400  of  70,000.    Elapsed:  0:01:37, Remaining:  0:05:06.\n",
      "  Batch 23,800  of  70,000.    Elapsed:  0:01:47, Remaining:  0:04:57.\n",
      "  Batch 25,200  of  70,000.    Elapsed:  0:01:56, Remaining:  0:04:48.\n",
      "  Batch 26,600  of  70,000.    Elapsed:  0:02:07, Remaining:  0:05:41.\n",
      "  Batch 28,000  of  70,000.    Elapsed:  0:02:17, Remaining:  0:05:00.\n",
      "  Batch 29,400  of  70,000.    Elapsed:  0:02:28, Remaining:  0:05:19.\n",
      "  Batch 30,800  of  70,000.    Elapsed:  0:02:39, Remaining:  0:05:36.\n",
      "  Batch 32,200  of  70,000.    Elapsed:  0:02:51, Remaining:  0:04:57.\n",
      "  Batch 33,600  of  70,000.    Elapsed:  0:03:03, Remaining:  0:05:12.\n",
      "  Batch 35,000  of  70,000.    Elapsed:  0:03:16, Remaining:  0:05:25.\n",
      "  Batch 36,400  of  70,000.    Elapsed:  0:03:29, Remaining:  0:05:12.\n",
      "  Batch 37,800  of  70,000.    Elapsed:  0:03:41, Remaining:  0:04:36.\n",
      "  Batch 39,200  of  70,000.    Elapsed:  0:03:55, Remaining:  0:05:08.\n",
      "  Batch 40,600  of  70,000.    Elapsed:  0:04:09, Remaining:  0:04:54.\n",
      "  Batch 42,000  of  70,000.    Elapsed:  0:04:23, Remaining:  0:04:40.\n",
      "  Batch 43,400  of  70,000.    Elapsed:  0:04:36, Remaining:  0:04:26.\n",
      "  Batch 44,800  of  70,000.    Elapsed:  0:04:50, Remaining:  0:04:12.\n",
      "  Batch 46,200  of  70,000.    Elapsed:  0:05:04, Remaining:  0:03:58.\n",
      "  Batch 47,600  of  70,000.    Elapsed:  0:05:17, Remaining:  0:03:44.\n",
      "  Batch 49,000  of  70,000.    Elapsed:  0:05:31, Remaining:  0:03:30.\n",
      "  Batch 50,400  of  70,000.    Elapsed:  0:05:45, Remaining:  0:03:16.\n",
      "  Batch 51,800  of  70,000.    Elapsed:  0:05:59, Remaining:  0:03:02.\n",
      "  Batch 53,200  of  70,000.    Elapsed:  0:06:13, Remaining:  0:02:48.\n",
      "  Batch 54,600  of  70,000.    Elapsed:  0:06:26, Remaining:  0:02:34.\n",
      "  Batch 56,000  of  70,000.    Elapsed:  0:06:40, Remaining:  0:02:20.\n",
      "  Batch 57,400  of  70,000.    Elapsed:  0:06:53, Remaining:  0:02:06.\n",
      "  Batch 58,800  of  70,000.    Elapsed:  0:07:07, Remaining:  0:01:44.\n",
      "  Batch 60,200  of  70,000.    Elapsed:  0:07:20, Remaining:  0:01:31.\n",
      "  Batch 61,600  of  70,000.    Elapsed:  0:07:34, Remaining:  0:01:18.\n",
      "  Batch 63,000  of  70,000.    Elapsed:  0:07:47, Remaining:  0:01:10.\n",
      "  Batch 64,400  of  70,000.    Elapsed:  0:08:01, Remaining:  0:00:56.\n",
      "  Batch 65,800  of  70,000.    Elapsed:  0:08:14, Remaining:  0:00:42.\n",
      "  Batch 67,200  of  70,000.    Elapsed:  0:08:28, Remaining:  0:00:28.\n",
      "  Batch 68,600  of  70,000.    Elapsed:  0:08:42, Remaining:  0:00:13.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.6966792962812569\n",
      "    accuracy: 0.5436614285714354\n",
      "  Training epoch took: 0:08:55\n",
      "\n",
      "Running Testing...\n",
      "  Average testing scores:\n",
      "    loss: 0.7730012092590332\n",
      "    accuracy: 0.50086\n",
      "  Testing took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch 1,400  of  70,000.    Elapsed:  0:00:03, Remaining:  0:02:27.\n",
      "  Batch 2,800  of  70,000.    Elapsed:  0:00:06, Remaining:  0:02:24.\n",
      "  Batch 4,200  of  70,000.    Elapsed:  0:00:10, Remaining:  0:02:21.\n",
      "  Batch 5,600  of  70,000.    Elapsed:  0:00:14, Remaining:  0:03:50.\n",
      "  Batch 7,000  of  70,000.    Elapsed:  0:00:19, Remaining:  0:03:45.\n",
      "  Batch 8,400  of  70,000.    Elapsed:  0:00:24, Remaining:  0:03:40.\n",
      "  Batch 9,800  of  70,000.    Elapsed:  0:00:30, Remaining:  0:04:18.\n",
      "  Batch 11,200  of  70,000.    Elapsed:  0:00:35, Remaining:  0:04:12.\n"
     ]
    }
   ],
   "source": [
    "model_training.STRING_BATCH_INDEX = True\n",
    "stats = train_bern_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    device,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    id2label,\n",
    "    batch_schema=BATCH_SCHEMA,\n",
    "    train_dataloader=train_buf,\n",
    "    test_dataloader=test_buf,\n",
    "    #vocab_size=VOCAB_SIZE,\n",
    "    print_status=True,\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    test_batch_size=BATCH_SIZE,\n",
    "    generic_output_class=True,\n",
    "    forward_args=[\"input_ids\", \"decoder_input_ids\", \"attention_mask\", \"labels\"],\n",
    "    chomsky_task=True,\n",
    "\n",
    "    calc_metrics=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
