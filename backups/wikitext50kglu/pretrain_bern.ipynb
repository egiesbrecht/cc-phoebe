{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/pushshift/cc-phoebe/rotary_embeddings.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/mnt/pushshift/cc-phoebe/rotary_embeddings.py:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from datasets import DatasetDict, load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    BertTokenizer, \n",
    "    RobertaTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "from load_set import load_set, load_moses_set\n",
    "from epoch_stats import EpochStats, print_stat_tuples\n",
    "import model_training\n",
    "from model_training import (\n",
    "    BatchBuffer, \n",
    "    mask_tokens, \n",
    "    train_bern_model, \n",
    "    preprocess_for_maskedlm, \n",
    "    preprocess_for_causallm, \n",
    "    preprocess_for_monologe, \n",
    "    preprocess_for_sparselm, \n",
    "    preprocess_for_binary_sparselm,\n",
    "    num_parameters, \n",
    "    num_trainable_parameters,\n",
    "    preprocess_for_translation,\n",
    "    preprocess_for_key_masking\n",
    ")\n",
    "\n",
    "import coin_i2C_modeling as ci2C\n",
    "import coin_i2D_modeling as ci2D\n",
    "import coin_i3A_modeling as ci3A\n",
    "import coin_i3C_modeling as ci3C\n",
    "import coin_i4_modeling as ci4\n",
    "import transformer_modeling as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBATCH = False\n",
    "TRAIN_BATCH_SIZE = 3\n",
    "TEST_BATCH_SIZE = 3\n",
    "#BASE_PATH = \"benchmark_models/Llama_config_B3/\"\n",
    "BASE_PATH = \"benchmark_models/ppl_play/10k_play/\"\n",
    "\n",
    "#BASE_PATH = \"pretrained_models/COIN-i3C_inbio_mask_no-decoder\"\n",
    "#BASE_PATH = \"tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2/\"\n",
    "\n",
    "if REBATCH:\n",
    "    if BASE_PATH[-1] == \"/\":\n",
    "        BASE_PATH = BASE_PATH[:-1]\n",
    "    BASE_PATH += \"_rebatch/\"\n",
    "DATASET_JSON_PATH = f\"{BASE_PATH}/dataset/\"\n",
    "CHECKPOINT_PATH = f\"{BASE_PATH}/model/\"\n",
    "WORDPIECE_TOKENIZER_DIR = f\"{BASE_PATH}/wordpiece_tokenizer/\"\n",
    "BPE_TOKENIZER_DIR = f\"{BASE_PATH}/bpe_tokenizer/\"\n",
    "SENTENCE_PIECE_TOKENIZER_DIR = f\"{BASE_PATH}/sentence_piece_tokenizer/\"\n",
    "USE_CUSTOM_DATALOADER = False\n",
    "LEARNING_RATE = 1e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_TRAIN_DATA = True\n",
    "SHUFFLE_TEST_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAUSAL_LM = False\n",
    "ENCODE_CAUSAL_LM = False\n",
    "GROUP_TEXTS = True\n",
    "SPARSIFY = True\n",
    "MASK_TOKEN = None\n",
    "PAD_TOKEN = None\n",
    "PREFIX = None#\"Translate the following text:\"\n",
    "#PREFIX = \"Replace all of the mask-tokens: \"\n",
    "#PREFIX = \"This sentence is completely obsolete \"\n",
    "SWITCH_II_DECODER_II = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out dir: benchmark_models/ppl_play/10k_play/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"out dir: {BASE_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOCAB_SIZE = 30_522\n",
    "#VOCAB_SIZE = 32_000\n",
    "VOCAB_SIZE = 50257 #+ 2\n",
    "MAX_POSITION_EMBEDDINGS = 512\n",
    "#MAX_POSITION_EMBEDDINGS = 516\n",
    "#MAX_POSITION_EMBEDDINGS = 768\n",
    "IS_HF_MODEL = False\n",
    "IS_ENCODER_DECODER_MODEL = False\n",
    "EPOCH_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCCA_SAVE_CONFIG = ci3C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=True,\n",
    "    num_decay_parts=1,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    hidden_pos_offset=False,\n",
    "    rope_dim=16,\n",
    "    num_query_heads=1,\n",
    "\n",
    "    decoder_output=\"adaptive\",\n",
    "    revert_decoder=True,\n",
    "    decoder_schema=[1] * 4,\n",
    "    cross_encoder_schema=[1] * 4,\n",
    "    block_io_schema=None,#[[1024, 1024*4, 1024], [1024, 1024*2, 1024]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = tm.TransformerForCausalLM(\n",
    "        config=tm.TransformerConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            hidden_size=1024,\n",
    "            intermediate_size=1536,\n",
    "            num_layers=4,\n",
    "            rms_norm_eps=1e-6,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 0.999999\n",
      "gamma: 0.999999\n",
      "gamma: 0.999999\n",
      "gamma: 0.999999\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    model = ci4.COINForCausalLM(\n",
    "        config=ci4.COINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            hidden_size=1024,\n",
    "            intermediate_size=1536,\n",
    "            forward_method=\"parallel\",\n",
    "            num_layers=4,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            rms_norm_eps=1e-6,\n",
    "            hidden_dropout_prob=0.0,\n",
    "            training_chunk_size=None,\n",
    "            inference_chunk_size=1,\n",
    "            reset_hidden_states=True,\n",
    "            apply_decay_mask=True,\n",
    "            apply_attention_mask=False,\n",
    "            apply_group_mask=False,\n",
    "            gamma=(1 - 1e-6),\n",
    "            num_heads=1,\n",
    "            num_key_value_heads=None,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    model = transformers.LlamaForCausalLM(\n",
    "        config=transformers.LlamaConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            hidden_size=1024,\n",
    "            intermediate_size=1536,\n",
    "            num_hidden_layers=4,\n",
    "            num_attention_heads=1,\n",
    "            num_key_value_heads=1,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            rms_norm_eps=1e-6,\n",
    "            hidden_dropout_prob=0.0,\n",
    "            use_cache=False,\n",
    "            _attn_implementation=\"eager\",\n",
    "            mlp_bias=True,\n",
    "            attention_bias=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI3C_CONFIG = ci3C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    hidden_size=1024,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=True,\n",
    "    num_decay_parts=1,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    hidden_pos_offset=False,\n",
    "    rope_dim=16,\n",
    "    num_query_heads=1,\n",
    "\n",
    "    decoder_output=\"none\",\n",
    "    revert_decoder=False,\n",
    "    decoder_schema=[0] * 2,\n",
    "    cross_encoder_schema=[0] * 2,\n",
    "    experts_schema=None,#[2, 2],\n",
    "    block_io_schema=None,#[[1024, 1024*4, 1024]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3C.COINForConditionalGeneration(\n",
    "        CI3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3C.COINForMaskedLM(\n",
    "        CI3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297,126,176\n",
      "297,126,144\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,}\\n{:,}\".format(num_parameters(model), num_trainable_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'benchmark_models/ppl_play/10k_play/'\n",
      "[Errno 17] File exists: 'benchmark_models/ppl_play/10k_play//model/'\n",
      "[Errno 17] File exists: 'benchmark_models/ppl_play/10k_play//wordpiece_tokenizer/'\n",
      "[Errno 17] File exists: 'benchmark_models/ppl_play/10k_play//bpe_tokenizer/'\n",
      "[Errno 17] File exists: 'benchmark_models/ppl_play/10k_play//sentence_piece_tokenizer/'\n",
      "[Errno 17] File exists: 'benchmark_models/ppl_play/10k_play//dataset/'\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "try:\n",
    "    os.mkdir(BASE_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(CHECKPOINT_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(WORDPIECE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(BPE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(SENTENCE_PIECE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(DATASET_JSON_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#dataset = DatasetDict({\n",
    "#    \"train\": load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train[0:10000]\"),\n",
    "#    \"test\":  load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"validation[:1500]\")\n",
    "#})\n",
    "\n",
    "#dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oasst1, aaabdon, mcca, translation_mcca\n",
    "#DATASET = \"translation_mcca\"\n",
    "#DATASET = \"slim_pajama\"\n",
    "#DATASET = \"oasst1\"\n",
    "DATASET = \"wikitext_ppl\"\n",
    "\n",
    "#HF_TRAIN_ROWS = 25_000\n",
    "#HF_TRAIN_ROWS = 10_000\n",
    "HF_TRAIN_ROWS = 10_000\n",
    "HF_TRAIN_FROM = 0#10_000\n",
    "\n",
    "#HF_TEST_ROWS = 1_500\n",
    "#HF_TEST_ROWS = 1000\n",
    "HF_TEST_ROWS = -1\n",
    "HF_TEST_FROM = 0\n",
    "\n",
    "CUSTOM_BASE_DS_PATH = \"../datasets/big_AAABDON_Nmax_st200_s0_a10_tvsplit.1_no_norm/\"\n",
    "CUSTOM_DS_TO_FILE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4358\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1805708\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "if DATASET in (\"wikitext_ppl\", \"wikitext_mlm\"):\n",
    "    train_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "    test_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\")\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"slim_pajama\":\n",
    "    CACHE_DIR = \"/mnt/pushshift/slim_lajama_627B/\"\n",
    "    train_dataset = load_dataset(\"cerebras/SlimPajama-627B\", split=f\"train[{HF_TRAIN_FROM}:{HF_TRAIN_ROWS}]\", cache_dir=CACHE_DIR)\n",
    "    test_dataset = load_dataset(\"cerebras/SlimPajama-627B\", split=f\"validation[{HF_TEST_FROM}:{HF_TEST_ROWS}]\", cache_dir=CACHE_DIR)\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"inbio_mask\":\n",
    "    inbio = load_set([\"INBIO.csv\"], unused_fields=(\"Synonyms,Obsolete,CUI,Semantic Types,Parents,achieves,adjacent to,affects,allocates,capable of,characteristic for,completed invasion phase,contained in,contains,contributes to,contributor,created by,decreases,decreases effort in,derives from,derives into,determines,don't use concept,editor note,enabled by,ends,ends during,ends with,enhance,facilitate,has alien range,has amount of closely related species,has amount of species,has area,has component,has decreased effort level by,has distribution,has growth,has habitat,has increased effort level by,has increased levels of,has index,has input,has invasion success likelihood,has level of,has measurement,has measurement unit label,has measurement value,has mortality,has natality,has native range,has number of individuals,has output,has part,has part structure that is capable of,has participant,has propagule pressure,has quality,has range,has recruitment,has role,has spatial occupant at some time,has specific name,has status,has value,http://data.bioontology.org/metadata/obo/part_of,http://data.bioontology.org/metadata/prefixIRI,http://data.bioontology.org/metadata/treeView,http://purl.obolibrary.org/obo/IAO_0000111,http://purl.obolibrary.org/obo/IAO_0000112,http://purl.obolibrary.org/obo/IAO_0000114,http://purl.obolibrary.org/obo/IAO_0000115,http://purl.obolibrary.org/obo/IAO_0000118,http://purl.obolibrary.org/obo/IAO_0000119,http://purl.obolibrary.org/obo/IAO_0000232,http://purl.obolibrary.org/obo/IAO_0000412,http://purl.obolibrary.org/obo/ncbitaxon#has_rank,http://purl.obolibrary.org/obo/NCIT_A8,http://purl.obolibrary.org/obo/NCIT_NHC0,http://purl.obolibrary.org/obo/NCIT_P106,http://purl.obolibrary.org/obo/NCIT_P107,http://purl.obolibrary.org/obo/NCIT_P108,http://purl.obolibrary.org/obo/NCIT_P207,http://purl.obolibrary.org/obo/NCIT_P322,http://purl.obolibrary.org/obo/NCIT_P325,http://purl.obolibrary.org/obo/NCIT_P366,http://purl.obolibrary.org/obo/OBI_0001886,http://purl.obolibrary.org/obo/RO_0001900,http://purl.org/dc/elements/1.1/source,http://purl.org/dc/terms/creator,http://www.geneontology.org/formats/oboInOwl#creation_date,http://www.geneontology.org/formats/oboInOwl#hasAlternativeId,http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym,http://www.geneontology.org/formats/oboInOwl#hasDbXref,http://www.geneontology.org/formats/oboInOwl#hasExactSynonym,http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym,http://www.geneontology.org/formats/oboInOwl#hasOBONamespace,http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym,http://www.geneontology.org/formats/oboInOwl#hasSynonymType,http://www.geneontology.org/formats/oboInOwl#id,http://www.geneontology.org/formats/oboInOwl#inSubset,http://www.w3.org/2000/01/rdf-schema#comment,http://www.w3.org/2000/01/rdf-schema#label,http://www.w3.org/2002/07/owl#deprecated,http://www.w3.org/2004/02/skos/core#altLabel,http://www.w3.org/2004/02/skos/core#definition,http://www.w3.org/2004/02/skos/core#notation,https://w3id.org/inbio#_000130,https://w3id.org/inbio#_000132,increases,increases effort in,interacts with,is absent,is affected by,is against,is aggregate of,is alien range to,is characteristic of,is characterized by,is closely related to,is enemy of,is enhanced by,is growth of,is habitat of,is in invasion phase,is mortality of,is natality of,is native range to,is part of,is prey of,is range of,is recruitment of,is similar to,is status of,license,license,license,license,located in,location of,occupies spatial region at some time,occurs in,output of,overlaps,part of,participates in,produced by,produces,quality of,role of,shows changes in species trait,spatially coextensive with,surrounded by,surrounds,title,TODO,license.1,license.2,license.3\".split(\",\")))\n",
    "    bio2def = dict(zip(inbio[\"Preferred Label\"], inbio[\"Definitions\"]))\n",
    "    mask_keys = inbio[\"Preferred Label\"]\n",
    "\n",
    "    DS_TRAIN_PATH = \"datasets/abstracts_all_labels_train.csv\"\n",
    "    DS_TEST_PATH = \"datasets/abstracts_all_labels_test.csv\"\n",
    "\n",
    "    train_dataset = load_set([DS_TRAIN_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"])\n",
    "    test_dataset = load_set([DS_TEST_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"])\n",
    "\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"mcca\":\n",
    "    train_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-2][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "    test_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "        ]\n",
    "    })\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"translation_mcca\":\n",
    "    train_dataset = load_moses_set({\n",
    "        \"src\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-5][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-5][0-9]\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-5][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-5][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "    test_dataset = load_moses_set({\n",
    "        \"src\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x800[0-1]\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x800[0-1]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "        ]\n",
    "    })\n",
    "    tok_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-9][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-9][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "elif DATASET == \"aaabdon\":\n",
    "    train_ds = [f\"{CUSTOM_BASE_DS_PATH}/train/train_00[0-{CUSTOM_DS_TO_FILE}].csv\"]\n",
    "    test_ds = [f\"{CUSTOM_BASE_DS_PATH}/validation/validation_00[0-{CUSTOM_DS_TO_FILE}].csv\"]\n",
    "    train_dataset = load_set(train_ds)#.select(list(range(HF_TRAIN_ROWS)))\n",
    "    test_dataset = load_set(test_ds)#.select(list(range(HF_TEST_ROWS)))\n",
    "    tok_dataset = load_set([f\"{CUSTOM_BASE_DS_PATH}/train/train_*.csv\"])\n",
    "elif DATASET == \"oasst1\":\n",
    "    #train_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=f\"train[0:{HF_TRAIN_ROWS}]\")\n",
    "    #test_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=f\"validation[:{HF_TEST_ROWS}]\")\n",
    "    #tok_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")\n",
    "    \n",
    "    #train_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\", split=f\"train[0:{HF_TRAIN_ROWS}]\")  .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "    #test_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\", split=f\"test[0:{HF_TEST_ROWS}]\")     .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "    #tok_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\")                                       .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "\n",
    "    train_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    test_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"validation\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    tok_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    #train_dataset = load_from_disk(\"../datasets/oasst1/train\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TRAIN_ROWS)))\n",
    "    #test_dataset = load_from_disk(\"../datasets/oasst1/validation\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TEST_ROWS)))\n",
    "    #tok_dataset = load_from_disk(\"../datasets/oasst1/train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "\n",
    "    #train_dataset = load_dataset(\"OpenAssistant/oasst2\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TRAIN_ROWS)))\n",
    "    #test_dataset = load_dataset(\"OpenAssistant/oasst2\", split=\"validation\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TEST_ROWS)))\n",
    "\n",
    "if SHUFFLE_TRAIN_DATA:\n",
    "    print(\"shuffle\")\n",
    "    train_dataset = train_dataset.shuffle()\n",
    "if HF_TRAIN_ROWS > 0:\n",
    "    train_dataset =  train_dataset.select(list(range(HF_TRAIN_FROM, HF_TRAIN_ROWS)))\n",
    "if SHUFFLE_TEST_DATA:\n",
    "    print(\"shuffle\")\n",
    "    test_dataset = test_dataset.shuffle()\n",
    "if HF_TEST_ROWS > 0:\n",
    "    test_dataset = test_dataset.select(list(range(HF_TEST_FROM, HF_TEST_ROWS)))\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "print(tok_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.to_json(f\"{DATASET_JSON_PATH}/train.json\")\n",
    "#test_dataset.to_json(f\"{DATASET_JSON_PATH}/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "labels = [label for label in train_dataset.features.keys() if label not in [\"text\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, BertWordPieceTokenizer, SentencePieceBPETokenizer, SentencePieceUnigramTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    #tok_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")\n",
    "    #tok_dataset = load_dataset(\"glue\", name=\"sst2\", split=\"train\")\n",
    "    #tok_dataset = tok_dataset.rename_column(\"sentence\", \"text\")\n",
    "    \n",
    "    tokenizer = SentencePieceUnigramTokenizer()\n",
    "\n",
    "    tokenizer.train_from_iterator(\n",
    "        iterator=tok_dataset[\"text\"], \n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        #min_frequency=2,\n",
    "        show_progress=True,\n",
    "        #limit_alphabet=500,\n",
    "        special_tokens=[\n",
    "            \"<PAD>\", \n",
    "            \"<UNK>\", \n",
    "            \"<CLS>\", \n",
    "            \"<SEP>\", \n",
    "            \"<DOC>\",\n",
    "            \"<MASK>\"\n",
    "        ])\n",
    "\n",
    "    tokenizer = PreTrainedTokenizer(\n",
    "        tokenizer_object=tokenizer\n",
    "    )\n",
    "    tokenizer.save_model(SENTENCE_PIECE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(SENTENCE_PIECE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(tok_dataset), TRAIN_BATCH_SIZE):\n",
    "        yield tok_dataset[i : i + TRAIN_BATCH_SIZE][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"gpt2\", \n",
    "    #    vocab_size=VOCAB_SIZE - 2,\n",
    "    #    pad_token=\"<|pad|>\",\n",
    "    #    unk_token=\"<|unk|>\",\n",
    "    #    bos_token=\"<|doc|>\",\n",
    "    #    eos_token=\"<|udoc|>\",\n",
    "    )\n",
    "    #tokenizer.eos_token_id = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 50257)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    old_tokenizer = transformers.AutoTokenizer.from_pretrained(\"AdithyaSK/LLama3Tokenizer\", token=ACCESS_TOKEN)\n",
    "    tokenizer = old_tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True,\n",
    "                                        strip_accents=True, lowercase=True)\n",
    "    #tokenizer = transformers.LlamaTokenizer()\n",
    "\n",
    "    tokenizer.train_from_iterator(iterator=tok_dataset[\"text\"], vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[\n",
    "        \"[PAD]\", \n",
    "        \"[UNK]\", \n",
    "        \"[CLS]\", \n",
    "        \"[SEP]\", \n",
    "        \"[DOC]\",\n",
    "    #    \"[UDOC]\",\n",
    "        \"[MASK]\"\n",
    "    ])\n",
    "    tokenizer.save_model(WORDPIECE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    tokenizer = BertTokenizer.from_pretrained(WORDPIECE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "    \n",
    "    tokenizer.train_from_iterator(iterator=tok_dataset[\"text\"], vocab_size=VOCAB_SIZE, min_frequency=2, length=MAX_POSITION_EMBEDDINGS, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<doc>\",\n",
    "        \"<mask>\",\n",
    "        \n",
    "        #\"<pad>\",\n",
    "        #\"<unk>\",\n",
    "        #\"<cls>\",\n",
    "        #\"<sep>\",\n",
    "        #\"<doc>\",\n",
    "        #\"<mask>\",\n",
    "    ])\n",
    "\n",
    "    # Save files to disk\n",
    "    tokenizer.save_model(BPE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(BPE_TOKENIZER_DIR)\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(BPE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def encode_and_batch(dataset, tokenizer, max_position_embeddings, batch_size, shuffle=False):\n",
    "    if DATASET == \"inbio_mask\":\n",
    "        encoded = preprocess_for_key_masking(mask_keys, dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names, to_mask=.15, chance_rand_token=.2, \n",
    "                                          group_texts=GROUP_TEXTS, pad_token=PAD_TOKEN, sparsify=SPARSIFY, prefix=PREFIX, switch_ii_decoder_ii=SWITCH_II_DECODER_II)\n",
    "    elif DATASET in (\"translation_mcca\"):\n",
    "        encoded = preprocess_for_translation(dataset, tokenizer, max_position_embeddings, source_lang=\"src\", target_lang=\"tgt\", prefix=PREFIX, num_proc=4, remove_columns=[\"src\", \"tgt\"], switch_ii_decoder_ii=SWITCH_II_DECODER_II)\n",
    "    elif DATASET in (\"wikitext_ppl\"):\n",
    "        encoded = preprocess_for_causallm(\n",
    "            dataset, \n",
    "            tokenizer, \n",
    "            block_size=MAX_POSITION_EMBEDDINGS, \n",
    "            remove_columns=dataset.column_names, \n",
    "            shift_right=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            doc_token_id=tokenizer.bos_token_id,\n",
    "            udoc_token_id=tokenizer.eos_token_id,    \n",
    "        )\n",
    "    elif DATASET in (\"wikitext_mlm\"):\n",
    "        encoded = preprocess_for_maskedlm(\n",
    "            dataset, \n",
    "            tokenizer, \n",
    "            max_position_embeddings, \n",
    "            remove_columns=dataset.column_names, \n",
    "            to_mask=.15, \n",
    "            chance_rand_token=.2, \n",
    "            group_texts=GROUP_TEXTS, \n",
    "            mask_token=MASK_TOKEN, \n",
    "            pad_token=PAD_TOKEN, \n",
    "            sparsify=SPARSIFY, \n",
    "            prefix=PREFIX, \n",
    "            switch_ii_decoder_ii=SWITCH_II_DECODER_II\n",
    "        )\n",
    "        \n",
    "    print(encoded)\n",
    "    batched = BatchBuffer(encoded, batch_size)\n",
    "    if shuffle:\n",
    "        batched.shuffle()\n",
    "    print(\"  finished\")\n",
    "    return batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Robert Boulter = \\n'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  10%|█         | 1000/10000 [00:00<00:02, 3665.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  50%|█████     | 5000/10000 [00:00<00:00, 14943.38 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  90%|█████████ | 9000/10000 [00:00<00:00, 22132.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 10000/10000 [00:00<00:00, 16060.46 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'group_mask', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
      "    num_rows: 1320\n",
      "})\n",
      "  finished\n",
      "Dataset({\n",
      "    features: ['input_ids', 'group_mask', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
      "    num_rows: 557\n",
      "})\n",
      "  finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "train_loader_call = lambda: encode_and_batch(train_dataset, tokenizer, MAX_POSITION_EMBEDDINGS, TRAIN_BATCH_SIZE, True)\n",
    "test_loader_call = lambda: encode_and_batch(test_dataset, tokenizer, MAX_POSITION_EMBEDDINGS, TEST_BATCH_SIZE)\n",
    "\n",
    "if not REBATCH:\n",
    "    train_dataloader = train_loader_call()\n",
    "    test_dataloader = test_loader_call()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.ds[0][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.ds[0][\"group_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#encoded_train_dataset = preprocess_for_maskedlm(dataset, tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=train_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#train_dataloader = BatchBuffer(encoded_dataset[\"train\"], BATCH_SIZE).shuffle()\n",
    "#test_dataloader = BatchBuffer(encoded_dataset[\"test\"], BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#batch_schema = list(encoded_dataset[\"train\"].features.keys())\n",
    "#batch_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def count_item(inp, item):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for n in inp:\n",
    "        for r in n:\n",
    "            i = r\n",
    "            if not i < 4:\n",
    "                total += 1\n",
    "            if i == item:\n",
    "                count += 1\n",
    "            #if i != 0 and i != item:\n",
    "            #    print(i)\n",
    "    return f\"{count} / {total} ; {count/total}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked tokens [input_ids]: 0 / 675840 ; 0.0\n",
      "masked tokens [labels]: 0 / 285184 ; 0.0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"masked tokens [input_ids]:\", count_item(train_dataloader.ds[\"input_ids\"], tokenizer.mask_token_id))\n",
    "print(\"masked tokens [labels]:\", count_item(test_dataloader.ds[\"labels\"], tokenizer.mask_token_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "warmup_steps = math.ceil(total_steps * 0.05)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'group_mask', 'attention_mask', 'decoder_input_ids', 'labels']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "train_dataloader.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'group_mask', 'attention_mask', 'decoder_input_ids', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(train_dataloader.schema)\n",
    "its = 0\n",
    "for i, n in enumerate(train_dataloader):\n",
    "    if i >= its:\n",
    "        break\n",
    "    for k in n:\n",
    "        print(i, \"############\")\n",
    "        for l in k:\n",
    "            print(len(l))\n",
    "        print(\"##############\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COINForCausalLM(\n",
       "  (coin): COINModel(\n",
       "    (encoder_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(50257, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (decoder_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(50257, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (sampler): COINSampler(\n",
       "      (layers): COINMultiLayerBlock(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x COINLayer(\n",
       "            (block): i4Block(\n",
       "              (Ub_inner): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (Wb_r): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (Ub_r): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (Wb_z): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (Ub_z): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (rope): RotaryEmbedding()\n",
       "              (gate_act): Sigmoid()\n",
       "              (act): ReLU()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (group_norm): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "              (fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (cope): CoPE()\n",
       "              (mixer): TranspositionMixer(\n",
       "                (fi): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (fc): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "              )\n",
       "              (Wb_h): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (Ub_h): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (rms): RMSNorm()\n",
       "            (post_rms): RMSNorm()\n",
       "            (skip_gate): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "            (glu): GLU(\n",
       "              (fi): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (fc): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (rglu): RGLU(\n",
       "              (fz): Linear(in_features=1024, out_features=1536, bias=True)\n",
       "              (fr): Linear(in_features=1024, out_features=1536, bias=True)\n",
       "              (fc): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "              (fh): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "            (mixer): TranspositionMixer(\n",
       "              (fi): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (fc): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (loss_fn): CrossEntropyLoss()\n",
       "      )\n",
       "    )\n",
       "    (pooler): COINPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (act): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=100_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 25 ========\n",
      "\n",
      "Generated batch schema as ['input_ids', 'group_mask', 'attention_mask', 'decoder_input_ids', 'labels']\n",
      "\n",
      "Training...\n",
      "  Batch     8  of    440.    Elapsed:  0:00:03, Remaining:  0:02:42.\n",
      "  Batch    16  of    440.    Elapsed:  0:00:05, Remaining:  0:01:46.\n",
      "  Batch    24  of    440.    Elapsed:  0:00:07, Remaining:  0:01:44.\n",
      "  Batch    32  of    440.    Elapsed:  0:00:10, Remaining:  0:01:42.\n",
      "  Batch    40  of    440.    Elapsed:  0:00:12, Remaining:  0:01:40.\n",
      "  Batch    48  of    440.    Elapsed:  0:00:14, Remaining:  0:01:38.\n",
      "  Batch    56  of    440.    Elapsed:  0:00:17, Remaining:  0:01:36.\n",
      "  Batch    64  of    440.    Elapsed:  0:00:19, Remaining:  0:01:34.\n",
      "  Batch    72  of    440.    Elapsed:  0:00:22, Remaining:  0:01:32.\n",
      "  Batch    80  of    440.    Elapsed:  0:00:24, Remaining:  0:01:30.\n",
      "  Batch    88  of    440.    Elapsed:  0:00:26, Remaining:  0:01:28.\n",
      "  Batch    96  of    440.    Elapsed:  0:00:29, Remaining:  0:01:26.\n",
      "  Batch   104  of    440.    Elapsed:  0:00:31, Remaining:  0:01:24.\n",
      "  Batch   112  of    440.    Elapsed:  0:00:33, Remaining:  0:01:22.\n",
      "  Batch   120  of    440.    Elapsed:  0:00:36, Remaining:  0:01:20.\n",
      "  Batch   128  of    440.    Elapsed:  0:00:38, Remaining:  0:01:18.\n",
      "  Batch   136  of    440.    Elapsed:  0:00:41, Remaining:  0:01:16.\n",
      "  Batch   144  of    440.    Elapsed:  0:00:43, Remaining:  0:01:14.\n",
      "  Batch   152  of    440.    Elapsed:  0:00:45, Remaining:  0:01:12.\n",
      "  Batch   160  of    440.    Elapsed:  0:00:48, Remaining:  0:01:10.\n",
      "  Batch   168  of    440.    Elapsed:  0:00:50, Remaining:  0:01:08.\n",
      "  Batch   176  of    440.    Elapsed:  0:00:52, Remaining:  0:01:06.\n",
      "  Batch   184  of    440.    Elapsed:  0:00:55, Remaining:  0:01:04.\n",
      "  Batch   192  of    440.    Elapsed:  0:00:57, Remaining:  0:01:02.\n",
      "  Batch   200  of    440.    Elapsed:  0:01:00, Remaining:  0:01:00.\n",
      "  Batch   208  of    440.    Elapsed:  0:01:02, Remaining:  0:00:58.\n",
      "  Batch   216  of    440.    Elapsed:  0:01:04, Remaining:  0:00:56.\n",
      "  Batch   224  of    440.    Elapsed:  0:01:07, Remaining:  0:00:54.\n",
      "  Batch   232  of    440.    Elapsed:  0:01:09, Remaining:  0:00:52.\n",
      "  Batch   240  of    440.    Elapsed:  0:01:11, Remaining:  0:00:50.\n",
      "  Batch   248  of    440.    Elapsed:  0:01:14, Remaining:  0:00:48.\n",
      "  Batch   256  of    440.    Elapsed:  0:01:16, Remaining:  0:00:46.\n",
      "  Batch   264  of    440.    Elapsed:  0:01:18, Remaining:  0:00:44.\n",
      "  Batch   272  of    440.    Elapsed:  0:01:21, Remaining:  0:00:42.\n",
      "  Batch   280  of    440.    Elapsed:  0:01:23, Remaining:  0:00:40.\n",
      "  Batch   288  of    440.    Elapsed:  0:01:26, Remaining:  0:00:38.\n",
      "  Batch   296  of    440.    Elapsed:  0:01:28, Remaining:  0:00:36.\n",
      "  Batch   304  of    440.    Elapsed:  0:01:30, Remaining:  0:00:34.\n",
      "  Batch   312  of    440.    Elapsed:  0:01:33, Remaining:  0:00:32.\n",
      "  Batch   320  of    440.    Elapsed:  0:01:35, Remaining:  0:00:30.\n",
      "  Batch   328  of    440.    Elapsed:  0:01:37, Remaining:  0:00:28.\n",
      "  Batch   336  of    440.    Elapsed:  0:01:40, Remaining:  0:00:26.\n",
      "  Batch   344  of    440.    Elapsed:  0:01:42, Remaining:  0:00:24.\n",
      "  Batch   352  of    440.    Elapsed:  0:01:44, Remaining:  0:00:22.\n",
      "  Batch   360  of    440.    Elapsed:  0:01:47, Remaining:  0:00:20.\n",
      "  Batch   368  of    440.    Elapsed:  0:01:49, Remaining:  0:00:18.\n",
      "  Batch   376  of    440.    Elapsed:  0:01:52, Remaining:  0:00:16.\n",
      "  Batch   384  of    440.    Elapsed:  0:01:54, Remaining:  0:00:14.\n",
      "  Batch   392  of    440.    Elapsed:  0:01:56, Remaining:  0:00:12.\n",
      "  Batch   400  of    440.    Elapsed:  0:01:59, Remaining:  0:00:10.\n",
      "  Batch   408  of    440.    Elapsed:  0:02:01, Remaining:  0:00:08.\n",
      "  Batch   416  of    440.    Elapsed:  0:02:03, Remaining:  0:00:06.\n",
      "  Batch   424  of    440.    Elapsed:  0:02:06, Remaining:  0:00:04.\n",
      "  Batch   432  of    440.    Elapsed:  0:02:08, Remaining:  0:00:02.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 10.706584278019992\n",
      "    aux_loss: 0.0\n",
      "  Training epoch took: 0:02:10\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 10.400093934342667\n",
      "    perplexity: 33061.18155956411\n",
      "    aux_loss: 0.0\n",
      "  Testing took: 0:01:52\n",
      "[Errno 17] File exists: 'benchmark_models/ppl_play/10k_play//model//epoch_0/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 2 / 25 ========\n",
      "\n",
      "Training...\n",
      "  Batch     8  of    440.    Elapsed:  0:00:02, Remaining:  0:01:48.\n",
      "  Batch    16  of    440.    Elapsed:  0:00:05, Remaining:  0:01:46.\n",
      "  Batch    24  of    440.    Elapsed:  0:00:07, Remaining:  0:01:44.\n",
      "  Batch    32  of    440.    Elapsed:  0:00:09, Remaining:  0:01:42.\n",
      "  Batch    40  of    440.    Elapsed:  0:00:12, Remaining:  0:01:40.\n",
      "  Batch    48  of    440.    Elapsed:  0:00:14, Remaining:  0:01:38.\n",
      "  Batch    56  of    440.    Elapsed:  0:00:16, Remaining:  0:01:36.\n",
      "  Batch    64  of    440.    Elapsed:  0:00:19, Remaining:  0:01:34.\n",
      "  Batch    72  of    440.    Elapsed:  0:00:21, Remaining:  0:01:32.\n",
      "  Batch    80  of    440.    Elapsed:  0:00:23, Remaining:  0:01:30.\n",
      "  Batch    88  of    440.    Elapsed:  0:00:26, Remaining:  0:01:28.\n",
      "  Batch    96  of    440.    Elapsed:  0:00:28, Remaining:  0:01:26.\n",
      "  Batch   104  of    440.    Elapsed:  0:00:30, Remaining:  0:01:24.\n",
      "  Batch   112  of    440.    Elapsed:  0:00:33, Remaining:  0:01:22.\n",
      "  Batch   120  of    440.    Elapsed:  0:00:35, Remaining:  0:01:20.\n",
      "  Batch   128  of    440.    Elapsed:  0:00:37, Remaining:  0:01:18.\n",
      "  Batch   136  of    440.    Elapsed:  0:00:40, Remaining:  0:01:16.\n",
      "  Batch   144  of    440.    Elapsed:  0:00:42, Remaining:  0:01:14.\n",
      "  Batch   152  of    440.    Elapsed:  0:00:45, Remaining:  0:01:12.\n",
      "  Batch   160  of    440.    Elapsed:  0:00:47, Remaining:  0:01:10.\n",
      "  Batch   168  of    440.    Elapsed:  0:00:49, Remaining:  0:01:08.\n",
      "  Batch   176  of    440.    Elapsed:  0:00:52, Remaining:  0:01:06.\n",
      "  Batch   184  of    440.    Elapsed:  0:00:54, Remaining:  0:01:04.\n",
      "  Batch   192  of    440.    Elapsed:  0:00:56, Remaining:  0:01:02.\n",
      "  Batch   200  of    440.    Elapsed:  0:00:59, Remaining:  0:01:00.\n",
      "  Batch   208  of    440.    Elapsed:  0:01:01, Remaining:  0:00:58.\n",
      "  Batch   216  of    440.    Elapsed:  0:01:04, Remaining:  0:00:56.\n",
      "  Batch   224  of    440.    Elapsed:  0:01:06, Remaining:  0:00:54.\n",
      "  Batch   232  of    440.    Elapsed:  0:01:08, Remaining:  0:00:52.\n",
      "  Batch   240  of    440.    Elapsed:  0:01:11, Remaining:  0:00:50.\n",
      "  Batch   248  of    440.    Elapsed:  0:01:13, Remaining:  0:00:48.\n",
      "  Batch   256  of    440.    Elapsed:  0:01:15, Remaining:  0:00:46.\n",
      "  Batch   264  of    440.    Elapsed:  0:01:18, Remaining:  0:00:44.\n",
      "  Batch   272  of    440.    Elapsed:  0:01:20, Remaining:  0:00:42.\n",
      "  Batch   280  of    440.    Elapsed:  0:01:23, Remaining:  0:00:40.\n",
      "  Batch   288  of    440.    Elapsed:  0:01:25, Remaining:  0:00:38.\n",
      "  Batch   296  of    440.    Elapsed:  0:01:27, Remaining:  0:00:36.\n",
      "  Batch   304  of    440.    Elapsed:  0:01:30, Remaining:  0:00:34.\n",
      "  Batch   312  of    440.    Elapsed:  0:01:32, Remaining:  0:00:32.\n",
      "  Batch   320  of    440.    Elapsed:  0:01:34, Remaining:  0:00:30.\n",
      "  Batch   328  of    440.    Elapsed:  0:01:37, Remaining:  0:00:28.\n",
      "  Batch   336  of    440.    Elapsed:  0:01:39, Remaining:  0:00:26.\n",
      "  Batch   344  of    440.    Elapsed:  0:01:41, Remaining:  0:00:24.\n",
      "  Batch   352  of    440.    Elapsed:  0:01:44, Remaining:  0:00:22.\n",
      "  Batch   360  of    440.    Elapsed:  0:01:46, Remaining:  0:00:20.\n",
      "  Batch   368  of    440.    Elapsed:  0:01:49, Remaining:  0:00:18.\n",
      "  Batch   376  of    440.    Elapsed:  0:01:51, Remaining:  0:00:16.\n",
      "  Batch   384  of    440.    Elapsed:  0:01:53, Remaining:  0:00:14.\n",
      "  Batch   392  of    440.    Elapsed:  0:01:56, Remaining:  0:00:12.\n",
      "  Batch   400  of    440.    Elapsed:  0:01:58, Remaining:  0:00:10.\n",
      "  Batch   408  of    440.    Elapsed:  0:02:00, Remaining:  0:00:08.\n",
      "  Batch   416  of    440.    Elapsed:  0:02:03, Remaining:  0:00:06.\n",
      "  Batch   424  of    440.    Elapsed:  0:02:05, Remaining:  0:00:04.\n",
      "  Batch   432  of    440.    Elapsed:  0:02:08, Remaining:  0:00:02.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 9.943044304847717\n",
      "    aux_loss: 0.0\n",
      "  Training epoch took: 0:02:10\n",
      "\n",
      "Running Test 1 ...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stats = train_bern_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    #train_dataloader,\n",
    "    #test_dataloader,\n",
    "    #batch_schema,\n",
    "    device,\n",
    "    loss_function,\n",
    "    id2label,\n",
    "    train_dataloader=train_dataloader if not REBATCH else None,\n",
    "    test_dataloader=test_dataloader if not REBATCH else None,\n",
    "    create_train_dataloader=train_loader_call if REBATCH else None,\n",
    "    create_test_dataloader=test_loader_call if REBATCH else None,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    print_status=True,\n",
    "    is_hf_model=IS_HF_MODEL,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    only_save_core=False,\n",
    "    epoch_i=EPOCH_I,\n",
    "    is_encoder_decoder_model=IS_ENCODER_DECODER_MODEL,\n",
    "    causal_lm=CAUSAL_LM,\n",
    "\n",
    "    forward_args=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    #forward_args=[\"input_ids\", \"decoder_input_ids\", \"group_mask\", \"labels\"],\n",
    "\n",
    "    masked_lm_task=True,\n",
    "    electra_task=False,\n",
    "    mlm_decode_n=0,\n",
    "    #mlm_decode_n=.0075,\n",
    "    #mlm_decode_n=.1,\n",
    "    mlm_decode_max_chars=200,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    dump_coin_regions=False,\n",
    "    generic_output_class=True,\n",
    "    #coin_region_lambda=lambda model: model.coin.core.regions\n",
    "\n",
    "    evaluate_autoregressively=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train:\n",
      "    loss: 10.622668109169268\n",
      "  Test:\n",
      "    loss: 9.689842956130569\n",
      "    perplexity: 16375.289287004804\n",
      "\n",
      "Epoch 2:\n",
      "  Train:\n",
      "    loss: 8.951763831778031\n",
      "  Test:\n",
      "    loss: 8.411100485518173\n",
      "    perplexity: 4574.336782624653\n",
      "\n",
      "Epoch 3:\n",
      "  Train:\n",
      "    loss: 7.877769417839137\n",
      "  Test:\n",
      "    loss: 7.55449881424775\n",
      "    perplexity: 1969.7468266255503\n",
      "\n",
      "Epoch 4:\n",
      "  Train:\n",
      "    loss: 7.267687891386194\n",
      "  Test:\n",
      "    loss: 7.14455542693267\n",
      "    perplexity: 1314.0777654034669\n",
      "\n",
      "Epoch 5:\n",
      "  Train:\n",
      "    loss: 6.962168479675014\n",
      "  Test:\n",
      "    loss: 6.94379976375683\n",
      "    perplexity: 1078.4973929683547\n",
      "\n",
      "Epoch 6:\n",
      "  Train:\n",
      "    loss: 6.7870453075085955\n",
      "  Test:\n",
      "    loss: 6.82009051297162\n",
      "    perplexity: 955.021693094451\n",
      "\n",
      "Epoch 7:\n",
      "  Train:\n",
      "    loss: 6.655083815620475\n",
      "  Test:\n",
      "    loss: 6.7224352604634054\n",
      "    perplexity: 867.798382941681\n",
      "\n",
      "Epoch 8:\n",
      "  Train:\n",
      "    loss: 6.534788072791198\n",
      "  Test:\n",
      "    loss: 6.637134670566868\n",
      "    perplexity: 797.8378715540773\n",
      "\n",
      "Epoch 9:\n",
      "  Train:\n",
      "    loss: 6.415604601190074\n",
      "  Test:\n",
      "    loss: 6.560382397110398\n",
      "    perplexity: 739.5352036213219\n",
      "\n",
      "Epoch 10:\n",
      "  Train:\n",
      "    loss: 6.294807916374992\n",
      "  Test:\n",
      "    loss: 6.49417970244949\n",
      "    perplexity: 692.9593975744328\n",
      "\n",
      "Epoch 11:\n",
      "  Train:\n",
      "    loss: 6.181252631224538\n",
      "  Test:\n",
      "    loss: 6.453516906016581\n",
      "    perplexity: 666.8243514122471\n",
      "\n",
      "Epoch 12:\n",
      "  Train:\n",
      "    loss: 6.074976801053883\n",
      "  Test:\n",
      "    loss: 6.425019756523339\n",
      "    perplexity: 648.0561270171629\n",
      "\n",
      "Epoch 13:\n",
      "  Train:\n",
      "    loss: 5.9680276669953995\n",
      "  Test:\n",
      "    loss: 6.401053279155009\n",
      "    perplexity: 633.3556859412458\n",
      "\n",
      "Epoch 14:\n",
      "  Train:\n",
      "    loss: 5.867036266239611\n",
      "  Test:\n",
      "    loss: 6.390338145075618\n",
      "    perplexity: 627.0867886901076\n",
      "\n",
      "Epoch 15:\n",
      "  Train:\n",
      "    loss: 5.7698061548054085\n",
      "  Test:\n",
      "    loss: 6.373400750031342\n",
      "    perplexity: 617.867637481831\n",
      "\n",
      "Epoch 16:\n",
      "  Train:\n",
      "    loss: 5.678072262683231\n",
      "  Test:\n",
      "    loss: 6.337846070366937\n",
      "    perplexity: 598.1999465121178\n",
      "\n",
      "Epoch 17:\n",
      "  Train:\n",
      "    loss: 5.58436417961557\n",
      "  Test:\n",
      "    loss: 6.347892493170661\n",
      "    perplexity: 604.7700942777974\n",
      "\n",
      "Epoch 18:\n",
      "  Train:\n",
      "    loss: 5.486700786878642\n",
      "  Test:\n",
      "    loss: 6.347349401422449\n",
      "    perplexity: 604.2897028522651\n",
      "\n",
      "Epoch 19:\n",
      "  Train:\n",
      "    loss: 5.3935169617153145\n",
      "  Test:\n",
      "    loss: 6.362635664037756\n",
      "    perplexity: 614.3875938258115\n",
      "\n",
      "Epoch 20:\n",
      "  Train:\n",
      "    loss: 5.30118457458112\n",
      "  Test:\n",
      "    loss: 6.423785328220677\n",
      "    perplexity: 656.1657429997371\n",
      "\n",
      "Epoch 21:\n",
      "  Train:\n",
      "    loss: 5.205352022664225\n",
      "  Test:\n",
      "    loss: 6.44606816317584\n",
      "    perplexity: 674.1457207794024\n",
      "\n",
      "Epoch 22:\n",
      "  Train:\n",
      "    loss: 5.1138222986828\n",
      "  Test:\n",
      "    loss: 6.456022930145264\n",
      "    perplexity: 682.3844420115846\n",
      "\n",
      "Epoch 23:\n",
      "  Train:\n",
      "    loss: 5.019247112885219\n",
      "  Test:\n",
      "    loss: 6.464602197183145\n",
      "    perplexity: 688.810252686679\n",
      "\n",
      "Epoch 24:\n",
      "  Train:\n",
      "    loss: 4.918151297885712\n",
      "  Test:\n",
      "    loss: 6.495541917955554\n",
      "    perplexity: 710.4167424630459\n",
      "\n",
      "Epoch 25:\n",
      "  Train:\n",
      "    loss: 4.812623675409662\n",
      "  Test:\n",
      "    loss: 6.511386582658098\n",
      "    perplexity: 722.7301171042916\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_stat_tuples(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
