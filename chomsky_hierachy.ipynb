{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import transformers\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from load_set import *\n",
    "import model_training\n",
    "from model_training import train_bern_model\n",
    "import coin_i3C_modeling as ci3C\n",
    "from rnn_modeling import *\n",
    "from ntm import *\n",
    "from lnn_modeling import *\n",
    "from pcoin_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "MAX_POSITION_EMBEDDINGS = 20\n",
    "TEST_POSITION_EMBEDDINGS = 200\n",
    "VOCAB_SIZE = 11#MAX_POSITION_EMBEDDINGS\n",
    "NUM_LABELS = 5\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "NUM_TRAIN_SAMPLES = 2_500_000 #256 * 380\n",
    "NUM_WARMUP_STEPS = 1_500_000\n",
    "SAMPLE_METHOD = \"static-warmup\"\n",
    "\n",
    "#NUM_TRAIN_SAMPLES = 2_500_000\n",
    "#NUM_WARMUP_STEPS = 1_500_000\n",
    "#SAMPLE_METHOD = \"linspace\"\n",
    "\n",
    "#SAMPLE_METHOD = \"uniform\"\n",
    "\n",
    "#NUM_TRAIN_SAMPLES = 50_000\n",
    "#SAMPLE_METHOD = \"static\"\n",
    "\n",
    "NUM_TEST_SAMPLES = 1000\n",
    "LEARNING_RATE = 1e-4#1e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_POSITION_EMBEDDINGS = max(MAX_POSITION_EMBEDDINGS, TEST_POSITION_EMBEDDINGS) # hotfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500000, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TRAIN_SAMPLES, NUM_TEST_SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket-sort, duplicate-string, parity-check, missing-duplicate-string, binary-addition, binary-sqrt, modular-arithmetic\n",
    "#TASK = \"duplicate-string\"\n",
    "#TASK = \"missing-duplicate-string\"\n",
    "TASK = \"parity-check\"\n",
    "#TASK = \"bucket-sort\"\n",
    "#TASK = \"binary-addition\"\n",
    "#TASK = \"binary-sqrt\"\n",
    "#TASK = \"modular-arithmetic\"\n",
    "#TASK = \"modular-arithmetic-brackets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK in (\"parity-check\", \"binary-sqrt\"):\n",
    "    VOCAB_SIZE = 2\n",
    "    NUM_LABELS = 2\n",
    "elif TASK in (\"missing-duplicate-string\", \"binary-addition\"):\n",
    "    VOCAB_SIZE = 3\n",
    "    NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    model = pCOINForSequenceClassification(\n",
    "        config=pCOINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "            one_hot_encoding=True,\n",
    "            chunk_schema=[\"1\", \"T / 2\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = NTMForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.0,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            intermediate_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # hidden_size = units = backbone_units\n",
    "    model = CfCForParityCheck(\n",
    "        CfCConfig(\n",
    "            input_size = 256,\n",
    "            hidden_size = 256,\n",
    "            sparsity_mask = None,\n",
    "            backbone_layers = 1,\n",
    "            backbone_units = 256,\n",
    "            backbone_dropout = 0.0,\n",
    "            mode = \"default\",\n",
    "            units = 256,\n",
    "            proj_size = 256,\n",
    "            wiring = None,\n",
    "            backbone_activation = \"tanh\",\n",
    "            mixed_memory = False,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = sLSTMForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.0,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            intermediate_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = COINForBucketSort(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            intermediate_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "            layer_norm_eps=1e-12,\n",
    "            carry_over_S=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    l_size = 32\n",
    "    model = CfCForBucketSort(\n",
    "        config=CfCConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            #max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            #num_hidden_layers=1,\n",
    "            input_size=l_size,\n",
    "            hidden_size=l_size,\n",
    "            units=l_size,\n",
    "            backbone_units=l_size,\n",
    "            #intermediate_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "            #layer_norm_eps=1e-12\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    l_size = 32\n",
    "    model = LTCForBucketSort(\n",
    "        config=LTCConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            #max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            #num_hidden_layers=1,\n",
    "            input_size=l_size,\n",
    "            hidden_size=l_size,\n",
    "            units=l_size,\n",
    "            #intermediate_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "            #layer_norm_eps=1e-12\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = COINForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            intermediate_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "            layer_norm_eps=1e-12,\n",
    "            carry_over_S=False,\n",
    "            chunk_schema=[ lambda T: 1]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = RNNForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = StackRNNForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.5,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # B=50\n",
    "    # T_train=20\n",
    "    # T_test= >500\n",
    "    class LSTMForParityCheck(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.n_layers = config.num_hidden_layers\n",
    "            #self.lstm = nn.LSTM(2, config.hidden_size, batch_first=True, num_layers=self.n_layers, dropout=config.hidden_dropout_prob)\n",
    "            self.lstm = nn.LSTM(config.hidden_size, config.hidden_size, batch_first=True, num_layers=self.n_layers, dropout=config.hidden_dropout_prob)\n",
    "            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "            self.classifier = nn.Sequential(\n",
    "                #nn.Dropout(config.hidden_dropout_prob, \n",
    "                nn.Linear(config.hidden_size, config.num_labels)\n",
    "            )\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            logits = F.one_hot(input_ids.long(), self.config.vocab_size).float()\n",
    "            logits = self.embeddings(logits)\n",
    "            B, T, C = logits.shape\n",
    "            hidden = (torch.randn(self.n_layers, B, C, device=logits.device), torch.randn(self.n_layers, B, C, device=logits.device))\n",
    "            logits, hidden = self.lstm(logits)\n",
    "            r_logits = self.classifier(logits[:, -1])\n",
    "            loss = self.loss_fn(r_logits, labels)\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=r_logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "    model = LSTMForParityCheck(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.5,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    class BertForBucketSort(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.bert = transformers.BertModel(config)\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            emb = self.embeddings(F.one_hot(input_ids.long(), self.config.vocab_size).float())\n",
    "            #print(input_ids.shape, emb.shape)\n",
    "            #emb = F.one_hot(input_ids.long(), self.config.vocab_size).float()\n",
    "            B, T, C = emb.shape\n",
    "            logits = self.bert(inputs_embeds=emb, attention_mask=attention_mask).last_hidden_state\n",
    "            \n",
    "            #logits = logits[:, -math.ceil(T / 2):]\n",
    "            \n",
    "            logits = self.lm_head(logits)\n",
    "            labels = labels.long()\n",
    "            \n",
    "            loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "\n",
    "    model = BertForBucketSort(\n",
    "        config=transformers.BertConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=max(MAX_POSITION_EMBEDDINGS, TEST_POSITION_EMBEDDINGS)+1,\n",
    "            num_hidden_layers=2,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_attention_heads=1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    class BertForModularArithmetic(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.bert = transformers.BertForSequenceClassification(config)\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            emb = F.one_hot(input_ids.long(), self.config.vocab_size).float()\n",
    "            emb = self.embeddings(emb)\n",
    "            logits = self.bert(inputs_embeds=emb, attention_mask=attention_mask).logits\n",
    "            #logits = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            #logits = self.classifier(logits[:, 0, :])\n",
    "            \n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "    model = BertForModularArithmetic(\n",
    "        config=transformers.BertConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_attention_heads=1,\n",
    "            num_labels=5,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    class BertForParityCheck(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            #self.bert = transformers.BertModel(config)\n",
    "            self.bert = transformers.BertForSequenceClassification(config)\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            emb = F.one_hot(input_ids.long(), self.config.vocab_size).float()\n",
    "            emb = self.embeddings(emb)\n",
    "            logits = self.bert(inputs_embeds=emb, attention_mask=attention_mask).logits\n",
    "            #logits = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            #logits = self.classifier(logits[:, 0, :])\n",
    "            \n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "\n",
    "    model = BertForParityCheck(\n",
    "        config=transformers.BertConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_attention_heads=1,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    class LSTMForSequenceClassification(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.lstm = nn.LSTM(\n",
    "                config.hidden_size, \n",
    "                config.hidden_size, \n",
    "                config.num_hidden_layers, \n",
    "                batch_first=True,\n",
    "                dropout=config.hidden_dropout_prob\n",
    "            )\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.classifier = nn.Sequential(\n",
    "                #nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                #nn.Tanh(),\n",
    "                nn.Linear(config.hidden_size, config.num_labels)\n",
    "            )\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            emb = F.one_hot(input_ids.long(), self.config.vocab_size).float()\n",
    "            emb = self.embeddings(emb)\n",
    "            logits, (h_n, c_n) = self.lstm(emb)\n",
    "            h_n = h_n.transpose(0, 1)\n",
    "            c_n = c_n.transpose(0, 1)\n",
    "            logits = self.classifier(logits[:, 0, :])\n",
    "            #logits = self.classifier(h_n)\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return Output(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "    model = LSTMForSequenceClassification(\n",
    "        config=RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample schema: tensor([ 1,  1,  1,  ..., 20, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "accuracy_mask = None\n",
    "if TASK == \"bucket-sort\":\n",
    "    gen_fn = generate_bucket_sort_set\n",
    "elif TASK == \"duplicate-string\":\n",
    "    gen_fn = generate_duplicate_string_set\n",
    "elif TASK == \"parity-check\":\n",
    "    gen_fn = generate_parity_check_set\n",
    "elif TASK == \"missing-duplicate-string\":\n",
    "    gen_fn = generate_missing_duplicate_string_set\n",
    "elif TASK == \"binary-addition\":\n",
    "    gen_fn = generate_binary_addition_set\n",
    "    accuracy_mask = binary_addition_mask\n",
    "elif TASK == \"binary-sqrt\":\n",
    "    gen_fn = generate_binary_sqrt_set\n",
    "    accuracy_mask = binary_sqrt_mask\n",
    "elif TASK == \"modular-arithmetic\":\n",
    "    gen_fn = generate_modular_arithmetic_set\n",
    "    #gen_fn = generate_str_modular_arithmetic_set\n",
    "elif TASK == \"modular-arithmetic-brackets\":\n",
    "    gen_fn = generate_modular_arithmetic_brackets_set\n",
    "\n",
    "#train_buf = generate_uniform_batches(gen_fn, B=BATCH_SIZE, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE, num_samples=NUM_TRAIN_SAMPLES)\n",
    "#test_buf = generate_uniform_batches(gen_fn, B=BATCH_SIZE, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE, num_samples=NUM_TEST_SAMPLES)\n",
    "train_buf, test_buf = generate_ch_batches(\n",
    "    generate_set_fn=gen_fn, \n",
    "    B=BATCH_SIZE, \n",
    "    T_train=MAX_POSITION_EMBEDDINGS, \n",
    "    T_test=TEST_POSITION_EMBEDDINGS, \n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    num_train_samples=NUM_TRAIN_SAMPLES, \n",
    "    num_test_samples=NUM_TEST_SAMPLES,\n",
    "    sample_method=SAMPLE_METHOD,\n",
    "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_buf, test_buf = generate_static_parity_check_set(BATCH_SIZE, MAX_POSITION_EMBEDDINGS, TEST_POSITION_EMBEDDINGS, NUM_TRAIN_SAMPLES, NUM_TEST_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SCHEMA = [\"input_ids\", \"decoder_input_ids\", \"attention_mask\", \"labels\"]\n",
    "#BATCH_SCHEMA = [\"input_ids\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n",
      "{0: 0, 1: 1}\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "labels = np.unique(train_buf[0][\"labels\"]).tolist()\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")# if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_buf) / BATCH_SIZE * EPOCHS\n",
    "#warmup_steps = math.ceil(total_steps * 0.05)\n",
    "warmup_steps = math.ceil(total_steps * 0.1)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'decoder_input_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SCHEMA = list(train_buf[0].keys())\n",
    "BATCH_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0],\n",
       "         [0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1],\n",
       "         [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "         [1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "         [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1],\n",
       "         [0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1],\n",
       "         [1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "         [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1],\n",
       "         [1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
       "         [0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "         [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "         [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "         [1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1],\n",
       "         [0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1],\n",
       "         [0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "         [1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1],\n",
       "         [1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0],\n",
       "         [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "         [0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "         [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1],\n",
       "         [1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "         [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "         [1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1],\n",
       "         [1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1],\n",
       "         [1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "         [1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       "         [1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "         [0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0],\n",
       "         [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "         [1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       "         [0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1],\n",
       "         [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]]),\n",
       " 'decoder_input_ids': tensor([[1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1]]),\n",
       " 'attention_mask': tensor([[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0],\n",
       "         [0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1],\n",
       "         [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "         [1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "         [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1],\n",
       "         [0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1],\n",
       "         [1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "         [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1],\n",
       "         [1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
       "         [0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "         [0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "         [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "         [1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1],\n",
       "         [0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1],\n",
       "         [0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "         [1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1],\n",
       "         [1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0],\n",
       "         [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "         [0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "         [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1],\n",
       "         [1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "         [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "         [1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1],\n",
       "         [1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1],\n",
       "         [1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "         [1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       "         [1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "         [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "         [0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0],\n",
       "         [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "         [1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
       "         [0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1],\n",
       "         [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(threshold=100_000_000)\n",
    "train_buf[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch 1,000  of  50,000.    Elapsed:  0:00:02, Remaining:  0:01:38.\n",
      "  Batch 2,000  of  50,000.    Elapsed:  0:00:05, Remaining:  0:01:36.\n",
      "  Batch 3,000  of  50,000.    Elapsed:  0:00:07, Remaining:  0:02:21.\n",
      "  Batch 4,000  of  50,000.    Elapsed:  0:00:11, Remaining:  0:02:18.\n",
      "  Batch 5,000  of  50,000.    Elapsed:  0:00:14, Remaining:  0:02:15.\n",
      "  Batch 6,000  of  50,000.    Elapsed:  0:00:18, Remaining:  0:02:56.\n",
      "  Batch 7,000  of  50,000.    Elapsed:  0:00:22, Remaining:  0:02:52.\n",
      "  Batch 8,000  of  50,000.    Elapsed:  0:00:26, Remaining:  0:02:48.\n",
      "  Batch 9,000  of  50,000.    Elapsed:  0:00:31, Remaining:  0:03:25.\n",
      "  Batch 10,000  of  50,000.    Elapsed:  0:00:36, Remaining:  0:03:20.\n",
      "  Batch 11,000  of  50,000.    Elapsed:  0:00:41, Remaining:  0:03:15.\n",
      "  Batch 12,000  of  50,000.    Elapsed:  0:00:46, Remaining:  0:03:48.\n",
      "  Batch 13,000  of  50,000.    Elapsed:  0:00:52, Remaining:  0:03:42.\n",
      "  Batch 14,000  of  50,000.    Elapsed:  0:00:58, Remaining:  0:03:36.\n",
      "  Batch 15,000  of  50,000.    Elapsed:  0:01:05, Remaining:  0:04:05.\n",
      "  Batch 16,000  of  50,000.    Elapsed:  0:01:11, Remaining:  0:03:58.\n",
      "  Batch 17,000  of  50,000.    Elapsed:  0:01:19, Remaining:  0:03:51.\n",
      "  Batch 18,000  of  50,000.    Elapsed:  0:01:26, Remaining:  0:03:44.\n",
      "  Batch 19,000  of  50,000.    Elapsed:  0:01:34, Remaining:  0:04:08.\n",
      "  Batch 20,000  of  50,000.    Elapsed:  0:01:42, Remaining:  0:04:00.\n",
      "  Batch 21,000  of  50,000.    Elapsed:  0:01:50, Remaining:  0:03:52.\n",
      "  Batch 22,000  of  50,000.    Elapsed:  0:01:59, Remaining:  0:04:12.\n",
      "  Batch 23,000  of  50,000.    Elapsed:  0:02:08, Remaining:  0:04:03.\n",
      "  Batch 24,000  of  50,000.    Elapsed:  0:02:17, Remaining:  0:03:54.\n",
      "  Batch 25,000  of  50,000.    Elapsed:  0:02:27, Remaining:  0:04:10.\n",
      "  Batch 26,000  of  50,000.    Elapsed:  0:02:37, Remaining:  0:04:00.\n",
      "  Batch 27,000  of  50,000.    Elapsed:  0:02:47, Remaining:  0:03:50.\n",
      "  Batch 28,000  of  50,000.    Elapsed:  0:02:58, Remaining:  0:04:02.\n",
      "  Batch 29,000  of  50,000.    Elapsed:  0:03:09, Remaining:  0:03:51.\n",
      "  Batch 30,000  of  50,000.    Elapsed:  0:03:20, Remaining:  0:03:40.\n",
      "  Batch 31,000  of  50,000.    Elapsed:  0:03:31, Remaining:  0:03:48.\n",
      "  Batch 32,000  of  50,000.    Elapsed:  0:03:43, Remaining:  0:03:36.\n",
      "  Batch 33,000  of  50,000.    Elapsed:  0:03:55, Remaining:  0:03:24.\n",
      "  Batch 34,000  of  50,000.    Elapsed:  0:04:06, Remaining:  0:02:56.\n",
      "  Batch 35,000  of  50,000.    Elapsed:  0:04:17, Remaining:  0:02:45.\n",
      "  Batch 36,000  of  50,000.    Elapsed:  0:04:28, Remaining:  0:02:34.\n",
      "  Batch 37,000  of  50,000.    Elapsed:  0:04:40, Remaining:  0:02:23.\n",
      "  Batch 38,000  of  50,000.    Elapsed:  0:04:51, Remaining:  0:02:12.\n",
      "  Batch 39,000  of  50,000.    Elapsed:  0:05:03, Remaining:  0:02:12.\n",
      "  Batch 40,000  of  50,000.    Elapsed:  0:05:14, Remaining:  0:02:00.\n",
      "  Batch 41,000  of  50,000.    Elapsed:  0:05:26, Remaining:  0:01:48.\n",
      "  Batch 42,000  of  50,000.    Elapsed:  0:05:37, Remaining:  0:01:28.\n",
      "  Batch 43,000  of  50,000.    Elapsed:  0:05:49, Remaining:  0:01:24.\n",
      "  Batch 44,000  of  50,000.    Elapsed:  0:06:00, Remaining:  0:01:06.\n",
      "  Batch 45,000  of  50,000.    Elapsed:  0:06:12, Remaining:  0:01:00.\n",
      "  Batch 46,000  of  50,000.    Elapsed:  0:06:23, Remaining:  0:00:44.\n",
      "  Batch 47,000  of  50,000.    Elapsed:  0:06:35, Remaining:  0:00:33.\n",
      "  Batch 48,000  of  50,000.    Elapsed:  0:06:46, Remaining:  0:00:22.\n",
      "  Batch 49,000  of  50,000.    Elapsed:  0:06:57, Remaining:  0:00:11.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.009962507554374169\n",
      "    accuracy: 0.9977820000000002\n",
      "  Training epoch took: 0:07:09\n",
      "\n",
      "Running Testing...\n",
      "  Average testing scores:\n",
      "    loss: 0.0008726120510837063\n",
      "    accuracy: 1.0\n",
      "  Testing took: 0:00:21\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "model_training.STRING_BATCH_INDEX = True\n",
    "stats = train_bern_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    device,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    id2label,\n",
    "    batch_schema=BATCH_SCHEMA,\n",
    "    train_dataloader=train_buf,\n",
    "    test_dataloader=test_buf,\n",
    "    #vocab_size=VOCAB_SIZE,\n",
    "    print_status=True,\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    test_batch_size=BATCH_SIZE,\n",
    "    generic_output_class=True,\n",
    "    forward_args=[\"input_ids\", \"decoder_input_ids\", \"attention_mask\", \"labels\"],\n",
    "    chomsky_task=True,\n",
    "    accuracy_mask=accuracy_mask,\n",
    "\n",
    "    calc_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .88 / .78 for lr=1e-4, B=15, T=(20, 20), 3_500_000 2_500_000 static-warmup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
