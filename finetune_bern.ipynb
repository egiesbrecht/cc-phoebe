{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/pushshift/cc-phoebe/rotary_embeddings.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/mnt/pushshift/cc-phoebe/rotary_embeddings.py:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "import datasets\n",
    "from datasets import DatasetDict, load_dataset, Dataset\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5Tokenizer,\n",
    "    T5ForSequenceClassification,\n",
    "    T5Config,\n",
    "    BertTokenizer, \n",
    "    RobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from load_set import *\n",
    "from epoch_stats import EpochStats#, print_stat_tuples\n",
    "import model_training\n",
    "from model_training import (\n",
    "    BatchBuffer, \n",
    "    train_bern_model, \n",
    "    mask_tokens, \n",
    "    preprocess_with_given_labels, \n",
    "    num_parameters, \n",
    "    num_trainable_parameters, \n",
    "    preprocess_for_causallm, \n",
    "    preprocess_for_multiple_choice,\n",
    "    preprocess_for_seq2seq_swag,\n",
    "    preprocess_with_given_labels_train_test_wrap\n",
    ")\n",
    "import bert_i1_1_modeling as bert_i1_1\n",
    "import coin_i2C_modeling as ci2C\n",
    "import coin_i2D_modeling as ci2D\n",
    "import coin_i3A_modeling as ci3A\n",
    "import coin_i3B_modeling as ci3B\n",
    "import coin_i3C_modeling as ci3C\n",
    "import rnn_modeling as ci_rnn\n",
    "import lnn_modeling as lnn\n",
    "import pcoin_modeling as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_FILE = 1\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "CHECKPOINT_PATH = None#\"hyp_cls/BERT-base_all-labels/\" # datetime.datetime.now().strftime(\"tmp_models/rann_sffn/run_part_load_%Y-%m-%d_%H:%M:%S\")\n",
    "USE_CUSTOM_DATALOADER = True\n",
    "SHUFFLE_CUSTOM_DATALOADER = True\n",
    "LEARNING_RATE = 1e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30_522\n",
    "#VOCAB_SIZE = 32_000\n",
    "#VOCAB_SIZE = 52_000\n",
    "MAX_POSITION_EMBEDDINGS = 512#24**2\n",
    "HIDDEN_SIZE = 1024\n",
    "IS_HF_MODEL = False\n",
    "GENERIC_OUTPUT_CLASS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALWAYS CHECK num_labels, RFFN doesn't throw an error on a wrong parameter\n",
    "rffn_base_model_path = \"tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x2_1dec-none_no-revert_chunkwise_group-exp_congen-head_B10_multi-query-2_switch-ii_mpe576_no-cross-att/\"\n",
    "#rffn_base_model_path = \"tmp_models/COIN-i2B_oasst1_25k_1x3_000dec_none-decoder-revert-out_chunkwise_nt-case-3_2-decay-parts_allow-enc-tf/\"\n",
    "#rffn_base_model_path = \"tmp_models/RRB_oasst1_25k_2-2-encoder_0-1-decoder_decay_maskedLM.15_.2share_docx1_wtf/\"\n",
    "#rffn_tokenizer_path = \"pretrained_models/rffn_wikitext_516_tokenizer\"\n",
    "rffn_tokenizer_path = rffn_base_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default, sst2, swag, uni-main-hyp, uni-side-hyp, uni-all-hyp, bucket-sort, duplicate-string, parity-check\n",
    "TEST_METHOD = \"sst2\"\n",
    "#TEST_METHOD = \"uni-all-hyp\"\n",
    "ILOC_LIMIT = None\n",
    "DEFAULT_TEACHER_FORCING = True\n",
    "TEACHER_FORCING_PREFIX = \"\"#\"The correct answer is: \"\n",
    "DOC_PAD_TOKENS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_METHOD == \"default\":\n",
    "    NUM_LABELS = 7\n",
    "elif TEST_METHOD in (\"parity-check\", \"sst2\"):\n",
    "    NUM_LABELS = 2\n",
    "elif TEST_METHOD == \"swag\":\n",
    "    NUM_LABELS = 4\n",
    "elif TEST_METHOD == \"uni-main-hyp\":\n",
    "    NUM_LABELS = 10\n",
    "elif TEST_METHOD == \"uni-side-hyp\":\n",
    "    NUM_LABELS = 20\n",
    "elif TEST_METHOD == \"uni-all-hyp\":\n",
    "    NUM_LABELS = 30\n",
    "else:\n",
    "    NUM_LABELS = 2\n",
    "\n",
    "TEST_SST2 = TEST_METHOD == \"sst2\"\n",
    "ONE_LABEL_ONLY = TEST_SST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_METHOD == \"parity-check\":\n",
    "    VOCAB_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "\n",
    "    import ncps\n",
    "    from ncps.torch import LTC\n",
    "    in_features = 124\n",
    "    out_features = 124\n",
    "    units = 140\n",
    "\n",
    "    class LTCForSequenceClassification(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.embeddings = nn.Embedding(VOCAB_SIZE, in_features)\n",
    "            wiring = ncps.wirings.wirings.AutoNCP(units, out_features)  # 16 units, 1 motor neuron\n",
    "            self.ltc = LTC(in_features, wiring, batch_first=True, ode_unfolds=1)\n",
    "            self.pooler = nn.Sequential(nn.Linear(out_features, out_features), nn.Tanh())\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.classifier = nn.Linear(out_features, NUM_LABELS)\n",
    "            self.loss_fn = nn.CrossEntropyLoss() if NUM_LABELS <= 2 else nn.BCEWithLogitsLoss()\n",
    "\n",
    "        def forward(self, input_ids, labels, **kwargs):\n",
    "            emb = self.embeddings(input_ids)\n",
    "            logits, _ = self.ltc(emb)\n",
    "            logits = self.pooler(logits[:, 0])\n",
    "            logits = self.dropout(logits)\n",
    "            logits = self.classifier(logits)\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return ci_rnn.Output(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "    model = LTCForSequenceClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = lnn.LTCForSequenceClassification(\n",
    "        lnn.LTCConfig(\n",
    "            input_size = 64,\n",
    "            hidden_size = 64,\n",
    "            units = 64,\n",
    "            mixed_memory = False,\n",
    "            input_mapping = \"affine\",\n",
    "            output_mapping = \"affine\",\n",
    "            ode_unfolds = 6,\n",
    "            epsilon  = 1e-8,\n",
    "            implicit_param_constraints = True,\n",
    "            vocab_size = VOCAB_SIZE,\n",
    "            num_labels = NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = lnn.CfCForSequenceClassification(\n",
    "        lnn.CfCConfig(\n",
    "            input_size = 1024,\n",
    "            hidden_size = 1024,\n",
    "            sparsity_mask = None,\n",
    "            backbone_layers = 1,\n",
    "            backbone_units = 1024,\n",
    "            backbone_dropout = 0.0,\n",
    "            mode = \"pure\",\n",
    "            units = 1024,\n",
    "            proj_size = 1024,\n",
    "            wiring = None,\n",
    "            backbone_activation = \"tanh\",\n",
    "            mixed_memory = False,\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = ci_rnn.COINForSequenceClassification(\n",
    "        config=ci_rnn.RNNConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            num_hidden_layers=2,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            intermediate_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "            layer_norm_eps=1e-12,\n",
    "            rope_dim=16,\n",
    "            chunk_schema=[lambda T: T, lambda T: T],\n",
    "\n",
    "            carry_over_S=False,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    ONE_LABEL_ONLY = True\n",
    "    tokenizer = None\n",
    "    class ParityLSTM(nn.Module):\n",
    "        def __init__(self, hidden_size=HIDDEN_SIZE):\n",
    "            super().__init__()\n",
    "            self.config = None\n",
    "            self.hidden_size = hidden_size\n",
    "            self.lstm = nn.LSTM(1, hidden_size, batch_first=True)\n",
    "            self.L1 = nn.Linear(hidden_size, 128)\n",
    "            self.L2 = nn.Linear(128, 2)\n",
    "        \n",
    "        def forward(self, X):\n",
    "            N = len()\n",
    "\n",
    "            y = F.relu(self.L1(l_out))\n",
    "            y = F.dropout(y, 0.5)\n",
    "            y = self.L2(y)\n",
    "            y = F.sigmoid(y)\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    ONE_LABEL_ONLY = True\n",
    "    tokenizer = None\n",
    "    class LSTMForParityCheck(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.n_layers = config.num_hidden_layers\n",
    "            self.lstm = nn.LSTM(2, config.hidden_size, batch_first=True, num_layers=self.n_layers, dropout=config.hidden_dropout_prob)\n",
    "            self.pooler = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "            self.classifier = nn.Sequential(\n",
    "                #nn.Dropout(config.hidden_dropout_prob, \n",
    "                nn.Linear(config.hidden_size, config.num_labels)\n",
    "            )\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            logits = F.one_hot(input_ids.long(), self.config.vocab_size).float()\n",
    "            #logits = self.embeddings(logits)\n",
    "            B, T, C = logits.shape\n",
    "            hidden = (torch.randn(self.n_layers, B, C, device=logits.device), torch.randn(self.n_layers, B, C, device=logits.device))\n",
    "            #logits, hidden = self.lstm(logits, hidden)\n",
    "            logits, hidden = self.lstm(logits)\n",
    "            #for i in range(T):\n",
    "            #    out, hidden = self.lstm(logits[:, i:i+1, :], hidden)\n",
    "            #r_logits = self.pooler(logits[:, -1, :])\n",
    "            #r_logits = F.tanh(logits)\n",
    "            #r_logits = out[:, -1]\n",
    "            #print(logits.shape, logits.view(T, -1).shape)\n",
    "            r_logits = self.classifier(logits[:, -1])\n",
    "            #r_logits = F.log_softmax(r_logits, 1)\n",
    "            loss = self.loss_fn(r_logits, labels)\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=r_logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "    model = LSTMForParityCheck(\n",
    "        config=GenConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_dropout_prob=0.5,\n",
    "            num_hidden_layers=1,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = False\n",
    "    ONE_LABEL_ONLY = True\n",
    "    tokenizer = None\n",
    "    class BertForParityCheck(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            #self.bert = transformers.BertModel(config)\n",
    "            self.bert = transformers.BertForSequenceClassification(config)\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            emb = self.embeddings(F.one_hot(input_ids, self.config.vocab_size).float())\n",
    "            B, T, C = emb.shape\n",
    "            logits = self.bert(inputs_embeds=emb, attention_mask=attention_mask).logits\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "\n",
    "    model = BertForParityCheck(\n",
    "        config=transformers.BertConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_hidden_layers=5,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_attention_heads=1,\n",
    "            num_labels=NUM_LABELS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    ONE_LABEL_ONLY = True\n",
    "    tokenizer = None\n",
    "    class BertForBucketSort(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            self.bert = transformers.BertModel(config)\n",
    "            self.embeddings = nn.Linear(config.vocab_size, config.hidden_size)\n",
    "            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "            emb = self.embeddings(F.one_hot(input_ids, self.config.vocab_size).float())\n",
    "            B, T, C = emb.shape\n",
    "            logits = self.bert(inputs_embeds=emb, attention_mask=attention_mask).last_hidden_state\n",
    "            logits = self.lm_head(logits)\n",
    "            loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            return ci3C.COINOutputClass(\n",
    "                logits=logits,\n",
    "                loss=loss\n",
    "            )\n",
    "\n",
    "\n",
    "    model = BertForBucketSort(\n",
    "        config=transformers.BertConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_hidden_layers=5,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_attention_heads=1,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_CONFIG = pc.RNNConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    intermediate_size=HIDDEN_SIZE,\n",
    "    num_labels=NUM_LABELS,\n",
    "    #hidden_retention_act=\"relu\",\n",
    "    #rope_dim=32,\n",
    "    #chunk_schema=[\"T\", \"T\"]\n",
    "    chunk_schema=[lambda T: T]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = pc.ACorN01ForSequenceClassification(\n",
    "        config=pc_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = pc.COINForSequenceClassification(\n",
    "        config=pc_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci3C_CONFIG = ci3C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_labels=NUM_LABELS,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=True,\n",
    "    num_decay_parts=1,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    hidden_pos_offset=False,\n",
    "    rope_dim=32,\n",
    "    num_query_heads=1,\n",
    "\n",
    "    decoder_output=\"none\",\n",
    "    revert_decoder=False,\n",
    "    decoder_schema=[0] * 2,\n",
    "    cross_encoder_schema=[0] * 2,\n",
    "    experts_schema=None,#[2, 2],\n",
    "    block_io_schema=None,#[[1024, 1024*4, 1024]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = None\n",
    "    ONE_LABEL_ONLY = True\n",
    "    model = ci3C.COINForParityCheck(\n",
    "        config=ci3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    ONE_LABEL_ONLY = True\n",
    "    tokenizer = None\n",
    "    model = ci3C.COINForBucketSort(\n",
    "        config=ci3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = ci3C.COINForSequenceClassification(\n",
    "        config=ci3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = ci3C.COINForFeedbackModelingClassification(\n",
    "        config=ci3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = ci3C.COINForHierachicalClassification(\n",
    "        config=ci3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    N_ITER = 4\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained(f\"{rffn_base_model_path}/bpe_tokenizer/\")\n",
    "    model = ci3C.COINForSequenceClassification.from_pretrained(\n",
    "        f\"{rffn_base_model_path}/model/epoch_{N_ITER}/model\",\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = ci3B.COINForSequenceClassification(\n",
    "        config=ci3B.COINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_labels=NUM_LABELS,\n",
    "            forward_method=\"parallel\",\n",
    "            apply_decay=False,\n",
    "            num_decay_parts=1,\n",
    "            hidden_retention_act=\"relu\",\n",
    "\n",
    "            decoder_output=\"strict\",\n",
    "            decoder_schema=[0, 1],\n",
    "            cross_encoder_schema=[0, 0],\n",
    "            experts_schema=None,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = ci3A.COINForSequenceClassification(\n",
    "        config=ci3A.COINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_labels=NUM_LABELS,\n",
    "            forward_method=\"parallel\",\n",
    "            apply_decay=False,\n",
    "            num_decay_parts=1,\n",
    "            hidden_retention_act=\"relu\",\n",
    "            apply_hidden_pos_offset=False,\n",
    "            #fuzed_decay_attention_mask=False,\n",
    "\n",
    "            decoder_output=\"none\",\n",
    "            revert_decoder=False,\n",
    "            decoder_schema=[1, 1],\n",
    "            cross_encoder_schema=[0] * 2,\n",
    "            experts_schema=None,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    N_ITER = 0\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained(f\"{rffn_base_model_path}/bpe_tokenizer/\")\n",
    "    model = ci3A.COINForSequenceClassification.from_pretrained(\n",
    "        f\"{rffn_base_model_path}/model/epoch_{N_ITER}/model\",\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained(f\"{rffn_base_model_path}/bpe_tokenizer/\")\n",
    "    NUM_REGIONS = 1\n",
    "    model = ci2D.COINForSequenceClassification(\n",
    "        config=ci2D.COINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_labels=NUM_LABELS,\n",
    "            hidden_retention_act=\"relu\",\n",
    "            #hidden_out_act=None,\n",
    "            forward_method=\"parallel\",\n",
    "            apply_decay=False,\n",
    "            reverse_decay=False,\n",
    "            num_decay_parts=1,\n",
    "            decoder_output=\"strict\",\n",
    "            #rope_dim=16,\n",
    "            \n",
    "            num_regions=NUM_REGIONS,\n",
    "            decoder_schema=      [0, 1],\n",
    "            cross_encoder_schema=[0, 0],\n",
    "            \n",
    "            share_S=False,\n",
    "            \n",
    "            #layer_norm_eps=1e-12,\n",
    "            #retention_group_norm_eps=1e-8,\n",
    "            #rms_norm_eps=1e-12,\n",
    "\n",
    "            disable_teacher_forcing=False,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    N_ITER = 0\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained(f\"{rffn_base_model_path}/bpe_tokenizer/\")\n",
    "    model = ci2D.COINForSequenceClassification.from_pretrained(\n",
    "        f\"{rffn_base_model_path}/model/epoch_{N_ITER}/model\",\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decay mask schema: [0.96875, 0.998046875]\n",
      "num layers: 2, num regions: 1\n",
      "num core parameters: 120,001,552\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained(f\"{rffn_base_model_path}/bpe_tokenizer/\")\n",
    "    NUM_REGIONS = 1\n",
    "    model = ci2C.COINForSequenceClassification(\n",
    "        config=ci2C.COINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            num_labels=NUM_LABELS,\n",
    "            hidden_retention_act=\"relu\",\n",
    "            #hidden_out_act=None,\n",
    "            forward_method=\"parallel\",\n",
    "            apply_decay=False,\n",
    "            #fixed_decay_value=None,\n",
    "            num_decay_parts=1,\n",
    "            #reverse_decay=False,\n",
    "            chunkwise_num_chunks=4,\n",
    "            apply_chunking_globally=False,\n",
    "            #apply_hidden_pos_offset=False,\n",
    "            decoder_output=\"none\",\n",
    "            \n",
    "            num_regions=NUM_REGIONS,\n",
    "            decoder_schema=      [0, 0],\n",
    "            cross_encoder_schema=[0, 0],\n",
    "            multi_head_qkv=False,\n",
    "            num_heads=16,\n",
    "            share_S=False,\n",
    "            #num_repetitions=1,\n",
    "            add_residual_query_skip=False,\n",
    "\n",
    "            #layer_norm_eps=1e-12,\n",
    "            #retention_group_norm_eps=1e-8,\n",
    "            #rms_norm_eps=1e-12,\n",
    "\n",
    "            print_checks=CHECK_RUN,\n",
    "            reset_S_n_state=False,\n",
    "            disable_teacher_forcing=False,\n",
    "\n",
    "            apply_selective_attention_params=False,\n",
    "            #selective_param_Ns=(2, 2),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    N_ITER = 0\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained(f\"{rffn_base_model_path}/bpe_tokenizer/\")\n",
    "    model = ci2C.COINForSequenceClassification.from_pretrained(\n",
    "        f\"{rffn_base_model_path}/model/epoch_{N_ITER}/model\",\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "    class MambaOutput:\n",
    "        def __init__(self, out):\n",
    "            self.logits = out\n",
    "            self.encoder_hidden_state = None\n",
    "            self.S = None\n",
    "            self.C = None\n",
    "            self.loss = None\n",
    "            self.aux_loss = None\n",
    "\n",
    "    class MambaForSequenceClassification(nn.Module):\n",
    "        def __init__(self, path, **kwargs):\n",
    "            super().__init__()\n",
    "            self.mamba = transformers.MambaModel.from_pretrained(path, num_hidden_layers=12, **kwargs)\n",
    "            self.config = self.mamba.config\n",
    "            self.dense = nn.Linear(768, 768)\n",
    "            self.act = nn.Tanh()\n",
    "            self.cls = nn.Linear(768, NUM_LABELS)\n",
    "\n",
    "        def forward(self, **kwargs):\n",
    "            logits = self.mamba(**kwargs).last_hidden_state\n",
    "            out = self.act(self.dense(logits[:, 0, :]))\n",
    "            out = self.cls(out)\n",
    "            return MambaOutput(out)\n",
    "\n",
    "        \n",
    "    #model = transformers.MambaModel.from_pretrained(\"state-spaces/mamba-130m-hf\", num_labels=NUM_LABELS)\n",
    "    model = MambaForSequenceClassification(\"state-spaces/mamba-130m-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = transformers.BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=NUM_LABELS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    tokenizer = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "    model = transformers.RobertaForSequenceClassification.from_pretrained(\n",
    "        \"roberta-base\",\n",
    "        num_labels=NUM_LABELS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    tokenizer = transformers.DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    model = transformers.DebertaForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/deberta-base\",\n",
    "        num_labels=NUM_LABELS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    #tokenizer = BertTokenizer.from_pretrained(f\"{rffn_base_model_path}/wordpiece_tokenizer/\")\n",
    "    model = transformers.BertForSequenceClassification(\n",
    "        config=transformers.BertConfig(\n",
    "            num_labels=NUM_LABELS,\n",
    "            #num_hidden_layers=2,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120,003,602\n",
      "120,003,586\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,}\\n{:,}\".format(num_parameters(model), num_trainable_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ds_path = \"../datasets/big_AAABDON_Nmax_st200_s0_a10_tvsplit.1_no_norm/\"\n",
    "\n",
    "train_ds = [\n",
    "    f\"{base_ds_path}/train/train_00[0-{TO_FILE}].csv\"\n",
    "\n",
    "    #f\"datasets/big_AAABDON_Nmax_st200_s0_a10_tvsplit.1_no_norm/train/train_00[0-9].csv\",\n",
    "    #f\"datasets/big_AAABDON_Nmax_st200_s0_a10_tvsplit.1_no_norm/train/train_01[0-9].csv\"\n",
    "]\n",
    "test_ds = [\n",
    "    f\"{base_ds_path}/validation/validation_00[0-{TO_FILE}].csv\"\n",
    "\n",
    "    #f\"datasets/big_AAABDON_Nmax_st200_s0_a10_tvsplit.1_no_norm/validation/validation_00[0-9].csv\",\n",
    "    #f\"datasets/big_AAABDON_Nmax_st200_s0_a10_tvsplit.1_no_norm/validation/validation_01[0-9].csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'idx'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if TEST_METHOD == \"sst2\":\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": load_dataset(\"glue\", name=\"sst2\", split=\"train[:10000]\").rename_column(\"sentence\", \"text\"),\n",
    "        \"test\": load_dataset(\"glue\", name=\"sst2\", split=\"validation\").rename_column(\"sentence\", \"text\")\n",
    "    })\n",
    "elif TEST_METHOD == \"default\":\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": load_set(train_ds, unused_fields=[\"author\", \"subreddit\", \"style\"], iloc_limit=ILOC_LIMIT),\n",
    "        \"test\":  load_set(test_ds, unused_fields=[\"author\", \"subreddit\", \"style\"], iloc_limit=ILOC_LIMIT)\n",
    "\n",
    "        #\"train\": load_dataset(\"glue\", name=\"mnli\", split=\"train[0:10000]\"),\n",
    "        #\"test\": load_dataset(\"glue\", name=\"mnli\", split=\"validation_matched[0:1500]\")\n",
    "\n",
    "        #\"train\": load_dataset(\"squad_v2\", split=\"train[0:10000]\"),\n",
    "        #\"test\": load_dataset(\"squad_v2\", split=\"test[0:1500]\")\n",
    "    })\n",
    "elif TEST_METHOD == \"swag\":\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": load_dataset(\"Rowan/hellaswag\", split=\"train\"),\n",
    "        \"test\": load_dataset(\"Rowan/hellaswag\", split=\"validation\")\n",
    "    })\n",
    "elif TEST_METHOD in (\"uni-main-hyp\", \"uni-side-hyp\", \"uni-all-hyp\"):\n",
    "    if TEST_METHOD == \"uni-main-hyp\":\n",
    "        DS_TRAIN_PATH = \"../uni-hyp-class/wordpiece_abstracts_train.csv\"\n",
    "        DS_TEST_PATH = \"../uni-hyp-class/wordpiece_abstracts_test.csv\"\n",
    "    elif TEST_METHOD == \"uni-side-hyp\":\n",
    "        DS_TRAIN_PATH = \"../uni-hyp-class/wordpiece_abstracts_train_side_label_1.csv\"\n",
    "        DS_TEST_PATH = \"../uni-hyp-class/wordpiece_abstracts_test_side_label_1.csv\"\n",
    "    elif TEST_METHOD == \"uni-all-hyp\":\n",
    "        DS_TRAIN_PATH = \"../uni-hyp-class/wordpiece_abstracts_train_all_labels.csv\"\n",
    "        DS_TEST_PATH = \"../uni-hyp-class/wordpiece_abstracts_test_all_labels.csv\"\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": load_set([DS_TRAIN_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"]),\n",
    "        \"test\": load_set([DS_TEST_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"]),\n",
    "    })\n",
    "elif TEST_METHOD == \"bucket-sort\":\n",
    "    #dataset = generate_bucket_sort_set(B=100, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE)\n",
    "    dataset = DatasetDict({\n",
    "        #\"train\": Dataset.from_dict(generate_bucket_sort_set(B=100_000, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE)),\n",
    "        #\"test\": Dataset.from_dict(generate_bucket_sort_set(B=1000, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE))\n",
    "        \"train\": generate_uniform_batches(generate_bucket_sort_set, B=TRAIN_BATCH_SIZE, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE, num_samples=10_000),\n",
    "        \"test\": generate_uniform_batches(generate_bucket_sort_set, B=TEST_BATCH_SIZE, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE, num_samples=1000)\n",
    "    })\n",
    "elif TEST_METHOD == \"duplicate-string\":\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": generate_duplicate_string_set(B=100_000, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE),\n",
    "        \"test\": generate_duplicate_string_set(B=1000, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE)\n",
    "    })\n",
    "elif TEST_METHOD == \"parity-check\":\n",
    "    dataset = DatasetDict({\n",
    "        #\"train\": generate_parity_check_set(B=10_000, T=MAX_POSITION_EMBEDDINGS),\n",
    "        #\"test\": generate_parity_check_set(B=1000, T=MAX_POSITION_EMBEDDINGS)\n",
    "        \"train\": generate_uniform_batches(generate_parity_check_set, B=TRAIN_BATCH_SIZE, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE, num_samples=10_000),\n",
    "        \"test\": generate_uniform_batches(generate_parity_check_set, B=TEST_BATCH_SIZE, T=MAX_POSITION_EMBEDDINGS, vocab_size=VOCAB_SIZE, num_samples=1000)\n",
    "    })\n",
    "else:\n",
    "    raise ValueError(TEST_METHOD)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1}\n",
      "{0: 0, 1: 1}\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if ONE_LABEL_ONLY:\n",
    "    #labels = np.unique(train_df[\"label\"]).tolist()\n",
    "    labels = np.unique(dataset[\"train\"][\"label\"]).tolist()\n",
    "else:\n",
    "    labels = [label for label in dataset['train'].features.keys() if label not in [\"text\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "if TEST_METHOD in (\"default\", \"sst2\", \"uni-main-hyp\", \"uni-side-hyp\", \"uni-all-hyp\"):\n",
    "    #encoded_dataset = preprocess_with_given_labels(dataset, tokenizer, labels, label2id, MAX_POSITION_EMBEDDINGS, ONE_LABEL_ONLY, remove_columns=dataset[\"train\"].column_names, \n",
    "    #                                               default_teacher_forcing=DEFAULT_TEACHER_FORCING, teacher_forcing_prefix=TEACHER_FORCING_PREFIX, doc_pad_tokens=DOC_PAD_TOKENS)\n",
    "    encoded_dataset = preprocess_with_given_labels_train_test_wrap(dataset, tokenizer, labels, label2id, MAX_POSITION_EMBEDDINGS, ONE_LABEL_ONLY, remove_columns=dataset[\"train\"].column_names, \n",
    "                                                   default_teacher_forcing=DEFAULT_TEACHER_FORCING, teacher_forcing_prefix=TEACHER_FORCING_PREFIX, doc_pad_tokens=DOC_PAD_TOKENS)\n",
    "elif TEST_METHOD == \"swag\":\n",
    "    #encoded_dataset = preprocess_for_multiple_choice(dataset, tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=dataset[\"train\"].column_names, num_proc=4)\n",
    "    encoded_dataset = preprocess_for_seq2seq_swag(dataset, tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=dataset[\"train\"].column_names, num_proc=4)\n",
    "elif TEST_METHOD in (\"bucket-sort\", \"duplicate-string\", \"parity-check\"):\n",
    "    encoded_dataset = dataset\n",
    "encoded_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask',\n",
       " 'labels',\n",
       " 'decoder_input_ids']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "batch_schema = list(encoded_dataset[\"train\"].features.keys())\n",
    "batch_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if USE_CUSTOM_DATALOADER:\n",
    "    #train_dataloader = create_dataloader(encoded_dataset[\"train\"])\n",
    "    #test_dataloader = create_dataloader(encoded_dataset[\"test\"])\n",
    "    train_dataloader = BatchBuffer(encoded_dataset[\"train\"], TRAIN_BATCH_SIZE)\n",
    "    if SHUFFLE_CUSTOM_DATALOADER:\n",
    "        train_dataloader.shuffle()\n",
    "    test_dataloader = BatchBuffer(encoded_dataset[\"test\"], TEST_BATCH_SIZE)\n",
    "else:\n",
    "    USE_TOKEN_TYPE_IDS = \"token_type_ids\" in encoded_dataset[\"train\"].features\n",
    "    USE_DEC_II = \"decoder_input_ids\" in encoded_dataset[\"train\"].features\n",
    "    # Load input data into tensors\n",
    "    train_input_ids = torch.tensor(encoded_dataset[\"train\"][\"input_ids\"])\n",
    "    if USE_TOKEN_TYPE_IDS:\n",
    "        train_token_type_ids = torch.tensor(encoded_dataset[\"train\"][\"token_type_ids\"])\n",
    "    train_masks = torch.tensor(encoded_dataset[\"train\"][\"attention_mask\"])\n",
    "    train_labels = torch.tensor(encoded_dataset[\"train\"][\"labels\"])\n",
    "    if USE_DEC_II:\n",
    "        train_dec_ii = torch.tensor(encoded_dataset[\"train\"][\"decoder_input_ids\"])\n",
    "\n",
    "    test_input_ids = torch.tensor(encoded_dataset[\"test\"][\"input_ids\"])\n",
    "    if USE_TOKEN_TYPE_IDS:\n",
    "        test_token_type_ids = torch.tensor(encoded_dataset[\"test\"][\"token_type_ids\"])\n",
    "    test_masks = torch.tensor(encoded_dataset[\"test\"][\"attention_mask\"])\n",
    "    test_labels = torch.tensor(encoded_dataset[\"test\"][\"labels\"])\n",
    "    if USE_DEC_II:\n",
    "        test_dec_ii = torch.tensor(encoded_dataset[\"test\"][\"decoder_input_ids\"])\n",
    "\n",
    "    # Create the DataLoader and Sampler for both sets.\n",
    "    if USE_TOKEN_TYPE_IDS:\n",
    "        train_data = TensorDataset(train_input_ids, train_token_type_ids, train_masks, train_labels)\n",
    "    else:\n",
    "        train_data = TensorDataset(train_input_ids, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, \n",
    "        sampler=train_sampler, \n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "    if USE_TOKEN_TYPE_IDS:\n",
    "        test_data = TensorDataset(test_input_ids, test_token_type_ids, test_masks, test_labels)\n",
    "    else:\n",
    "        test_data = TensorDataset(test_input_ids, test_masks, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, \n",
    "        sampler=test_sampler, \n",
    "        batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_dataloader) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "warmup_steps = math.ceil(total_steps * 0.05)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "if len(labels) <= 2:\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "1562.5\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(loss_function)\n",
    "print(total_steps)\n",
    "print(warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if CHECKPOINT_PATH is not None:\n",
    "    try:\n",
    "        os.mkdir(CHECKPOINT_PATH)\n",
    "    except OSError as err:\n",
    "        print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#len(encoded_dataset[\"train\"][\"input_ids\"]), len(encoded_dataset[\"train\"][\"input_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask',\n",
       " 'labels',\n",
       " 'decoder_input_ids']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "batch_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COINForSequenceClassification(\n",
       "  (coin): COINModel(\n",
       "    (regions): ModuleList(\n",
       "      (0): COINRegionHolder(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x COINBlock(\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (block): EncoderBlock(\n",
       "              (wqkv): WQKV(\n",
       "                (rope): RotaryEmbedding()\n",
       "                (act): ReLU()\n",
       "                (out_act): ReLU()\n",
       "                (group_norm): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
       "                (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (rms_norm): RMSNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (out_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (sampler): COINSampler(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x COINSamplerLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): COINBlock(\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (block): EncoderBlock(\n",
       "                (wqkv): WQKV(\n",
       "                  (rope): RotaryEmbedding()\n",
       "                  (act): ReLU()\n",
       "                  (out_act): ReLU()\n",
       "                  (group_norm): GroupNorm(16, 1024, eps=1e-05, affine=True)\n",
       "                  (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                  (rms_norm): RMSNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (out_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (residual_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (pooler): COINPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (act): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=100_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    25  of  1,250.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    50  of  1,250.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    75  of  1,250.    Elapsed:  0:00:03, Remaining:  0:00:47.\n",
      "  Batch   100  of  1,250.    Elapsed:  0:00:04, Remaining:  0:00:46.\n",
      "  Batch   125  of  1,250.    Elapsed:  0:00:05, Remaining:  0:00:45.\n",
      "  Batch   150  of  1,250.    Elapsed:  0:00:06, Remaining:  0:00:44.\n",
      "  Batch   175  of  1,250.    Elapsed:  0:00:07, Remaining:  0:00:43.\n",
      "  Batch   200  of  1,250.    Elapsed:  0:00:08, Remaining:  0:00:42.\n",
      "  Batch   225  of  1,250.    Elapsed:  0:00:09, Remaining:  0:00:41.\n",
      "  Batch   250  of  1,250.    Elapsed:  0:00:10, Remaining:  0:00:40.\n",
      "  Batch   275  of  1,250.    Elapsed:  0:00:11, Remaining:  0:00:39.\n",
      "  Batch   300  of  1,250.    Elapsed:  0:00:12, Remaining:  0:00:38.\n",
      "  Batch   325  of  1,250.    Elapsed:  0:00:13, Remaining:  0:00:37.\n",
      "  Batch   350  of  1,250.    Elapsed:  0:00:14, Remaining:  0:00:36.\n",
      "  Batch   375  of  1,250.    Elapsed:  0:00:15, Remaining:  0:00:35.\n",
      "  Batch   400  of  1,250.    Elapsed:  0:00:16, Remaining:  0:00:34.\n",
      "  Batch   425  of  1,250.    Elapsed:  0:00:17, Remaining:  0:00:33.\n",
      "  Batch   450  of  1,250.    Elapsed:  0:00:18, Remaining:  0:00:32.\n",
      "  Batch   475  of  1,250.    Elapsed:  0:00:19, Remaining:  0:00:31.\n",
      "  Batch   500  of  1,250.    Elapsed:  0:00:20, Remaining:  0:00:30.\n",
      "  Batch   525  of  1,250.    Elapsed:  0:00:21, Remaining:  0:00:29.\n",
      "  Batch   550  of  1,250.    Elapsed:  0:00:22, Remaining:  0:00:28.\n",
      "  Batch   575  of  1,250.    Elapsed:  0:00:23, Remaining:  0:00:27.\n",
      "  Batch   600  of  1,250.    Elapsed:  0:00:24, Remaining:  0:00:26.\n",
      "  Batch   625  of  1,250.    Elapsed:  0:00:25, Remaining:  0:00:25.\n",
      "  Batch   650  of  1,250.    Elapsed:  0:00:26, Remaining:  0:00:24.\n",
      "  Batch   675  of  1,250.    Elapsed:  0:00:27, Remaining:  0:00:23.\n",
      "  Batch   700  of  1,250.    Elapsed:  0:00:28, Remaining:  0:00:22.\n",
      "  Batch   725  of  1,250.    Elapsed:  0:00:29, Remaining:  0:00:21.\n",
      "  Batch   750  of  1,250.    Elapsed:  0:00:30, Remaining:  0:00:20.\n",
      "  Batch   775  of  1,250.    Elapsed:  0:00:31, Remaining:  0:00:19.\n",
      "  Batch   800  of  1,250.    Elapsed:  0:00:32, Remaining:  0:00:18.\n",
      "  Batch   825  of  1,250.    Elapsed:  0:00:33, Remaining:  0:00:17.\n",
      "  Batch   850  of  1,250.    Elapsed:  0:00:34, Remaining:  0:00:16.\n",
      "  Batch   875  of  1,250.    Elapsed:  0:00:35, Remaining:  0:00:15.\n",
      "  Batch   900  of  1,250.    Elapsed:  0:00:36, Remaining:  0:00:14.\n",
      "  Batch   925  of  1,250.    Elapsed:  0:00:37, Remaining:  0:00:13.\n",
      "  Batch   950  of  1,250.    Elapsed:  0:00:38, Remaining:  0:00:12.\n",
      "  Batch   975  of  1,250.    Elapsed:  0:00:39, Remaining:  0:00:11.\n",
      "  Batch 1,000  of  1,250.    Elapsed:  0:00:40, Remaining:  0:00:10.\n",
      "  Batch 1,025  of  1,250.    Elapsed:  0:00:41, Remaining:  0:00:09.\n",
      "  Batch 1,050  of  1,250.    Elapsed:  0:00:42, Remaining:  0:00:08.\n",
      "  Batch 1,075  of  1,250.    Elapsed:  0:00:43, Remaining:  0:00:07.\n",
      "  Batch 1,100  of  1,250.    Elapsed:  0:00:44, Remaining:  0:00:06.\n",
      "  Batch 1,125  of  1,250.    Elapsed:  0:00:45, Remaining:  0:00:05.\n",
      "  Batch 1,150  of  1,250.    Elapsed:  0:00:46, Remaining:  0:00:04.\n",
      "  Batch 1,175  of  1,250.    Elapsed:  0:00:46, Remaining:  0:00:03.\n",
      "  Batch 1,200  of  1,250.    Elapsed:  0:00:47, Remaining:  0:00:02.\n",
      "  Batch 1,225  of  1,250.    Elapsed:  0:00:48, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.5491549488663673\n",
      "    accuracy: 0.7189\n",
      "    hamming: 0.7189\n",
      "    f1_micro: 0.7189\n",
      "    f1_macro: 0.6746209745809758\n",
      "    recall_micro: 0.7189\n",
      "    recall_macro: 0.7112242857142853\n",
      "    precision_micro: 0.7189\n",
      "    precision_macro: 0.729662857142857\n",
      "  Training epoch took: 0:00:49\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.49405554955432174\n",
      "    perplexity: 1.6830152251295833\n",
      "    accuracy: 0.7580275229357798\n",
      "    hamming: 0.7580275229357798\n",
      "    f1_micro: 0.7580275229357798\n",
      "    f1_macro: 0.7317219986944762\n",
      "    recall_micro: 0.7580275229357798\n",
      "    recall_macro: 0.754357798165138\n",
      "    precision_micro: 0.7580275229357798\n",
      "    precision_macro: 0.7609217999126259\n",
      "  Testing took: 0:00:04\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    25  of  1,250.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    50  of  1,250.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    75  of  1,250.    Elapsed:  0:00:03, Remaining:  0:00:47.\n",
      "  Batch   100  of  1,250.    Elapsed:  0:00:04, Remaining:  0:00:46.\n",
      "  Batch   125  of  1,250.    Elapsed:  0:00:05, Remaining:  0:00:45.\n",
      "  Batch   150  of  1,250.    Elapsed:  0:00:06, Remaining:  0:00:44.\n",
      "  Batch   175  of  1,250.    Elapsed:  0:00:07, Remaining:  0:00:43.\n",
      "  Batch   200  of  1,250.    Elapsed:  0:00:08, Remaining:  0:00:42.\n",
      "  Batch   225  of  1,250.    Elapsed:  0:00:09, Remaining:  0:00:41.\n",
      "  Batch   250  of  1,250.    Elapsed:  0:00:10, Remaining:  0:00:40.\n",
      "  Batch   275  of  1,250.    Elapsed:  0:00:11, Remaining:  0:00:39.\n",
      "  Batch   300  of  1,250.    Elapsed:  0:00:12, Remaining:  0:00:38.\n",
      "  Batch   325  of  1,250.    Elapsed:  0:00:13, Remaining:  0:00:37.\n",
      "  Batch   350  of  1,250.    Elapsed:  0:00:14, Remaining:  0:00:36.\n",
      "  Batch   375  of  1,250.    Elapsed:  0:00:15, Remaining:  0:00:35.\n",
      "  Batch   400  of  1,250.    Elapsed:  0:00:16, Remaining:  0:00:34.\n",
      "  Batch   425  of  1,250.    Elapsed:  0:00:17, Remaining:  0:00:33.\n",
      "  Batch   450  of  1,250.    Elapsed:  0:00:18, Remaining:  0:00:32.\n",
      "  Batch   475  of  1,250.    Elapsed:  0:00:19, Remaining:  0:00:31.\n",
      "  Batch   500  of  1,250.    Elapsed:  0:00:20, Remaining:  0:00:30.\n",
      "  Batch   525  of  1,250.    Elapsed:  0:00:21, Remaining:  0:00:29.\n",
      "  Batch   550  of  1,250.    Elapsed:  0:00:22, Remaining:  0:00:28.\n",
      "  Batch   575  of  1,250.    Elapsed:  0:00:23, Remaining:  0:00:27.\n",
      "  Batch   600  of  1,250.    Elapsed:  0:00:24, Remaining:  0:00:26.\n",
      "  Batch   625  of  1,250.    Elapsed:  0:00:25, Remaining:  0:00:25.\n",
      "  Batch   650  of  1,250.    Elapsed:  0:00:26, Remaining:  0:00:24.\n",
      "  Batch   675  of  1,250.    Elapsed:  0:00:27, Remaining:  0:00:23.\n",
      "  Batch   700  of  1,250.    Elapsed:  0:00:28, Remaining:  0:00:22.\n",
      "  Batch   725  of  1,250.    Elapsed:  0:00:29, Remaining:  0:00:21.\n",
      "  Batch   750  of  1,250.    Elapsed:  0:00:30, Remaining:  0:00:20.\n",
      "  Batch   775  of  1,250.    Elapsed:  0:00:31, Remaining:  0:00:19.\n",
      "  Batch   800  of  1,250.    Elapsed:  0:00:32, Remaining:  0:00:18.\n",
      "  Batch   825  of  1,250.    Elapsed:  0:00:33, Remaining:  0:00:17.\n",
      "  Batch   850  of  1,250.    Elapsed:  0:00:34, Remaining:  0:00:16.\n",
      "  Batch   875  of  1,250.    Elapsed:  0:00:35, Remaining:  0:00:15.\n",
      "  Batch   900  of  1,250.    Elapsed:  0:00:36, Remaining:  0:00:14.\n",
      "  Batch   925  of  1,250.    Elapsed:  0:00:37, Remaining:  0:00:13.\n",
      "  Batch   950  of  1,250.    Elapsed:  0:00:38, Remaining:  0:00:12.\n",
      "  Batch   975  of  1,250.    Elapsed:  0:00:39, Remaining:  0:00:11.\n",
      "  Batch 1,000  of  1,250.    Elapsed:  0:00:40, Remaining:  0:00:10.\n",
      "  Batch 1,025  of  1,250.    Elapsed:  0:00:41, Remaining:  0:00:09.\n",
      "  Batch 1,050  of  1,250.    Elapsed:  0:00:42, Remaining:  0:00:08.\n",
      "  Batch 1,075  of  1,250.    Elapsed:  0:00:43, Remaining:  0:00:07.\n",
      "  Batch 1,100  of  1,250.    Elapsed:  0:00:44, Remaining:  0:00:06.\n",
      "  Batch 1,125  of  1,250.    Elapsed:  0:00:45, Remaining:  0:00:05.\n",
      "  Batch 1,150  of  1,250.    Elapsed:  0:00:46, Remaining:  0:00:04.\n",
      "  Batch 1,175  of  1,250.    Elapsed:  0:00:47, Remaining:  0:00:03.\n",
      "  Batch 1,200  of  1,250.    Elapsed:  0:00:48, Remaining:  0:00:02.\n",
      "  Batch 1,225  of  1,250.    Elapsed:  0:00:48, Remaining:  0:00:01.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 0.33767314858436587\n",
      "    accuracy: 0.8553\n",
      "    hamming: 0.8553\n",
      "    f1_micro: 0.8553\n",
      "    f1_macro: 0.8342792096792128\n",
      "    recall_micro: 0.8553\n",
      "    recall_macro: 0.8543004761904751\n",
      "    precision_micro: 0.8553\n",
      "    precision_macro: 0.8569676190476188\n",
      "  Training epoch took: 0:00:49\n",
      "\n",
      "Running Test 1 ...\n",
      "  Average testing scores:\n",
      "    loss: 0.40314876366782626\n",
      "    perplexity: 1.5326033052098726\n",
      "    accuracy: 0.8084862385321101\n",
      "    hamming: 0.8084862385321101\n",
      "    f1_micro: 0.8084862385321101\n",
      "    f1_macro: 0.7869750025713332\n",
      "    recall_micro: 0.8084862385321101\n",
      "    recall_macro: 0.8095019659239846\n",
      "    precision_micro: 0.8084862385321101\n",
      "    precision_macro: 0.8101026649191787\n",
      "  Testing took: 0:00:04\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    25  of  1,250.    Elapsed:  0:00:01, Remaining:  0:00:49.\n",
      "  Batch    50  of  1,250.    Elapsed:  0:00:02, Remaining:  0:00:48.\n",
      "  Batch    75  of  1,250.    Elapsed:  0:00:03, Remaining:  0:00:47.\n",
      "  Batch   100  of  1,250.    Elapsed:  0:00:04, Remaining:  0:00:46.\n",
      "  Batch   125  of  1,250.    Elapsed:  0:00:05, Remaining:  0:00:45.\n",
      "  Batch   150  of  1,250.    Elapsed:  0:00:05, Remaining:  0:00:44.\n",
      "  Batch   175  of  1,250.    Elapsed:  0:00:06, Remaining:  0:00:43.\n",
      "  Batch   200  of  1,250.    Elapsed:  0:00:07, Remaining:  0:00:42.\n",
      "  Batch   225  of  1,250.    Elapsed:  0:00:08, Remaining:  0:00:41.\n",
      "  Batch   250  of  1,250.    Elapsed:  0:00:09, Remaining:  0:00:40.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stats = train_bern_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    device,\n",
    "    loss_function,\n",
    "    id2label,\n",
    "    batch_schema=batch_schema,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    print_status=True,\n",
    "    is_hf_model=IS_HF_MODEL,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    only_save_core=False,\n",
    "    one_label_only=ONE_LABEL_ONLY,\n",
    "    mixed_lm_task=False,\n",
    "    mixed_lm_loss_function=nn.CrossEntropyLoss(),\n",
    "    \n",
    "    generic_output_class=True,\n",
    "    \n",
    "    #forward_args=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"],\n",
    "    #forward_args=[\"input_ids\"],\n",
    "\n",
    "    add_layers_on_stagnation=False,\n",
    "    num_layers_to_add=1,\n",
    "    add_layers_threshold=0.01, #0.005,\n",
    "    plot_k_topics=False,\n",
    "\n",
    "    batch_hack_train=True,\n",
    "    mlm_decode_n=0,#.0075,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    masked_lm_task=False,\n",
    "    check_run=CHECK_RUN,\n",
    "    retain_graph=False,\n",
    "\n",
    "    calc_metrics=True,\n",
    "    per_class_f1=False,\n",
    "\n",
    "    backprop_during_testing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## SST 2 test\n",
    "## [loss] / [acc/ham]\n",
    "\n",
    "# 35k\n",
    "# .61 / .79 epoch 4 ; .59 / .78 epoch 3 ; .52 / .776 epoch 2 15k pretraining, num_labels=2\n",
    "\n",
    "# .48 / .80 epoch 3\n",
    "\n",
    "\n",
    "# 10k\n",
    "# .51 / .758 epoch 2 empty\n",
    "# .57 / .778 epoch 3 empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# hf bert (empty): batch_size=6, time per epoch=6:30min, 5734MiB VRAM, 0.756 0.786 epoch 6\n",
    "# hf t5 (empty): batch_size=6, time per epoch=5:17min, 5306MiB VRAM, 0.758 0.773 epoch 5\n",
    "# hf t5 (empty) num_layers=12 num_heads=12: batch_size=3, time per epoch=11:45min, 7416MiB VRAM, 0.758 0.787 epoch 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
