{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from datasets import DatasetDict, load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    BertTokenizer, \n",
    "    RobertaTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "from load_set import load_set, load_moses_set\n",
    "from epoch_stats import EpochStats, print_stat_tuples\n",
    "import model_training\n",
    "from model_training import (\n",
    "    BatchBuffer, \n",
    "    mask_tokens, \n",
    "    train_bern_model, \n",
    "    preprocess_for_maskedlm, \n",
    "    preprocess_for_causallm, \n",
    "    preprocess_for_monologe, \n",
    "    preprocess_for_sparselm, \n",
    "    preprocess_for_binary_sparselm,\n",
    "    num_parameters, \n",
    "    num_trainable_parameters,\n",
    "    preprocess_for_translation,\n",
    ")\n",
    "\n",
    "import coin_i2C_modeling as ci2C\n",
    "import coin_i2D_modeling as ci2D\n",
    "import coin_i3A_modeling as ci3A\n",
    "import coin_i3C_modeling as ci3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBATCH = False\n",
    "TRAIN_BATCH_SIZE = 5\n",
    "TEST_BATCH_SIZE = 5\n",
    "#BASE_PATH = \"tmp_models/COIN-i2D_parallel_control-unit\"\n",
    "BASE_PATH = \"tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x1_1dec-none_no-revert_chunkwise_group-exp_congen-head_B6_multi-query-2_mpe576_no-cross-att/\"\n",
    "#BASE_PATH = \"tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2/\"\n",
    "\n",
    "if REBATCH:\n",
    "    if BASE_PATH[-1] == \"/\":\n",
    "        BASE_PATH = BASE_PATH[:-1]\n",
    "    BASE_PATH += \"_rebatch/\"\n",
    "DATASET_JSON_PATH = f\"{BASE_PATH}/dataset/\"\n",
    "CHECKPOINT_PATH = f\"{BASE_PATH}/model/\"\n",
    "WORDPIECE_TOKENIZER_DIR = f\"{BASE_PATH}/wordpiece_tokenizer/\"\n",
    "BPE_TOKENIZER_DIR = f\"{BASE_PATH}/bpe_tokenizer/\"\n",
    "SENTENCE_PIECE_TOKENIZER_DIR = f\"{BASE_PATH}/sentence_piece_tokenizer/\"\n",
    "USE_CUSTOM_DATALOADER = False\n",
    "LEARNING_RATE = 1e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_TRAIN_DATA = True\n",
    "SHUFFLE_TEST_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAUSAL_LM = False\n",
    "ENCODE_CAUSAL_LM = False\n",
    "GROUP_TEXTS = True\n",
    "SPARSIFY = True\n",
    "MASK_TOKEN = None\n",
    "PAD_TOKEN = None\n",
    "PREFIX = None#\"Translate the following text:\"\n",
    "#PREFIX = \"Replace all of the mask-tokens: \"\n",
    "#PREFIX = \"This sentence is completely obsolete \"\n",
    "SWITCH_II_DECODER_II = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out dir: tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x1_1dec-none_no-revert_chunkwise_group-exp_congen-head_B6_multi-query-2_mpe576_no-cross-att/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"out dir: {BASE_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30_522\n",
    "#VOCAB_SIZE = 32_000\n",
    "#VOCAB_SIZE = 52_000\n",
    "MAX_POSITION_EMBEDDINGS = 576#512\n",
    "#MAX_POSITION_EMBEDDINGS = 516\n",
    "#MAX_POSITION_EMBEDDINGS = 768\n",
    "IS_HF_MODEL = False\n",
    "IS_ENCODER_DECODER_MODEL = False\n",
    "EPOCH_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCCA_SAVE_CONFIG = ci3C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=True,\n",
    "    num_decay_parts=1,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    hidden_pos_offset=False,\n",
    "    rope_dim=16,\n",
    "    num_query_heads=1,\n",
    "\n",
    "    decoder_output=\"adaptive\",\n",
    "    revert_decoder=True,\n",
    "    decoder_schema=[1] * 4,\n",
    "    cross_encoder_schema=[1] * 4,\n",
    "    block_io_schema=None,#[[1024, 1024*4, 1024], [1024, 1024*2, 1024]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI3C_CONFIG = ci3C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=True,\n",
    "    num_decay_parts=1,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    hidden_pos_offset=False,\n",
    "    rope_dim=16,\n",
    "    num_query_heads=1,\n",
    "\n",
    "    decoder_output=\"adaptive\",\n",
    "    revert_decoder=True,\n",
    "    decoder_schema=[1] * 4,\n",
    "    cross_encoder_schema=[1] * 4,\n",
    "    block_io_schema=None,#[[1024, 1024*4, 1024], [1024, 1024*2, 1024]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num layers: 4\n",
      "gamma schema: [[0.96875, 0.9789702892303467], [0.985848069190979, 0.9904764294624329], [0.9935911297798157, 0.9956871271133423], [0.9970976710319519, 0.998046875]]\n",
      "layer 0 num experts: 1\n",
      "layer 1 num experts: 1\n",
      "layer 2 num experts: 1\n",
      "layer 3 num experts: 1\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    model = ci3C.COINForConditionalGeneration(\n",
    "        CI3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3C.COINForMaskedLM(\n",
    "        CI3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI3A_CONFIG = ci3A.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    forward_method=\"chunkwise\",\n",
    "    apply_decay=False,\n",
    "    num_decay_parts=1,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    apply_hidden_pos_offset=True,\n",
    "\n",
    "    decoder_output=\"none\",\n",
    "    revert_decoder=False,\n",
    "    decoder_schema=[1] * 1,\n",
    "    cross_encoder_schema=[0] * 1,\n",
    "    experts_schema=None,\n",
    "\n",
    "    switch_ii_decoder_ii=SWITCH_II_DECODER_II,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3A.COINForMaskedLM(\n",
    "        CI3A_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3A.COINForConditionalGeneration(\n",
    "        CI3A_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REGIONS = 1\n",
    "COIN_CONFIG = ci2D.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    #hidden_out_act=None,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=False,\n",
    "    reverse_decay=False,\n",
    "    num_decay_parts=1,\n",
    "    decoder_output=\"strict\",\n",
    "    #rope_dim=16,\n",
    "    \n",
    "    num_regions=NUM_REGIONS,\n",
    "    decoder_schema=      [1],\n",
    "    cross_encoder_schema=[0],\n",
    "    \n",
    "    share_S=False,\n",
    "    \n",
    "    #layer_norm_eps=1e-12,\n",
    "    #retention_group_norm_eps=1e-8,\n",
    "    #rms_norm_eps=1e-12,\n",
    "    switch_ii_decoder_ii=SWITCH_II_DECODER_II,\n",
    "    disable_teacher_forcing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci2D.COINForMaskedLM(COIN_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REGIONS = 1\n",
    "COIN_CONFIG = ci2C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    #hidden_out_act=None,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=True,\n",
    "    #fixed_decay_value=None,\n",
    "    num_decay_parts=1,\n",
    "    #reverse_decay=False,\n",
    "    #chunkwise_num_chunks=1,\n",
    "    #apply_chunking_globally=False,\n",
    "    #apply_hidden_pos_offset=False,\n",
    "    decoder_output=\"none\",\n",
    "    \n",
    "    num_regions=NUM_REGIONS,\n",
    "    decoder_schema=[0] * 8,\n",
    "    cross_encoder_schema=[0] * 8,\n",
    "    multi_head_qkv=False,\n",
    "    #num_heads=16,\n",
    "    #share_S=False,\n",
    "    num_repetitions=1,\n",
    "\n",
    "    #rms_norm_eps=1e-8,\n",
    "\n",
    "    disable_teacher_forcing=False,\n",
    "    switch_ii_decoder_ii=SWITCH_II_DECODER_II,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci2C.COINForMaskedLM(\n",
    "        config=COIN_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci2C.COINForConditionalGeneration(\n",
    "        config=COIN_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    model = transformers.XLMWithLMHeadModel(\n",
    "        config=transformers.XLMConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159,665,216\n",
      "159,665,152\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,}\\n{:,}\".format(num_parameters(model), num_trainable_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x1_1dec-none_no-revert_chunkwise_group-exp_congen-head_B6_multi-query-2_mpe576_no-cross-att/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x1_1dec-none_no-revert_chunkwise_group-exp_congen-head_B6_multi-query-2_mpe576_no-cross-att//model/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x1_1dec-none_no-revert_chunkwise_group-exp_congen-head_B6_multi-query-2_mpe576_no-cross-att//wordpiece_tokenizer/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x1_1dec-none_no-revert_chunkwise_group-exp_congen-head_B6_multi-query-2_mpe576_no-cross-att//bpe_tokenizer/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x1_1dec-none_no-revert_chunkwise_group-exp_congen-head_B6_multi-query-2_mpe576_no-cross-att//sentence_piece_tokenizer/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x1_1dec-none_no-revert_chunkwise_group-exp_congen-head_B6_multi-query-2_mpe576_no-cross-att//dataset/'\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "try:\n",
    "    os.mkdir(BASE_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(CHECKPOINT_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(WORDPIECE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(BPE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(SENTENCE_PIECE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(DATASET_JSON_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#dataset = DatasetDict({\n",
    "#    \"train\": load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train[0:10000]\"),\n",
    "#    \"test\":  load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"validation[:1500]\")\n",
    "#})\n",
    "\n",
    "#dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oasst1, aaabdon, mcca, translation_mcca\n",
    "DATASET = \"translation_mcca\"\n",
    "#DATASET = \"oasst1\"\n",
    "\n",
    "#HF_TRAIN_ROWS = 25_000\n",
    "HF_TRAIN_ROWS = 500_000\n",
    "#HF_TRAIN_ROWS = -1\n",
    "HF_TRAIN_FROM = 0#10_000\n",
    "\n",
    "#HF_TEST_ROWS = 1_500\n",
    "HF_TEST_ROWS = 5000\n",
    "#HF_TEST_ROWS = -1\n",
    "HF_TEST_FROM = 0\n",
    "\n",
    "CUSTOM_BASE_DS_PATH = \"../datasets/big_AAABDON_Nmax_st200_s0_a10_tvsplit.1_no_norm/\"\n",
    "CUSTOM_DS_TO_FILE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle\n",
      "Dataset({\n",
      "    features: ['src', 'tgt'],\n",
      "    num_rows: 500000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['src', 'tgt'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2000000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "if DATASET == \"mcca\":\n",
    "    train_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-2][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "    test_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "        ]\n",
    "    })\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"translation_mcca\":\n",
    "    train_dataset = load_moses_set({\n",
    "        \"src\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-5][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-5][0-9]\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-5][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-5][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "    test_dataset = load_moses_set({\n",
    "        \"src\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x800[0-1]\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x800[0-1]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "        ]\n",
    "    })\n",
    "    tok_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-9][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-9][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "elif DATASET == \"aaabdon\":\n",
    "    train_ds = [f\"{CUSTOM_BASE_DS_PATH}/train/train_00[0-{CUSTOM_DS_TO_FILE}].csv\"]\n",
    "    test_ds = [f\"{CUSTOM_BASE_DS_PATH}/validation/validation_00[0-{CUSTOM_DS_TO_FILE}].csv\"]\n",
    "    train_dataset = load_set(train_ds)#.select(list(range(HF_TRAIN_ROWS)))\n",
    "    test_dataset = load_set(test_ds)#.select(list(range(HF_TEST_ROWS)))\n",
    "    tok_dataset = load_set([f\"{CUSTOM_BASE_DS_PATH}/train/train_*.csv\"])\n",
    "elif DATASET == \"oasst1\":\n",
    "    #train_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=f\"train[0:{HF_TRAIN_ROWS}]\")\n",
    "    #test_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=f\"validation[:{HF_TEST_ROWS}]\")\n",
    "    #tok_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")\n",
    "    \n",
    "    #train_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\", split=f\"train[0:{HF_TRAIN_ROWS}]\")  .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "    #test_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\", split=f\"test[0:{HF_TEST_ROWS}]\")     .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "    #tok_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\")                                       .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "\n",
    "    train_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    test_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"validation\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    tok_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    #train_dataset = load_from_disk(\"../datasets/oasst1/train\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TRAIN_ROWS)))\n",
    "    #test_dataset = load_from_disk(\"../datasets/oasst1/validation\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TEST_ROWS)))\n",
    "    #tok_dataset = load_from_disk(\"../datasets/oasst1/train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "\n",
    "    #train_dataset = load_dataset(\"OpenAssistant/oasst2\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TRAIN_ROWS)))\n",
    "    #test_dataset = load_dataset(\"OpenAssistant/oasst2\", split=\"validation\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TEST_ROWS)))\n",
    "\n",
    "if SHUFFLE_TRAIN_DATA:\n",
    "    print(\"shuffle\")\n",
    "    train_dataset = train_dataset.shuffle()\n",
    "if HF_TRAIN_ROWS > 0:\n",
    "    train_dataset =  train_dataset.select(list(range(HF_TRAIN_FROM, HF_TRAIN_ROWS)))\n",
    "if SHUFFLE_TEST_DATA:\n",
    "    print(\"shuffle\")\n",
    "    test_dataset = test_dataset.shuffle()\n",
    "if HF_TEST_ROWS > 0:\n",
    "    test_dataset = test_dataset.select(list(range(HF_TEST_FROM, HF_TEST_ROWS)))\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "print(tok_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 500/500 [00:01<00:00, 285.05ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 467.14ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "801082"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_json(f\"{DATASET_JSON_PATH}/train.json\")\n",
    "test_dataset.to_json(f\"{DATASET_JSON_PATH}/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': 0, 'tgt': 1}\n",
      "{0: 'src', 1: 'tgt'}\n",
      "['src', 'tgt']\n"
     ]
    }
   ],
   "source": [
    "labels = [label for label in train_dataset.features.keys() if label not in [\"text\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, BertWordPieceTokenizer, SentencePieceBPETokenizer, SentencePieceUnigramTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    #tok_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")\n",
    "    #tok_dataset = load_dataset(\"glue\", name=\"sst2\", split=\"train\")\n",
    "    #tok_dataset = tok_dataset.rename_column(\"sentence\", \"text\")\n",
    "    \n",
    "    tokenizer = SentencePieceUnigramTokenizer()\n",
    "\n",
    "    tokenizer.train_from_iterator(\n",
    "        iterator=tok_dataset[\"text\"], \n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        #min_frequency=2,\n",
    "        show_progress=True,\n",
    "        #limit_alphabet=500,\n",
    "        special_tokens=[\n",
    "            \"<PAD>\", \n",
    "            \"<UNK>\", \n",
    "            \"<CLS>\", \n",
    "            \"<SEP>\", \n",
    "            \"<DOC>\",\n",
    "            \"<MASK>\"\n",
    "        ])\n",
    "\n",
    "    tokenizer = PreTrainedTokenizer(\n",
    "        tokenizer_object=tokenizer\n",
    "    )\n",
    "    tokenizer.save_model(SENTENCE_PIECE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(SENTENCE_PIECE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if 1:\n",
    "    tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True,\n",
    "                                        strip_accents=True, lowercase=True)\n",
    "\n",
    "    tokenizer.train_from_iterator(iterator=tok_dataset[\"text\"], vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[\n",
    "        \"[PAD]\", \n",
    "        \"[UNK]\", \n",
    "        \"[CLS]\", \n",
    "        \"[SEP]\", \n",
    "        \"[DOC]\",\n",
    "    #    \"[UDOC]\",\n",
    "        \"[MASK]\"\n",
    "    ])\n",
    "    tokenizer.save_model(WORDPIECE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    tokenizer = BertTokenizer.from_pretrained(WORDPIECE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "    \n",
    "    tokenizer.train_from_iterator(iterator=tok_dataset[\"text\"], vocab_size=VOCAB_SIZE, min_frequency=2, length=MAX_POSITION_EMBEDDINGS, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<doc>\",\n",
    "        \"<mask>\",\n",
    "        \n",
    "        #\"<pad>\",\n",
    "        #\"<unk>\",\n",
    "        #\"<cls>\",\n",
    "        #\"<sep>\",\n",
    "        #\"<doc>\",\n",
    "        #\"<mask>\",\n",
    "    ])\n",
    "\n",
    "    # Save files to disk\n",
    "    tokenizer.save_model(BPE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(BPE_TOKENIZER_DIR)\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(BPE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def encode_and_batch(dataset, tokenizer, max_position_embeddings, batch_size, shuffle=False):\n",
    "    if DATASET in (\"translation_mcca\"):\n",
    "        encoded = preprocess_for_translation(dataset, tokenizer, max_position_embeddings, source_lang=\"src\", target_lang=\"tgt\", prefix=PREFIX, num_proc=4, remove_columns=[\"src\", \"tgt\"], switch_ii_decoder_ii=SWITCH_II_DECODER_II)\n",
    "    elif ENCODE_CAUSAL_LM:\n",
    "        encoded = preprocess_for_causallm(dataset, tokenizer, block_size=MAX_POSITION_EMBEDDINGS, remove_columns=dataset.column_names, shift_right=False)\n",
    "    else:\n",
    "        encoded = preprocess_for_maskedlm(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names, to_mask=.15, chance_rand_token=.2, \n",
    "                                          group_texts=GROUP_TEXTS, mask_token=MASK_TOKEN, pad_token=PAD_TOKEN, sparsify=SPARSIFY, prefix=PREFIX, switch_ii_decoder_ii=SWITCH_II_DECODER_II)\n",
    "        #encoded = preprocess_for_cot(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names, group_texts=GROUP_TEXTS, pad_token=PAD_TOKEN, sparsify=SPARSIFY, prefix=PREFIX)\n",
    "       \n",
    "       \n",
    "        #encoded = preprocess_for_monologe(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names)\n",
    "        #encoded = preprocess_for_sparselm(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names)\n",
    "        #encoded = preprocess_for_binary_sparselm(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names)\n",
    "        #encoded = preprocess_for_maskedlm(encoded, tokenizer, max_position_embeddings, to_mask=.15, chance_rand_token=.2, group_texts=GROUP_TEXTS, mask_token=MASK_TOKEN)\n",
    "    print(encoded)\n",
    "    batched = BatchBuffer(encoded, batch_size)\n",
    "    if shuffle:\n",
    "        batched.shuffle()\n",
    "    print(\"  finished\")\n",
    "    return batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 500000/500000 [00:30<00:00, 16505.36 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
      "    num_rows: 11675\n",
      "})\n",
      "  finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Map (num_proc=4): 100%|██████████| 5000/5000 [00:01<00:00, 4291.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
      "    num_rows: 197\n",
      "})\n",
      "  finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "train_loader_call = lambda: encode_and_batch(train_dataset, tokenizer, MAX_POSITION_EMBEDDINGS, TRAIN_BATCH_SIZE, True)\n",
    "test_loader_call = lambda: encode_and_batch(test_dataset, tokenizer, MAX_POSITION_EMBEDDINGS, TEST_BATCH_SIZE)\n",
    "\n",
    "if not REBATCH:\n",
    "    train_dataloader = train_loader_call()\n",
    "    test_dataloader = test_loader_call()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#encoded_train_dataset = preprocess_for_maskedlm(dataset, tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=train_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#train_dataloader = BatchBuffer(encoded_dataset[\"train\"], BATCH_SIZE).shuffle()\n",
    "#test_dataloader = BatchBuffer(encoded_dataset[\"test\"], BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#batch_schema = list(encoded_dataset[\"train\"].features.keys())\n",
    "#batch_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def count_item(inp, item):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for n in inp:\n",
    "        for r in n:\n",
    "            i = r\n",
    "            if not i < 4:\n",
    "                total += 1\n",
    "            if i == item:\n",
    "                count += 1\n",
    "            #if i != 0 and i != item:\n",
    "            #    print(i)\n",
    "    return f\"{count} / {total} ; {count/total}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked tokens [input_ids]: 0 / 5224170 ; 0.0\n",
      "masked tokens [labels]: 0 / 87639 ; 0.0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"masked tokens [input_ids]:\", count_item(train_dataloader.ds[\"input_ids\"], tokenizer.mask_token_id))\n",
    "print(\"masked tokens [labels]:\", count_item(test_dataloader.ds[\"labels\"], tokenizer.mask_token_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "warmup_steps = math.ceil(total_steps * 0.05)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask',\n",
       " 'labels',\n",
       " 'decoder_input_ids']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "train_dataloader.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(train_dataloader.schema)\n",
    "its = 0\n",
    "for i, n in enumerate(train_dataloader):\n",
    "    if i >= its:\n",
    "        break\n",
    "    for k in n:\n",
    "        print(i, \"############\")\n",
    "        for l in k:\n",
    "            print(len(l))\n",
    "        print(\"##############\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COINForConditionalGeneration(\n",
       "  (coin): COINModel(\n",
       "    (encoder_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (residual_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (stack): COINStack(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x COINLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): COINBlock(\n",
       "              (cross_qkv): MultiQueryQKV(\n",
       "                (rope): RotaryEmbedding()\n",
       "                (xpos): XPOS()\n",
       "                (act): ReLU()\n",
       "                (out_act): ReLU()\n",
       "                (group_norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (proj_G): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (cross_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (qkv): MultiQueryQKV(\n",
       "                (rope): RotaryEmbedding()\n",
       "                (xpos): XPOS()\n",
       "                (act): ReLU()\n",
       "                (out_act): ReLU()\n",
       "                (group_norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (proj_G): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (norm): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): COINPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (act): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=30522, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 25 ========\n",
      "\n",
      "Generated batch schema as ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    58  of  2,335.    Elapsed:  0:00:20, Remaining:  0:13:05.\n",
      "  Batch   116  of  2,335.    Elapsed:  0:00:40, Remaining:  0:12:45.\n",
      "  Batch   174  of  2,335.    Elapsed:  0:00:59, Remaining:  0:12:25.\n",
      "  Batch   232  of  2,335.    Elapsed:  0:01:19, Remaining:  0:12:05.\n",
      "  Batch   290  of  2,335.    Elapsed:  0:01:39, Remaining:  0:11:45.\n",
      "  Batch   348  of  2,335.    Elapsed:  0:01:58, Remaining:  0:11:25.\n",
      "  Batch   406  of  2,335.    Elapsed:  0:02:18, Remaining:  0:11:05.\n",
      "  Batch   464  of  2,335.    Elapsed:  0:02:38, Remaining:  0:10:45.\n",
      "  Batch   522  of  2,335.    Elapsed:  0:02:58, Remaining:  0:10:25.\n",
      "  Batch   580  of  2,335.    Elapsed:  0:03:17, Remaining:  0:10:05.\n",
      "  Batch   638  of  2,335.    Elapsed:  0:03:37, Remaining:  0:09:45.\n",
      "  Batch   696  of  2,335.    Elapsed:  0:03:57, Remaining:  0:09:25.\n",
      "  Batch   754  of  2,335.    Elapsed:  0:04:17, Remaining:  0:09:05.\n",
      "  Batch   812  of  2,335.    Elapsed:  0:04:36, Remaining:  0:08:45.\n",
      "  Batch   870  of  2,335.    Elapsed:  0:04:56, Remaining:  0:08:25.\n",
      "  Batch   928  of  2,335.    Elapsed:  0:05:16, Remaining:  0:08:05.\n",
      "  Batch   986  of  2,335.    Elapsed:  0:05:36, Remaining:  0:07:45.\n",
      "  Batch 1,044  of  2,335.    Elapsed:  0:05:55, Remaining:  0:07:25.\n",
      "  Batch 1,102  of  2,335.    Elapsed:  0:06:15, Remaining:  0:07:05.\n",
      "  Batch 1,160  of  2,335.    Elapsed:  0:06:35, Remaining:  0:06:45.\n",
      "  Batch 1,218  of  2,335.    Elapsed:  0:06:54, Remaining:  0:06:25.\n",
      "  Batch 1,276  of  2,335.    Elapsed:  0:07:14, Remaining:  0:06:05.\n",
      "  Batch 1,334  of  2,335.    Elapsed:  0:07:34, Remaining:  0:05:45.\n",
      "  Batch 1,392  of  2,335.    Elapsed:  0:07:53, Remaining:  0:05:25.\n",
      "  Batch 1,450  of  2,335.    Elapsed:  0:08:13, Remaining:  0:05:05.\n",
      "  Batch 1,508  of  2,335.    Elapsed:  0:08:33, Remaining:  0:04:45.\n",
      "  Batch 1,566  of  2,335.    Elapsed:  0:08:53, Remaining:  0:04:25.\n",
      "  Batch 1,624  of  2,335.    Elapsed:  0:09:12, Remaining:  0:04:05.\n",
      "  Batch 1,682  of  2,335.    Elapsed:  0:09:32, Remaining:  0:03:45.\n",
      "  Batch 1,740  of  2,335.    Elapsed:  0:09:52, Remaining:  0:03:25.\n",
      "  Batch 1,798  of  2,335.    Elapsed:  0:10:12, Remaining:  0:03:05.\n",
      "  Batch 1,856  of  2,335.    Elapsed:  0:10:31, Remaining:  0:02:45.\n",
      "  Batch 1,914  of  2,335.    Elapsed:  0:10:51, Remaining:  0:02:25.\n",
      "  Batch 1,972  of  2,335.    Elapsed:  0:11:11, Remaining:  0:02:05.\n",
      "  Batch 2,030  of  2,335.    Elapsed:  0:11:30, Remaining:  0:01:45.\n",
      "  Batch 2,088  of  2,335.    Elapsed:  0:11:50, Remaining:  0:01:25.\n",
      "  Batch 2,146  of  2,335.    Elapsed:  0:12:10, Remaining:  0:01:05.\n",
      "  Batch 2,204  of  2,335.    Elapsed:  0:12:29, Remaining:  0:00:45.\n",
      "  Batch 2,262  of  2,335.    Elapsed:  0:12:49, Remaining:  0:00:25.\n",
      "  Batch 2,320  of  2,335.    Elapsed:  0:13:09, Remaining:  0:00:05.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 10.643598113376527\n",
      "  Training epoch took: 0:13:14\n",
      "\n",
      "Running Testing...\n",
      "  Average testing scores:\n",
      "    loss: 9.9745575097891\n",
      "  Testing took: 0:00:10\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x1_1dec-none_no-revert_chunkwise_group-exp_congen-head_B6_multi-query-2_mpe576_no-cross-att//model//epoch_0/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 2 / 25 ========\n",
      "\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stats = train_bern_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    #train_dataloader,\n",
    "    #test_dataloader,\n",
    "    #batch_schema,\n",
    "    device,\n",
    "    loss_function,\n",
    "    id2label,\n",
    "    train_dataloader=train_dataloader if not REBATCH else None,\n",
    "    test_dataloader=test_dataloader if not REBATCH else None,\n",
    "    create_train_dataloader=train_loader_call if REBATCH else None,\n",
    "    create_test_dataloader=test_loader_call if REBATCH else None,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    print_status=True,\n",
    "    is_hf_model=IS_HF_MODEL,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    only_save_core=False,\n",
    "    epoch_i=EPOCH_I,\n",
    "    is_encoder_decoder_model=IS_ENCODER_DECODER_MODEL,\n",
    "    causal_lm=CAUSAL_LM,\n",
    "\n",
    "    #forward_args=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"],\n",
    "\n",
    "    masked_lm_task=True,\n",
    "    electra_task=False,\n",
    "    mlm_decode_n=0,\n",
    "    #mlm_decode_n=.0075,\n",
    "    #mlm_decode_n=.1,\n",
    "    mlm_decode_max_chars=200,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    dump_coin_regions=False,\n",
    "    generic_output_class=True,\n",
    "    #coin_region_lambda=lambda model: model.coin.core.regions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m print_stat_tuples(\u001b[43mstats\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stats' is not defined"
     ]
    }
   ],
   "source": [
    "print_stat_tuples(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
