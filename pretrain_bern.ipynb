{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from datasets import DatasetDict, load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    BertTokenizer, \n",
    "    RobertaTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "from load_set import load_set, load_moses_set\n",
    "from epoch_stats import EpochStats, print_stat_tuples\n",
    "import model_training\n",
    "from model_training import (\n",
    "    BatchBuffer, \n",
    "    mask_tokens, \n",
    "    train_bern_model, \n",
    "    preprocess_for_maskedlm, \n",
    "    preprocess_for_causallm, \n",
    "    preprocess_for_monologe, \n",
    "    preprocess_for_sparselm, \n",
    "    preprocess_for_binary_sparselm,\n",
    "    num_parameters, \n",
    "    num_trainable_parameters,\n",
    "    preprocess_for_translation,\n",
    ")\n",
    "\n",
    "import coin_i2C_modeling as ci2C\n",
    "import coin_i2D_modeling as ci2D\n",
    "import coin_i3A_modeling as ci3A\n",
    "import coin_i3C_modeling as ci3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBATCH = False\n",
    "TRAIN_BATCH_SIZE = 6\n",
    "TEST_BATCH_SIZE = 6\n",
    "BASE_PATH = \"tmp_models/COIN-i3C_mcca-translation-en-de_0029-500k_1x2_1dec-none_no-revert_parallel_group-exp_congen-head_B6_multi-query-2_switch-ii/\"\n",
    "#BASE_PATH = \"tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2/\"\n",
    "\n",
    "if REBATCH:\n",
    "    if BASE_PATH[-1] == \"/\":\n",
    "        BASE_PATH = BASE_PATH[:-1]\n",
    "    BASE_PATH += \"_rebatch/\"\n",
    "DATASET_JSON_PATH = f\"{BASE_PATH}/dataset/\"\n",
    "CHECKPOINT_PATH = f\"{BASE_PATH}/model/\"\n",
    "WORDPIECE_TOKENIZER_DIR = f\"{BASE_PATH}/wordpiece_tokenizer/\"\n",
    "BPE_TOKENIZER_DIR = f\"{BASE_PATH}/bpe_tokenizer/\"\n",
    "SENTENCE_PIECE_TOKENIZER_DIR = f\"{BASE_PATH}/sentence_piece_tokenizer/\"\n",
    "USE_CUSTOM_DATALOADER = False\n",
    "LEARNING_RATE = 1e-5\n",
    "EPS = 1e-8\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_TRAIN_DATA = False\n",
    "SHUFFLE_TEST_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAUSAL_LM = False\n",
    "ENCODE_CAUSAL_LM = False\n",
    "GROUP_TEXTS = True\n",
    "SPARSIFY = True\n",
    "MASK_TOKEN = None\n",
    "PAD_TOKEN = None\n",
    "PREFIX = None#\"Translate the following text:\"\n",
    "#PREFIX = \"Replace all of the mask-tokens: \"\n",
    "#PREFIX = \"This sentence is completely obsolete \"\n",
    "SWITCH_II_DECODER_II = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out dir: tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"out dir: {BASE_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30_522\n",
    "#VOCAB_SIZE = 32_000\n",
    "#VOCAB_SIZE = 52_000\n",
    "MAX_POSITION_EMBEDDINGS = 512\n",
    "#MAX_POSITION_EMBEDDINGS = 516\n",
    "#MAX_POSITION_EMBEDDINGS = 768\n",
    "IS_HF_MODEL = False\n",
    "IS_ENCODER_DECODER_MODEL = False\n",
    "EPOCH_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI3C_CONFIG = ci3C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            forward_method=\"chunkwise\",\n",
    "            apply_decay=False,\n",
    "            num_decay_parts=1,\n",
    "            hidden_retention_act=\"relu\",\n",
    "            hidden_pos_offset=True,\n",
    "            rope_dim=16,\n",
    "            num_query_heads=2,\n",
    "\n",
    "            decoder_output=\"none\",\n",
    "            revert_decoder=False,\n",
    "            decoder_schema=[1, 1],\n",
    "            cross_encoder_schema=[0, 0],\n",
    "            block_io_schema=None,#[[1024, 1024*4, 1024], [1024, 1024*2, 1024]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    model = ci3C.COINForConditionalGeneration(\n",
    "        CI3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num layers: 2\n",
      "gamma schema: [[0.96875, 0.9875984191894531], [0.995078444480896, 0.998046875]]\n",
      "layer 0 num experts: 1\n",
      "layer 1 num experts: 1\n"
     ]
    }
   ],
   "source": [
    "if 0:\n",
    "    model = ci3C.COINForMaskedLM(\n",
    "        CI3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI3A_CONFIG = ci3A.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    forward_method=\"chunkwise\",\n",
    "    apply_decay=False,\n",
    "    num_decay_parts=1,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    apply_hidden_pos_offset=True,\n",
    "\n",
    "    decoder_output=\"none\",\n",
    "    revert_decoder=False,\n",
    "    decoder_schema=[1] * 2,\n",
    "    cross_encoder_schema=[0] * 2,\n",
    "    experts_schema=None,\n",
    "\n",
    "    switch_ii_decoder_ii=SWITCH_II_DECODER_II,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3A.COINForMaskedLM(\n",
    "        CI3A_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3A.COINForConditionalGeneration(\n",
    "        CI3A_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REGIONS = 1\n",
    "COIN_CONFIG = ci2D.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    #hidden_out_act=None,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=False,\n",
    "    reverse_decay=False,\n",
    "    num_decay_parts=1,\n",
    "    decoder_output=\"strict\",\n",
    "    #rope_dim=16,\n",
    "    \n",
    "    num_regions=NUM_REGIONS,\n",
    "    decoder_schema=      [1],\n",
    "    cross_encoder_schema=[0],\n",
    "    \n",
    "    share_S=False,\n",
    "    \n",
    "    #layer_norm_eps=1e-12,\n",
    "    #retention_group_norm_eps=1e-8,\n",
    "    #rms_norm_eps=1e-12,\n",
    "    switch_ii_decoder_ii=SWITCH_II_DECODER_II,\n",
    "    disable_teacher_forcing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci2D.COINForMaskedLM(COIN_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REGIONS = 1\n",
    "COIN_CONFIG = ci2C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    #hidden_out_act=None,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=True,\n",
    "    #fixed_decay_value=None,\n",
    "    num_decay_parts=1,\n",
    "    #reverse_decay=False,\n",
    "    #chunkwise_num_chunks=1,\n",
    "    #apply_chunking_globally=False,\n",
    "    #apply_hidden_pos_offset=False,\n",
    "    decoder_output=\"none\",\n",
    "    \n",
    "    num_regions=NUM_REGIONS,\n",
    "    decoder_schema=[0, 0],\n",
    "    cross_encoder_schema=[0] * 2,\n",
    "    multi_head_qkv=False,\n",
    "    #num_heads=16,\n",
    "    #share_S=False,\n",
    "    num_repetitions=1,\n",
    "\n",
    "    #rms_norm_eps=1e-8,\n",
    "\n",
    "    disable_teacher_forcing=False,\n",
    "    switch_ii_decoder_ii=SWITCH_II_DECODER_II,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci2C.COINForMaskedLM(\n",
    "        config=COIN_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci2C.COINForConditionalGeneration(\n",
    "        config=COIN_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    model = transformers.XLMWithLMHeadModel(\n",
    "        config=transformers.XLMConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148,148,058\n",
      "148,148,026\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,}\\n{:,}\".format(num_parameters(model), num_trainable_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2//model/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2//wordpiece_tokenizer/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2//bpe_tokenizer/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2//sentence_piece_tokenizer/'\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2//dataset/'\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "try:\n",
    "    os.mkdir(BASE_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(CHECKPOINT_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(WORDPIECE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(BPE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(SENTENCE_PIECE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(DATASET_JSON_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#dataset = DatasetDict({\n",
    "#    \"train\": load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train[0:10000]\"),\n",
    "#    \"test\":  load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"validation[:1500]\")\n",
    "#})\n",
    "\n",
    "#dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oasst1, aaabdon, mcca, translation_mcca\n",
    "DATASET = \"translation_mcca\"\n",
    "#DATASET = \"oasst1\"\n",
    "\n",
    "#HF_TRAIN_ROWS = 25_000\n",
    "HF_TRAIN_ROWS = 500_000\n",
    "#HF_TRAIN_ROWS = -1\n",
    "HF_TRAIN_FROM = 0#10_000\n",
    "\n",
    "HF_TEST_ROWS = 1_500#5000\n",
    "#HF_TEST_ROWS = -1\n",
    "HF_TEST_FROM = 0\n",
    "\n",
    "CUSTOM_BASE_DS_PATH = \"../datasets/big_AAABDON_Nmax_st200_s0_a10_tvsplit.1_no_norm/\"\n",
    "CUSTOM_DS_TO_FILE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
      "    num_rows: 1500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
      "    num_rows: 39283\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "if DATASET == \"mcca\":\n",
    "    train_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-2][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "    test_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "        ]\n",
    "    })\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"translation_mcca\":\n",
    "    train_dataset = load_moses_set({\n",
    "        \"src\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-5][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-5][0-9]\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-5][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-5][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "    test_dataset = load_moses_set({\n",
    "        \"src\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x800[0-1]\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x800[0-1]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "        ]\n",
    "    })\n",
    "    tok_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-9][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-9][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "elif DATASET == \"aaabdon\":\n",
    "    train_ds = [f\"{CUSTOM_BASE_DS_PATH}/train/train_00[0-{CUSTOM_DS_TO_FILE}].csv\"]\n",
    "    test_ds = [f\"{CUSTOM_BASE_DS_PATH}/validation/validation_00[0-{CUSTOM_DS_TO_FILE}].csv\"]\n",
    "    train_dataset = load_set(train_ds)#.select(list(range(HF_TRAIN_ROWS)))\n",
    "    test_dataset = load_set(test_ds)#.select(list(range(HF_TEST_ROWS)))\n",
    "    tok_dataset = load_set([f\"{CUSTOM_BASE_DS_PATH}/train/train_*.csv\"])\n",
    "elif DATASET == \"oasst1\":\n",
    "    #train_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=f\"train[0:{HF_TRAIN_ROWS}]\")\n",
    "    #test_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=f\"validation[:{HF_TEST_ROWS}]\")\n",
    "    #tok_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")\n",
    "    \n",
    "    #train_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\", split=f\"train[0:{HF_TRAIN_ROWS}]\")  .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "    #test_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\", split=f\"test[0:{HF_TEST_ROWS}]\")     .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "    #tok_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\")                                       .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "\n",
    "    train_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    test_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"validation\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    tok_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    #train_dataset = load_from_disk(\"../datasets/oasst1/train\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TRAIN_ROWS)))\n",
    "    #test_dataset = load_from_disk(\"../datasets/oasst1/validation\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TEST_ROWS)))\n",
    "    #tok_dataset = load_from_disk(\"../datasets/oasst1/train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "\n",
    "    #train_dataset = load_dataset(\"OpenAssistant/oasst2\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TRAIN_ROWS)))\n",
    "    #test_dataset = load_dataset(\"OpenAssistant/oasst2\", split=\"validation\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TEST_ROWS)))\n",
    "\n",
    "if SHUFFLE_TRAIN_DATA:\n",
    "    print(\"shuffle\")\n",
    "    train_dataset = train_dataset.shuffle()\n",
    "if HF_TRAIN_ROWS > 0:\n",
    "    train_dataset =  train_dataset.select(list(range(HF_TRAIN_FROM, HF_TRAIN_ROWS)))\n",
    "if SHUFFLE_TEST_DATA:\n",
    "    print(\"shuffle\")\n",
    "    test_dataset = test_dataset.shuffle()\n",
    "if HF_TEST_ROWS > 0:\n",
    "    test_dataset = test_dataset.select(list(range(HF_TEST_FROM, HF_TEST_ROWS)))\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "print(tok_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 25/25 [00:01<00:00, 18.74ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 26.05ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2309275"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_json(f\"{DATASET_JSON_PATH}/train.json\")\n",
    "test_dataset.to_json(f\"{DATASET_JSON_PATH}/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message_id': 0, 'parent_id': 1, 'user_id': 2, 'created_date': 3, 'role': 4, 'lang': 5, 'review_count': 6, 'review_result': 7, 'deleted': 8, 'rank': 9, 'synthetic': 10, 'model_name': 11, 'detoxify': 12, 'message_tree_id': 13, 'tree_state': 14, 'emojis': 15, 'labels': 16}\n",
      "{0: 'message_id', 1: 'parent_id', 2: 'user_id', 3: 'created_date', 4: 'role', 5: 'lang', 6: 'review_count', 7: 'review_result', 8: 'deleted', 9: 'rank', 10: 'synthetic', 11: 'model_name', 12: 'detoxify', 13: 'message_tree_id', 14: 'tree_state', 15: 'emojis', 16: 'labels'}\n",
      "['message_id', 'parent_id', 'user_id', 'created_date', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels']\n"
     ]
    }
   ],
   "source": [
    "labels = [label for label in train_dataset.features.keys() if label not in [\"text\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, BertWordPieceTokenizer, SentencePieceBPETokenizer, SentencePieceUnigramTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    #tok_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")\n",
    "    #tok_dataset = load_dataset(\"glue\", name=\"sst2\", split=\"train\")\n",
    "    #tok_dataset = tok_dataset.rename_column(\"sentence\", \"text\")\n",
    "    \n",
    "    tokenizer = SentencePieceUnigramTokenizer()\n",
    "\n",
    "    tokenizer.train_from_iterator(\n",
    "        iterator=tok_dataset[\"text\"], \n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        #min_frequency=2,\n",
    "        show_progress=True,\n",
    "        #limit_alphabet=500,\n",
    "        special_tokens=[\n",
    "            \"<PAD>\", \n",
    "            \"<UNK>\", \n",
    "            \"<CLS>\", \n",
    "            \"<SEP>\", \n",
    "            \"<DOC>\",\n",
    "            \"<MASK>\"\n",
    "        ])\n",
    "\n",
    "    tokenizer = PreTrainedTokenizer(\n",
    "        tokenizer_object=tokenizer\n",
    "    )\n",
    "    tokenizer.save_model(SENTENCE_PIECE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(SENTENCE_PIECE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if 1:\n",
    "    tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True,\n",
    "                                        strip_accents=True, lowercase=True)\n",
    "\n",
    "    tokenizer.train_from_iterator(iterator=tok_dataset[\"text\"], vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[\n",
    "        \"[PAD]\", \n",
    "        \"[UNK]\", \n",
    "        \"[CLS]\", \n",
    "        \"[SEP]\", \n",
    "        \"[DOC]\",\n",
    "    #    \"[UDOC]\",\n",
    "        \"[MASK]\"\n",
    "    ])\n",
    "    tokenizer.save_model(WORDPIECE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    tokenizer = BertTokenizer.from_pretrained(WORDPIECE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "    \n",
    "    tokenizer.train_from_iterator(iterator=tok_dataset[\"text\"], vocab_size=VOCAB_SIZE, min_frequency=2, length=MAX_POSITION_EMBEDDINGS, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<doc>\",\n",
    "        \"<mask>\",\n",
    "        \n",
    "        #\"<pad>\",\n",
    "        #\"<unk>\",\n",
    "        #\"<cls>\",\n",
    "        #\"<sep>\",\n",
    "        #\"<doc>\",\n",
    "        #\"<mask>\",\n",
    "    ])\n",
    "\n",
    "    # Save files to disk\n",
    "    tokenizer.save_model(BPE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(BPE_TOKENIZER_DIR)\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(BPE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def encode_and_batch(dataset, tokenizer, max_position_embeddings, batch_size, shuffle=False):\n",
    "    if DATASET in (\"translation_mcca\"):\n",
    "        encoded = preprocess_for_translation(dataset, tokenizer, max_position_embeddings, source_lang=\"src\", target_lang=\"tgt\", prefix=PREFIX, num_proc=4, remove_columns=[\"src\", \"tgt\"], switch_ii_decoder_ii=SWITCH_II_DECODER_II)\n",
    "    elif ENCODE_CAUSAL_LM:\n",
    "        encoded = preprocess_for_causallm(dataset, tokenizer, block_size=MAX_POSITION_EMBEDDINGS, remove_columns=dataset.column_names, shift_right=False)\n",
    "    else:\n",
    "        encoded = preprocess_for_maskedlm(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names, to_mask=.15, chance_rand_token=.2, \n",
    "                                          group_texts=GROUP_TEXTS, mask_token=MASK_TOKEN, pad_token=PAD_TOKEN, sparsify=SPARSIFY, prefix=PREFIX, switch_ii_decoder_ii=SWITCH_II_DECODER_II)\n",
    "        #encoded = preprocess_for_cot(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names, group_texts=GROUP_TEXTS, pad_token=PAD_TOKEN, sparsify=SPARSIFY, prefix=PREFIX)\n",
    "       \n",
    "       \n",
    "        #encoded = preprocess_for_monologe(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names)\n",
    "        #encoded = preprocess_for_sparselm(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names)\n",
    "        #encoded = preprocess_for_binary_sparselm(dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names)\n",
    "        #encoded = preprocess_for_maskedlm(encoded, tokenizer, max_position_embeddings, to_mask=.15, chance_rand_token=.2, group_texts=GROUP_TEXTS, mask_token=MASK_TOKEN)\n",
    "    print(encoded)\n",
    "    batched = BatchBuffer(encoded, batch_size)\n",
    "    if shuffle:\n",
    "        batched.shuffle()\n",
    "    print(\"  finished\")\n",
    "    return batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 25000/25000 [00:07<00:00, 3440.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'decoder_input_ids'],\n",
      "    num_rows: 5844\n",
      "})\n",
      "  finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 1500/1500 [00:01<00:00, 1421.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'decoder_input_ids'],\n",
      "    num_rows: 371\n",
      "})\n",
      "  finished\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "train_loader_call = lambda: encode_and_batch(train_dataset, tokenizer, MAX_POSITION_EMBEDDINGS, TRAIN_BATCH_SIZE, True)\n",
    "test_loader_call = lambda: encode_and_batch(test_dataset, tokenizer, MAX_POSITION_EMBEDDINGS, TEST_BATCH_SIZE)\n",
    "\n",
    "if not REBATCH:\n",
    "    train_dataloader = train_loader_call()\n",
    "    test_dataloader = test_loader_call()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#encoded_train_dataset = preprocess_for_maskedlm(dataset, tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=train_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#train_dataloader = BatchBuffer(encoded_dataset[\"train\"], BATCH_SIZE).shuffle()\n",
    "#test_dataloader = BatchBuffer(encoded_dataset[\"test\"], BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#batch_schema = list(encoded_dataset[\"train\"].features.keys())\n",
    "#batch_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def count_item(inp, item):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for n in inp:\n",
    "        for r in n:\n",
    "            i = r\n",
    "            if not i < 4:\n",
    "                total += 1\n",
    "            if i == item:\n",
    "                count += 1\n",
    "            #if i != 0 and i != item:\n",
    "            #    print(i)\n",
    "    return f\"{count} / {total} ; {count/total}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked tokens [input_ids]: 350322 / 2942304 ; 0.11906383568795066\n",
      "masked tokens [labels]: 0 / 186934 ; 0.0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"masked tokens [input_ids]:\", count_item(train_dataloader.ds[\"input_ids\"], tokenizer.mask_token_id))\n",
    "print(\"masked tokens [labels]:\", count_item(test_dataloader.ds[\"labels\"], tokenizer.mask_token_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "warmup_steps = math.ceil(total_steps * 0.05)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels',\n",
       " 'input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask',\n",
       " 'decoder_input_ids']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "train_dataloader.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'decoder_input_ids']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(train_dataloader.schema)\n",
    "its = 0\n",
    "for i, n in enumerate(train_dataloader):\n",
    "    if i >= its:\n",
    "        break\n",
    "    for k in n:\n",
    "        print(i, \"############\")\n",
    "        for l in k:\n",
    "            print(len(l))\n",
    "        print(\"##############\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COINForMaskedLM(\n",
       "  (coin): COINModel(\n",
       "    (encoder_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (residual_embeddings): COINEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (stack): COINStack(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x COINLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): COINBlock(\n",
       "              (cross_qkv): MultiQueryQKV(\n",
       "                (rope): RotaryEmbedding()\n",
       "                (xpos): XPOS()\n",
       "                (act): ReLU()\n",
       "                (out_act): ReLU()\n",
       "                (group_norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (proj_G): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (cross_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (qkv): MultiQueryQKV(\n",
       "                (rope): RotaryEmbedding()\n",
       "                (xpos): XPOS()\n",
       "                (act): ReLU()\n",
       "                (out_act): ReLU()\n",
       "                (group_norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (proj_G): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): COINPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (act): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): COINLMPredictionHead(\n",
       "    (transform): COINPredictionHeadTransform(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (transform_act_fn): Tanh()\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=1024, out_features=30522, bias=True)\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "\n",
      "Generated batch schema as ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'decoder_input_ids']\n",
      "\n",
      "Training...\n",
      "  Batch    24  of    974.    Elapsed:  0:00:11, Remaining:  0:07:15.\n",
      "  Batch    48  of    974.    Elapsed:  0:00:23, Remaining:  0:07:04.\n",
      "  Batch    72  of    974.    Elapsed:  0:00:34, Remaining:  0:06:53.\n",
      "  Batch    96  of    974.    Elapsed:  0:00:45, Remaining:  0:06:42.\n",
      "  Batch   120  of    974.    Elapsed:  0:00:57, Remaining:  0:06:31.\n",
      "  Batch   144  of    974.    Elapsed:  0:01:08, Remaining:  0:06:55.\n",
      "  Batch   168  of    974.    Elapsed:  0:01:20, Remaining:  0:06:43.\n",
      "  Batch   192  of    974.    Elapsed:  0:01:31, Remaining:  0:05:58.\n",
      "  Batch   216  of    974.    Elapsed:  0:01:43, Remaining:  0:05:47.\n",
      "  Batch   240  of    974.    Elapsed:  0:01:54, Remaining:  0:05:36.\n",
      "  Batch   264  of    974.    Elapsed:  0:02:05, Remaining:  0:05:25.\n",
      "  Batch   288  of    974.    Elapsed:  0:02:17, Remaining:  0:05:14.\n",
      "  Batch   312  of    974.    Elapsed:  0:02:28, Remaining:  0:05:31.\n",
      "  Batch   336  of    974.    Elapsed:  0:02:40, Remaining:  0:05:19.\n",
      "  Batch   360  of    974.    Elapsed:  0:02:51, Remaining:  0:04:41.\n",
      "  Batch   384  of    974.    Elapsed:  0:03:03, Remaining:  0:04:30.\n",
      "  Batch   408  of    974.    Elapsed:  0:03:14, Remaining:  0:04:19.\n",
      "  Batch   432  of    974.    Elapsed:  0:03:26, Remaining:  0:04:31.\n",
      "  Batch   456  of    974.    Elapsed:  0:03:37, Remaining:  0:03:57.\n",
      "  Batch   480  of    974.    Elapsed:  0:03:49, Remaining:  0:03:46.\n",
      "  Batch   504  of    974.    Elapsed:  0:04:00, Remaining:  0:03:35.\n",
      "  Batch   528  of    974.    Elapsed:  0:04:11, Remaining:  0:03:24.\n",
      "  Batch   552  of    974.    Elapsed:  0:04:23, Remaining:  0:03:13.\n",
      "  Batch   576  of    974.    Elapsed:  0:04:34, Remaining:  0:03:02.\n",
      "  Batch   600  of    974.    Elapsed:  0:04:46, Remaining:  0:02:51.\n",
      "  Batch   624  of    974.    Elapsed:  0:04:57, Remaining:  0:02:40.\n",
      "  Batch   648  of    974.    Elapsed:  0:05:09, Remaining:  0:02:29.\n",
      "  Batch   672  of    974.    Elapsed:  0:05:20, Remaining:  0:02:31.\n",
      "  Batch   696  of    974.    Elapsed:  0:05:31, Remaining:  0:02:07.\n",
      "  Batch   720  of    974.    Elapsed:  0:05:43, Remaining:  0:01:56.\n",
      "  Batch   744  of    974.    Elapsed:  0:05:54, Remaining:  0:01:45.\n",
      "  Batch   768  of    974.    Elapsed:  0:06:05, Remaining:  0:01:34.\n",
      "  Batch   792  of    974.    Elapsed:  0:06:17, Remaining:  0:01:23.\n",
      "  Batch   816  of    974.    Elapsed:  0:06:28, Remaining:  0:01:12.\n",
      "  Batch   840  of    974.    Elapsed:  0:06:39, Remaining:  0:01:01.\n",
      "  Batch   864  of    974.    Elapsed:  0:06:51, Remaining:  0:00:50.\n",
      "  Batch   888  of    974.    Elapsed:  0:07:02, Remaining:  0:00:39.\n",
      "  Batch   912  of    974.    Elapsed:  0:07:14, Remaining:  0:00:28.\n",
      "  Batch   936  of    974.    Elapsed:  0:07:25, Remaining:  0:00:17.\n",
      "  Batch   960  of    974.    Elapsed:  0:07:37, Remaining:  0:00:07.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 7.118097979430056\n",
      "  Training epoch took: 0:07:43\n",
      "\n",
      "Running Testing...\n",
      "  Average testing scores:\n",
      "    loss: 4.956859307210953\n",
      "  Testing took: 0:00:21\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2//model//epoch_0/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    24  of    974.    Elapsed:  0:00:11, Remaining:  0:07:15.\n",
      "  Batch    48  of    974.    Elapsed:  0:00:23, Remaining:  0:07:04.\n",
      "  Batch    72  of    974.    Elapsed:  0:00:34, Remaining:  0:06:53.\n",
      "  Batch    96  of    974.    Elapsed:  0:00:46, Remaining:  0:07:19.\n",
      "  Batch   120  of    974.    Elapsed:  0:00:57, Remaining:  0:06:31.\n",
      "  Batch   144  of    974.    Elapsed:  0:01:09, Remaining:  0:06:20.\n",
      "  Batch   168  of    974.    Elapsed:  0:01:20, Remaining:  0:06:09.\n",
      "  Batch   192  of    974.    Elapsed:  0:01:31, Remaining:  0:05:58.\n",
      "  Batch   216  of    974.    Elapsed:  0:01:43, Remaining:  0:05:47.\n",
      "  Batch   240  of    974.    Elapsed:  0:01:54, Remaining:  0:05:36.\n",
      "  Batch   264  of    974.    Elapsed:  0:02:05, Remaining:  0:05:25.\n",
      "  Batch   288  of    974.    Elapsed:  0:02:17, Remaining:  0:05:14.\n",
      "  Batch   312  of    974.    Elapsed:  0:02:28, Remaining:  0:05:03.\n",
      "  Batch   336  of    974.    Elapsed:  0:02:40, Remaining:  0:04:52.\n",
      "  Batch   360  of    974.    Elapsed:  0:02:51, Remaining:  0:04:41.\n",
      "  Batch   384  of    974.    Elapsed:  0:03:02, Remaining:  0:04:30.\n",
      "  Batch   408  of    974.    Elapsed:  0:03:14, Remaining:  0:04:19.\n",
      "  Batch   432  of    974.    Elapsed:  0:03:25, Remaining:  0:04:08.\n",
      "  Batch   456  of    974.    Elapsed:  0:03:37, Remaining:  0:03:57.\n",
      "  Batch   480  of    974.    Elapsed:  0:03:48, Remaining:  0:03:46.\n",
      "  Batch   504  of    974.    Elapsed:  0:03:59, Remaining:  0:03:35.\n",
      "  Batch   528  of    974.    Elapsed:  0:04:11, Remaining:  0:03:24.\n",
      "  Batch   552  of    974.    Elapsed:  0:04:22, Remaining:  0:03:13.\n",
      "  Batch   576  of    974.    Elapsed:  0:04:33, Remaining:  0:03:02.\n",
      "  Batch   600  of    974.    Elapsed:  0:04:44, Remaining:  0:02:51.\n",
      "  Batch   624  of    974.    Elapsed:  0:04:55, Remaining:  0:02:40.\n",
      "  Batch   648  of    974.    Elapsed:  0:05:07, Remaining:  0:02:29.\n",
      "  Batch   672  of    974.    Elapsed:  0:05:18, Remaining:  0:02:18.\n",
      "  Batch   696  of    974.    Elapsed:  0:05:29, Remaining:  0:02:07.\n",
      "  Batch   720  of    974.    Elapsed:  0:05:41, Remaining:  0:01:56.\n",
      "  Batch   744  of    974.    Elapsed:  0:05:52, Remaining:  0:01:45.\n",
      "  Batch   768  of    974.    Elapsed:  0:06:04, Remaining:  0:01:34.\n",
      "  Batch   792  of    974.    Elapsed:  0:06:15, Remaining:  0:01:23.\n",
      "  Batch   816  of    974.    Elapsed:  0:06:26, Remaining:  0:01:12.\n",
      "  Batch   840  of    974.    Elapsed:  0:06:37, Remaining:  0:01:01.\n",
      "  Batch   864  of    974.    Elapsed:  0:06:49, Remaining:  0:00:50.\n",
      "  Batch   888  of    974.    Elapsed:  0:07:00, Remaining:  0:00:39.\n",
      "  Batch   912  of    974.    Elapsed:  0:07:11, Remaining:  0:00:28.\n",
      "  Batch   936  of    974.    Elapsed:  0:07:22, Remaining:  0:00:17.\n",
      "  Batch   960  of    974.    Elapsed:  0:07:33, Remaining:  0:00:06.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 4.239031622052438\n",
      "  Training epoch took: 0:07:40\n",
      "\n",
      "Running Testing...\n",
      "  Average testing scores:\n",
      "    loss: 3.4517435597591715\n",
      "  Testing took: 0:00:21\n",
      "[Errno 17] File exists: 'tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2//model//epoch_1/'\n",
      "model dumped\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    24  of    974.    Elapsed:  0:00:11, Remaining:  0:07:15.\n",
      "  Batch    48  of    974.    Elapsed:  0:00:22, Remaining:  0:07:04.\n",
      "  Batch    72  of    974.    Elapsed:  0:00:33, Remaining:  0:06:53.\n",
      "  Batch    96  of    974.    Elapsed:  0:00:44, Remaining:  0:06:42.\n",
      "  Batch   120  of    974.    Elapsed:  0:00:55, Remaining:  0:06:31.\n",
      "  Batch   144  of    974.    Elapsed:  0:01:05, Remaining:  0:06:20.\n",
      "  Batch   168  of    974.    Elapsed:  0:01:16, Remaining:  0:06:09.\n",
      "  Batch   192  of    974.    Elapsed:  0:01:27, Remaining:  0:05:58.\n",
      "  Batch   216  of    974.    Elapsed:  0:01:38, Remaining:  0:05:47.\n",
      "  Batch   240  of    974.    Elapsed:  0:01:49, Remaining:  0:05:36.\n",
      "  Batch   264  of    974.    Elapsed:  0:02:00, Remaining:  0:05:25.\n",
      "  Batch   288  of    974.    Elapsed:  0:02:11, Remaining:  0:05:14.\n",
      "  Batch   312  of    974.    Elapsed:  0:02:22, Remaining:  0:05:03.\n",
      "  Batch   336  of    974.    Elapsed:  0:02:33, Remaining:  0:04:52.\n",
      "  Batch   360  of    974.    Elapsed:  0:02:44, Remaining:  0:04:41.\n",
      "  Batch   384  of    974.    Elapsed:  0:02:56, Remaining:  0:04:30.\n",
      "  Batch   408  of    974.    Elapsed:  0:03:07, Remaining:  0:04:19.\n",
      "  Batch   432  of    974.    Elapsed:  0:03:18, Remaining:  0:04:08.\n",
      "  Batch   456  of    974.    Elapsed:  0:03:29, Remaining:  0:03:57.\n",
      "  Batch   480  of    974.    Elapsed:  0:03:40, Remaining:  0:03:46.\n",
      "  Batch   504  of    974.    Elapsed:  0:03:51, Remaining:  0:03:35.\n",
      "  Batch   528  of    974.    Elapsed:  0:04:03, Remaining:  0:03:24.\n",
      "  Batch   552  of    974.    Elapsed:  0:04:14, Remaining:  0:03:13.\n",
      "  Batch   576  of    974.    Elapsed:  0:04:25, Remaining:  0:03:02.\n",
      "  Batch   600  of    974.    Elapsed:  0:04:36, Remaining:  0:02:51.\n",
      "  Batch   624  of    974.    Elapsed:  0:04:47, Remaining:  0:02:40.\n",
      "  Batch   648  of    974.    Elapsed:  0:04:58, Remaining:  0:02:29.\n",
      "  Batch   672  of    974.    Elapsed:  0:05:09, Remaining:  0:02:18.\n",
      "  Batch   696  of    974.    Elapsed:  0:05:20, Remaining:  0:02:07.\n",
      "  Batch   720  of    974.    Elapsed:  0:05:32, Remaining:  0:01:56.\n",
      "  Batch   744  of    974.    Elapsed:  0:05:43, Remaining:  0:01:45.\n",
      "  Batch   768  of    974.    Elapsed:  0:05:54, Remaining:  0:01:34.\n",
      "  Batch   792  of    974.    Elapsed:  0:06:05, Remaining:  0:01:23.\n",
      "  Batch   816  of    974.    Elapsed:  0:06:16, Remaining:  0:01:12.\n",
      "  Batch   840  of    974.    Elapsed:  0:06:27, Remaining:  0:01:01.\n",
      "  Batch   864  of    974.    Elapsed:  0:06:39, Remaining:  0:00:50.\n",
      "  Batch   888  of    974.    Elapsed:  0:06:50, Remaining:  0:00:39.\n",
      "  Batch   912  of    974.    Elapsed:  0:07:01, Remaining:  0:00:28.\n",
      "  Batch   936  of    974.    Elapsed:  0:07:12, Remaining:  0:00:17.\n",
      "  Batch   960  of    974.    Elapsed:  0:07:23, Remaining:  0:00:06.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 3.000057064409863\n",
      "  Training epoch took: 0:07:30\n",
      "\n",
      "Running Testing...\n",
      "  Average testing scores:\n",
      "    loss: 2.4989710694453757\n",
      "  Testing took: 0:00:21\n",
      "model dumped\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    24  of    974.    Elapsed:  0:00:11, Remaining:  0:07:15.\n",
      "  Batch    48  of    974.    Elapsed:  0:00:22, Remaining:  0:07:04.\n",
      "  Batch    72  of    974.    Elapsed:  0:00:33, Remaining:  0:06:53.\n",
      "  Batch    96  of    974.    Elapsed:  0:00:45, Remaining:  0:06:42.\n",
      "  Batch   120  of    974.    Elapsed:  0:00:56, Remaining:  0:06:31.\n",
      "  Batch   144  of    974.    Elapsed:  0:01:07, Remaining:  0:06:20.\n",
      "  Batch   168  of    974.    Elapsed:  0:01:18, Remaining:  0:06:09.\n",
      "  Batch   192  of    974.    Elapsed:  0:01:30, Remaining:  0:05:58.\n",
      "  Batch   216  of    974.    Elapsed:  0:01:41, Remaining:  0:05:47.\n",
      "  Batch   240  of    974.    Elapsed:  0:01:52, Remaining:  0:05:36.\n",
      "  Batch   264  of    974.    Elapsed:  0:02:03, Remaining:  0:05:25.\n",
      "  Batch   288  of    974.    Elapsed:  0:02:15, Remaining:  0:05:14.\n",
      "  Batch   312  of    974.    Elapsed:  0:02:26, Remaining:  0:05:03.\n",
      "  Batch   336  of    974.    Elapsed:  0:02:37, Remaining:  0:04:52.\n",
      "  Batch   360  of    974.    Elapsed:  0:02:48, Remaining:  0:04:41.\n",
      "  Batch   384  of    974.    Elapsed:  0:02:59, Remaining:  0:04:30.\n",
      "  Batch   408  of    974.    Elapsed:  0:03:10, Remaining:  0:04:19.\n",
      "  Batch   432  of    974.    Elapsed:  0:03:22, Remaining:  0:04:08.\n",
      "  Batch   456  of    974.    Elapsed:  0:03:33, Remaining:  0:03:57.\n",
      "  Batch   480  of    974.    Elapsed:  0:03:43, Remaining:  0:03:46.\n",
      "  Batch   504  of    974.    Elapsed:  0:03:54, Remaining:  0:03:35.\n",
      "  Batch   528  of    974.    Elapsed:  0:04:05, Remaining:  0:03:24.\n",
      "  Batch   552  of    974.    Elapsed:  0:04:15, Remaining:  0:02:56.\n",
      "  Batch   576  of    974.    Elapsed:  0:04:25, Remaining:  0:02:46.\n",
      "  Batch   600  of    974.    Elapsed:  0:04:36, Remaining:  0:02:51.\n",
      "  Batch   624  of    974.    Elapsed:  0:04:46, Remaining:  0:02:40.\n",
      "  Batch   648  of    974.    Elapsed:  0:04:57, Remaining:  0:02:16.\n",
      "  Batch   672  of    974.    Elapsed:  0:05:07, Remaining:  0:02:06.\n",
      "  Batch   696  of    974.    Elapsed:  0:05:17, Remaining:  0:01:56.\n",
      "  Batch   720  of    974.    Elapsed:  0:05:28, Remaining:  0:01:56.\n",
      "  Batch   744  of    974.    Elapsed:  0:05:38, Remaining:  0:01:36.\n",
      "  Batch   768  of    974.    Elapsed:  0:05:49, Remaining:  0:01:26.\n",
      "  Batch   792  of    974.    Elapsed:  0:05:59, Remaining:  0:01:16.\n",
      "  Batch   816  of    974.    Elapsed:  0:06:09, Remaining:  0:01:06.\n",
      "  Batch   840  of    974.    Elapsed:  0:06:20, Remaining:  0:01:01.\n",
      "  Batch   864  of    974.    Elapsed:  0:06:30, Remaining:  0:00:50.\n",
      "  Batch   888  of    974.    Elapsed:  0:06:42, Remaining:  0:00:39.\n",
      "  Batch   912  of    974.    Elapsed:  0:06:53, Remaining:  0:00:28.\n",
      "  Batch   936  of    974.    Elapsed:  0:07:04, Remaining:  0:00:17.\n",
      "  Batch   960  of    974.    Elapsed:  0:07:15, Remaining:  0:00:06.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 2.1999496201959725\n",
      "  Training epoch took: 0:07:21\n",
      "\n",
      "Running Testing...\n",
      "  Average testing scores:\n",
      "    loss: 1.98489504954854\n",
      "  Testing took: 0:00:20\n",
      "model dumped\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    24  of    974.    Elapsed:  0:00:11, Remaining:  0:07:15.\n",
      "  Batch    48  of    974.    Elapsed:  0:00:22, Remaining:  0:07:04.\n",
      "  Batch    72  of    974.    Elapsed:  0:00:33, Remaining:  0:06:53.\n",
      "  Batch    96  of    974.    Elapsed:  0:00:44, Remaining:  0:06:42.\n",
      "  Batch   120  of    974.    Elapsed:  0:00:55, Remaining:  0:06:31.\n",
      "  Batch   144  of    974.    Elapsed:  0:01:06, Remaining:  0:06:20.\n",
      "  Batch   168  of    974.    Elapsed:  0:01:17, Remaining:  0:06:09.\n",
      "  Batch   192  of    974.    Elapsed:  0:01:28, Remaining:  0:05:58.\n",
      "  Batch   216  of    974.    Elapsed:  0:01:39, Remaining:  0:05:47.\n",
      "  Batch   240  of    974.    Elapsed:  0:01:50, Remaining:  0:05:36.\n",
      "  Batch   264  of    974.    Elapsed:  0:02:00, Remaining:  0:05:25.\n",
      "  Batch   288  of    974.    Elapsed:  0:02:11, Remaining:  0:05:14.\n",
      "  Batch   312  of    974.    Elapsed:  0:02:22, Remaining:  0:05:03.\n",
      "  Batch   336  of    974.    Elapsed:  0:02:33, Remaining:  0:04:52.\n",
      "  Batch   360  of    974.    Elapsed:  0:02:44, Remaining:  0:04:41.\n",
      "  Batch   384  of    974.    Elapsed:  0:02:54, Remaining:  0:04:06.\n",
      "  Batch   408  of    974.    Elapsed:  0:03:05, Remaining:  0:04:19.\n",
      "  Batch   432  of    974.    Elapsed:  0:03:16, Remaining:  0:04:08.\n",
      "  Batch   456  of    974.    Elapsed:  0:03:27, Remaining:  0:03:57.\n",
      "  Batch   480  of    974.    Elapsed:  0:03:37, Remaining:  0:03:46.\n",
      "  Batch   504  of    974.    Elapsed:  0:03:47, Remaining:  0:03:16.\n",
      "  Batch   528  of    974.    Elapsed:  0:03:58, Remaining:  0:03:06.\n",
      "  Batch   552  of    974.    Elapsed:  0:04:08, Remaining:  0:02:56.\n",
      "  Batch   576  of    974.    Elapsed:  0:04:19, Remaining:  0:03:02.\n",
      "  Batch   600  of    974.    Elapsed:  0:04:29, Remaining:  0:02:36.\n",
      "  Batch   624  of    974.    Elapsed:  0:04:39, Remaining:  0:02:26.\n",
      "  Batch   648  of    974.    Elapsed:  0:04:50, Remaining:  0:02:16.\n",
      "  Batch   672  of    974.    Elapsed:  0:05:00, Remaining:  0:02:18.\n",
      "  Batch   696  of    974.    Elapsed:  0:05:11, Remaining:  0:02:07.\n",
      "  Batch   720  of    974.    Elapsed:  0:05:22, Remaining:  0:01:46.\n",
      "  Batch   744  of    974.    Elapsed:  0:05:32, Remaining:  0:01:45.\n",
      "  Batch   768  of    974.    Elapsed:  0:05:43, Remaining:  0:01:34.\n",
      "  Batch   792  of    974.    Elapsed:  0:05:54, Remaining:  0:01:23.\n",
      "  Batch   816  of    974.    Elapsed:  0:06:05, Remaining:  0:01:12.\n",
      "  Batch   840  of    974.    Elapsed:  0:06:16, Remaining:  0:01:01.\n",
      "  Batch   864  of    974.    Elapsed:  0:06:27, Remaining:  0:00:50.\n",
      "  Batch   888  of    974.    Elapsed:  0:06:37, Remaining:  0:00:39.\n",
      "  Batch   912  of    974.    Elapsed:  0:06:48, Remaining:  0:00:28.\n",
      "  Batch   936  of    974.    Elapsed:  0:06:59, Remaining:  0:00:17.\n",
      "  Batch   960  of    974.    Elapsed:  0:07:10, Remaining:  0:00:06.\n",
      "\n",
      "  Average training scores:\n",
      "    loss: 1.7681358940057932\n",
      "  Training epoch took: 0:07:16\n",
      "\n",
      "Running Testing...\n",
      "  Average testing scores:\n",
      "    loss: 1.7043441455872332\n",
      "  Testing took: 0:00:20\n",
      "model dumped\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "\n",
      "Training...\n",
      "  Batch    24  of    974.    Elapsed:  0:00:11, Remaining:  0:07:15.\n",
      "  Batch    48  of    974.    Elapsed:  0:00:22, Remaining:  0:07:04.\n",
      "  Batch    72  of    974.    Elapsed:  0:00:33, Remaining:  0:06:53.\n",
      "  Batch    96  of    974.    Elapsed:  0:00:44, Remaining:  0:06:42.\n",
      "  Batch   120  of    974.    Elapsed:  0:00:55, Remaining:  0:06:31.\n",
      "  Batch   144  of    974.    Elapsed:  0:01:06, Remaining:  0:06:20.\n",
      "  Batch   168  of    974.    Elapsed:  0:01:17, Remaining:  0:06:09.\n",
      "  Batch   192  of    974.    Elapsed:  0:01:28, Remaining:  0:05:58.\n",
      "  Batch   216  of    974.    Elapsed:  0:01:39, Remaining:  0:05:47.\n",
      "  Batch   240  of    974.    Elapsed:  0:01:50, Remaining:  0:05:36.\n",
      "  Batch   264  of    974.    Elapsed:  0:02:01, Remaining:  0:05:25.\n",
      "  Batch   288  of    974.    Elapsed:  0:02:12, Remaining:  0:05:14.\n",
      "  Batch   312  of    974.    Elapsed:  0:02:23, Remaining:  0:05:03.\n",
      "  Batch   336  of    974.    Elapsed:  0:02:34, Remaining:  0:04:52.\n",
      "  Batch   360  of    974.    Elapsed:  0:02:45, Remaining:  0:04:41.\n",
      "  Batch   384  of    974.    Elapsed:  0:02:56, Remaining:  0:04:30.\n",
      "  Batch   408  of    974.    Elapsed:  0:03:07, Remaining:  0:04:19.\n",
      "  Batch   432  of    974.    Elapsed:  0:03:18, Remaining:  0:04:08.\n",
      "  Batch   456  of    974.    Elapsed:  0:03:29, Remaining:  0:03:57.\n",
      "  Batch   480  of    974.    Elapsed:  0:03:40, Remaining:  0:03:46.\n",
      "  Batch   504  of    974.    Elapsed:  0:03:50, Remaining:  0:03:35.\n",
      "  Batch   528  of    974.    Elapsed:  0:04:01, Remaining:  0:03:24.\n",
      "  Batch   552  of    974.    Elapsed:  0:04:12, Remaining:  0:03:13.\n",
      "  Batch   576  of    974.    Elapsed:  0:04:23, Remaining:  0:03:02.\n",
      "  Batch   600  of    974.    Elapsed:  0:04:34, Remaining:  0:02:51.\n",
      "  Batch   624  of    974.    Elapsed:  0:04:45, Remaining:  0:02:40.\n",
      "  Batch   648  of    974.    Elapsed:  0:04:55, Remaining:  0:02:16.\n",
      "  Batch   672  of    974.    Elapsed:  0:05:04, Remaining:  0:02:06.\n",
      "  Batch   696  of    974.    Elapsed:  0:05:15, Remaining:  0:01:56.\n",
      "  Batch   720  of    974.    Elapsed:  0:05:25, Remaining:  0:01:46.\n",
      "  Batch   744  of    974.    Elapsed:  0:05:35, Remaining:  0:01:36.\n",
      "  Batch   768  of    974.    Elapsed:  0:05:46, Remaining:  0:01:26.\n",
      "  Batch   792  of    974.    Elapsed:  0:05:56, Remaining:  0:01:23.\n",
      "  Batch   816  of    974.    Elapsed:  0:06:07, Remaining:  0:01:12.\n",
      "  Batch   840  of    974.    Elapsed:  0:06:18, Remaining:  0:01:01.\n",
      "  Batch   864  of    974.    Elapsed:  0:06:29, Remaining:  0:00:50.\n",
      "  Batch   888  of    974.    Elapsed:  0:06:39, Remaining:  0:00:36.\n",
      "  Batch   912  of    974.    Elapsed:  0:06:48, Remaining:  0:00:26.\n",
      "  Batch   936  of    974.    Elapsed:  0:06:58, Remaining:  0:00:16.\n",
      "  Batch   960  of    974.    Elapsed:  0:07:07, Remaining:  0:00:05.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stats = train_bern_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    #train_dataloader,\n",
    "    #test_dataloader,\n",
    "    #batch_schema,\n",
    "    device,\n",
    "    loss_function,\n",
    "    id2label,\n",
    "    train_dataloader=train_dataloader if not REBATCH else None,\n",
    "    test_dataloader=test_dataloader if not REBATCH else None,\n",
    "    create_train_dataloader=train_loader_call if REBATCH else None,\n",
    "    create_test_dataloader=test_loader_call if REBATCH else None,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    print_status=True,\n",
    "    is_hf_model=IS_HF_MODEL,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    only_save_core=False,\n",
    "    epoch_i=EPOCH_I,\n",
    "    is_encoder_decoder_model=IS_ENCODER_DECODER_MODEL,\n",
    "    causal_lm=CAUSAL_LM,\n",
    "\n",
    "    #forward_args=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"],\n",
    "\n",
    "    masked_lm_task=True,\n",
    "    electra_task=False,\n",
    "    mlm_decode_n=0,\n",
    "    #mlm_decode_n=.0075,\n",
    "    #mlm_decode_n=.1,\n",
    "    mlm_decode_max_chars=200,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    dump_coin_regions=False,\n",
    "    generic_output_class=True,\n",
    "    #coin_region_lambda=lambda model: model.coin.core.regions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m print_stat_tuples(\u001b[43mstats\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stats' is not defined"
     ]
    }
   ],
   "source": [
    "print_stat_tuples(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
