{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/pushshift/cc-phoebe/rotary_embeddings.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/mnt/pushshift/cc-phoebe/rotary_embeddings.py:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from datasets import DatasetDict, load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    BertTokenizer, \n",
    "    RobertaTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "from load_set import load_set, load_moses_set\n",
    "from epoch_stats import EpochStats, print_stat_tuples\n",
    "import model_training\n",
    "from model_training import (\n",
    "    BatchBuffer, \n",
    "    mask_tokens, \n",
    "    train_bern_model, \n",
    "    preprocess_for_maskedlm, \n",
    "    preprocess_for_causallm, \n",
    "    preprocess_for_monologe, \n",
    "    preprocess_for_sparselm, \n",
    "    preprocess_for_binary_sparselm,\n",
    "    num_parameters, \n",
    "    num_trainable_parameters,\n",
    "    preprocess_for_translation,\n",
    "    preprocess_for_key_masking\n",
    ")\n",
    "\n",
    "import coin_i2C_modeling as ci2C\n",
    "import coin_i2D_modeling as ci2D\n",
    "import coin_i3A_modeling as ci3A\n",
    "import coin_i3C_modeling as ci3C\n",
    "import coin_i4_modeling as ci4\n",
    "import transformer_modeling as tm\n",
    "import coin_rnn_modeling as ciR\n",
    "import perceiver_modeling as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBATCH = False\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "#BASE_PATH = \"benchmark_models/Llama_config_B3/\"\n",
    "#BASE_PATH = \"benchmark_models/ppl_play/10k_play/\"\n",
    "#BASE_PATH = \"tmp_models/ci3c_mlm_wikitext/\"\n",
    "BASE_PATH = None\n",
    "\n",
    "#BASE_PATH = \"pretrained_models/COIN-i3C_inbio_mask_no-decoder\"\n",
    "#BASE_PATH = \"tmp_models/COIN-i3C_oasst1_25k_mlm_1x2_1dec-none_no-revert_parallel_mlm-head_B6_no-shuffle_A-T_multi-query-2/\"\n",
    "\n",
    "if REBATCH:\n",
    "    if BASE_PATH[-1] == \"/\":\n",
    "        BASE_PATH = BASE_PATH[:-1]\n",
    "    BASE_PATH += \"_rebatch/\"\n",
    "DATASET_JSON_PATH = f\"{BASE_PATH}/dataset/\"\n",
    "CHECKPOINT_PATH = f\"{BASE_PATH}/model/\" if BASE_PATH is not None else None\n",
    "WORDPIECE_TOKENIZER_DIR = f\"{BASE_PATH}/wordpiece_tokenizer/\"\n",
    "BPE_TOKENIZER_DIR = f\"{BASE_PATH}/bpe_tokenizer/\"\n",
    "SENTENCE_PIECE_TOKENIZER_DIR = f\"{BASE_PATH}/sentence_piece_tokenizer/\"\n",
    "USE_CUSTOM_DATALOADER = False\n",
    "LEARNING_RATE = 3e-4\n",
    "EPS = 1e-8\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_TRAIN_DATA = True\n",
    "SHUFFLE_TEST_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAUSAL_LM = False\n",
    "ENCODE_CAUSAL_LM = False\n",
    "GROUP_TEXTS = True\n",
    "SPARSIFY = True\n",
    "MASK_TOKEN = None\n",
    "PAD_TOKEN = None\n",
    "PREFIX = None#\"Translate the following text:\"\n",
    "#PREFIX = \"Replace all of the mask-tokens: \"\n",
    "#PREFIX = \"This sentence is completely obsolete \"\n",
    "SWITCH_II_DECODER_II = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out dir: None'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"out dir: {BASE_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOCAB_SIZE = 30_522\n",
    "#VOCAB_SIZE = 32_000\n",
    "VOCAB_SIZE = 50257 #+ 2\n",
    "MAX_POSITION_EMBEDDINGS = 512\n",
    "IS_HF_MODEL = False\n",
    "IS_ENCODER_DECODER_MODEL = False\n",
    "EPOCH_I = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCCA_SAVE_CONFIG = ci3C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=True,\n",
    "    num_decay_parts=1,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    hidden_pos_offset=False,\n",
    "    rope_dim=16,\n",
    "    num_query_heads=1,\n",
    "\n",
    "    decoder_output=\"adaptive\",\n",
    "    revert_decoder=True,\n",
    "    decoder_schema=[1] * 4,\n",
    "    cross_encoder_schema=[1] * 4,\n",
    "    block_io_schema=None,#[[1024, 1024*4, 1024], [1024, 1024*2, 1024]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_CONFIG = tm.TransformerConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_layers=1,\n",
    "    rms_norm_eps=1e-6,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    model = tm.TransformerForCausalLM(\n",
    "        config=TM_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = tm.TransformerForMaskedLM(\n",
    "        config=TM_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ciR.COINForCausalLM(\n",
    "        config=ciR.COINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            hidden_size=1024,\n",
    "            num_layers=4,\n",
    "            rms_norm_eps=1e-6\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCEIVER_CONFIG = pm.PerceiverConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_layers=1,\n",
    "    rms_norm_eps=1e-6,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    num_heads=1,\n",
    "    num_kv_heads=1,\n",
    "    #conv_kernel_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = pm.PerceiverForCausalLM(\n",
    "        config=PERCEIVER_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = pm.PerceiverForMaskedLM(\n",
    "        config=PERCEIVER_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci4.COINForCausalLM(\n",
    "        config=ci4.COINConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            hidden_size=1024,\n",
    "            intermediate_size=1536,\n",
    "            forward_method=\"llama\",\n",
    "            num_layers=4,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            rms_norm_eps=1e-6,\n",
    "            hidden_dropout_prob=0.0,\n",
    "            training_chunk_size=None,\n",
    "            inference_chunk_size=None,\n",
    "            reset_hidden_states=True,\n",
    "            apply_decay_mask=True,\n",
    "            apply_attention_mask=False,\n",
    "            apply_group_mask=False,\n",
    "            gamma=(1 - 1e-6),\n",
    "            num_heads=1,\n",
    "            num_key_value_heads=None,\n",
    "            conv_kernel_size=8,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    IS_HF_MODEL = True\n",
    "    model = transformers.LlamaForCausalLM(\n",
    "        config=transformers.LlamaConfig(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            hidden_size=1024,\n",
    "            intermediate_size=1536,\n",
    "            num_hidden_layers=4,\n",
    "            num_attention_heads=1,\n",
    "            num_key_value_heads=1,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "            rms_norm_eps=1e-6,\n",
    "            #hidden_dropout_prob=0.0,\n",
    "            #use_cache=False,\n",
    "            #_attn_implementation=\"eager\",\n",
    "            #mlp_bias=True,\n",
    "            #attention_bias=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI3C_CONFIG = ci3C.COINConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "    hidden_size=1024,\n",
    "    forward_method=\"parallel\",\n",
    "    apply_decay=True,\n",
    "    num_decay_parts=1,\n",
    "    hidden_retention_act=\"relu\",\n",
    "    hidden_pos_offset=False,\n",
    "    rope_dim=16,\n",
    "    num_query_heads=1,\n",
    "\n",
    "    decoder_output=\"none\",\n",
    "    revert_decoder=False,\n",
    "    decoder_schema=[0] * 2,\n",
    "    cross_encoder_schema=[0] * 2,\n",
    "    experts_schema=None,#[2, 2],\n",
    "    block_io_schema=None,#[[1024, 1024*4, 1024]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3C.COINForConditionalGeneration(\n",
    "        CI3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3C.COINForMaskedLM(\n",
    "        CI3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model = ci3C.COINForCausalLM(\n",
    "        CI3C_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    CHECKPOINT = \"tmp_models/ci3c_mlm_wikitext/\"\n",
    "    I = 5\n",
    "    tokenizer = BertTokenizer.from_pretrained(f\"{CHECKPOINT}/wordpiece_tokenizer/\")\n",
    "    model = ci3C.COINForCausalLM.from_pretrained(f\"{CHECKPOINT}/model/epoch_{I}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113,949,184\n",
      "113,948,672\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,}\\n{:,}\".format(num_parameters(model), num_trainable_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'None/wordpiece_tokenizer/'\n",
      "[Errno 17] File exists: 'None/bpe_tokenizer/'\n",
      "[Errno 17] File exists: 'None/sentence_piece_tokenizer/'\n",
      "[Errno 17] File exists: 'None/dataset/'\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if BASE_PATH is not None:\n",
    "    try:\n",
    "        os.mkdir(BASE_PATH)\n",
    "    except OSError as err:\n",
    "        print(err)\n",
    "if CHECKPOINT_PATH is not None:\n",
    "    try:\n",
    "        os.mkdir(CHECKPOINT_PATH)\n",
    "    except OSError as err:\n",
    "        print(err)\n",
    "try:\n",
    "    os.mkdir(WORDPIECE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(BPE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(SENTENCE_PIECE_TOKENIZER_DIR)\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    os.mkdir(DATASET_JSON_PATH)\n",
    "except OSError as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#dataset = DatasetDict({\n",
    "#    \"train\": load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train[0:10000]\"),\n",
    "#    \"test\":  load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"validation[:1500]\")\n",
    "#})\n",
    "\n",
    "#dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oasst1, aaabdon, mcca, translation_mcca\n",
    "DATASET = \"wikitext_ppl\"\n",
    "#DATASET = \"wikitext_mlm\"\n",
    "\n",
    "#HF_TRAIN_ROWS = 25_000\n",
    "#HF_TRAIN_ROWS = 10_000\n",
    "HF_TRAIN_ROWS = 25_000\n",
    "HF_TRAIN_FROM = 0#10_000\n",
    "\n",
    "#HF_TEST_ROWS = 1_500\n",
    "#HF_TEST_ROWS = 1000\n",
    "HF_TEST_ROWS = -1\n",
    "HF_TEST_FROM = 0\n",
    "\n",
    "CUSTOM_BASE_DS_PATH = \"../datasets/big_AAABDON_Nmax_st200_s0_a10_tvsplit.1_no_norm/\"\n",
    "CUSTOM_DS_TO_FILE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4358\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1805708\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "if DATASET in (\"wikitext_ppl\", \"wikitext_mlm\"):\n",
    "    train_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "    test_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"test\")\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"slim_pajama\":\n",
    "    CACHE_DIR = \"/mnt/pushshift/slim_lajama_627B/\"\n",
    "    train_dataset = load_dataset(\"cerebras/SlimPajama-627B\", split=f\"train[{HF_TRAIN_FROM}:{HF_TRAIN_ROWS}]\", cache_dir=CACHE_DIR)\n",
    "    test_dataset = load_dataset(\"cerebras/SlimPajama-627B\", split=f\"validation[{HF_TEST_FROM}:{HF_TEST_ROWS}]\", cache_dir=CACHE_DIR)\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"inbio_mask\":\n",
    "    inbio = load_set([\"INBIO.csv\"], unused_fields=(\"Synonyms,Obsolete,CUI,Semantic Types,Parents,achieves,adjacent to,affects,allocates,capable of,characteristic for,completed invasion phase,contained in,contains,contributes to,contributor,created by,decreases,decreases effort in,derives from,derives into,determines,don't use concept,editor note,enabled by,ends,ends during,ends with,enhance,facilitate,has alien range,has amount of closely related species,has amount of species,has area,has component,has decreased effort level by,has distribution,has growth,has habitat,has increased effort level by,has increased levels of,has index,has input,has invasion success likelihood,has level of,has measurement,has measurement unit label,has measurement value,has mortality,has natality,has native range,has number of individuals,has output,has part,has part structure that is capable of,has participant,has propagule pressure,has quality,has range,has recruitment,has role,has spatial occupant at some time,has specific name,has status,has value,http://data.bioontology.org/metadata/obo/part_of,http://data.bioontology.org/metadata/prefixIRI,http://data.bioontology.org/metadata/treeView,http://purl.obolibrary.org/obo/IAO_0000111,http://purl.obolibrary.org/obo/IAO_0000112,http://purl.obolibrary.org/obo/IAO_0000114,http://purl.obolibrary.org/obo/IAO_0000115,http://purl.obolibrary.org/obo/IAO_0000118,http://purl.obolibrary.org/obo/IAO_0000119,http://purl.obolibrary.org/obo/IAO_0000232,http://purl.obolibrary.org/obo/IAO_0000412,http://purl.obolibrary.org/obo/ncbitaxon#has_rank,http://purl.obolibrary.org/obo/NCIT_A8,http://purl.obolibrary.org/obo/NCIT_NHC0,http://purl.obolibrary.org/obo/NCIT_P106,http://purl.obolibrary.org/obo/NCIT_P107,http://purl.obolibrary.org/obo/NCIT_P108,http://purl.obolibrary.org/obo/NCIT_P207,http://purl.obolibrary.org/obo/NCIT_P322,http://purl.obolibrary.org/obo/NCIT_P325,http://purl.obolibrary.org/obo/NCIT_P366,http://purl.obolibrary.org/obo/OBI_0001886,http://purl.obolibrary.org/obo/RO_0001900,http://purl.org/dc/elements/1.1/source,http://purl.org/dc/terms/creator,http://www.geneontology.org/formats/oboInOwl#creation_date,http://www.geneontology.org/formats/oboInOwl#hasAlternativeId,http://www.geneontology.org/formats/oboInOwl#hasBroadSynonym,http://www.geneontology.org/formats/oboInOwl#hasDbXref,http://www.geneontology.org/formats/oboInOwl#hasExactSynonym,http://www.geneontology.org/formats/oboInOwl#hasNarrowSynonym,http://www.geneontology.org/formats/oboInOwl#hasOBONamespace,http://www.geneontology.org/formats/oboInOwl#hasRelatedSynonym,http://www.geneontology.org/formats/oboInOwl#hasSynonymType,http://www.geneontology.org/formats/oboInOwl#id,http://www.geneontology.org/formats/oboInOwl#inSubset,http://www.w3.org/2000/01/rdf-schema#comment,http://www.w3.org/2000/01/rdf-schema#label,http://www.w3.org/2002/07/owl#deprecated,http://www.w3.org/2004/02/skos/core#altLabel,http://www.w3.org/2004/02/skos/core#definition,http://www.w3.org/2004/02/skos/core#notation,https://w3id.org/inbio#_000130,https://w3id.org/inbio#_000132,increases,increases effort in,interacts with,is absent,is affected by,is against,is aggregate of,is alien range to,is characteristic of,is characterized by,is closely related to,is enemy of,is enhanced by,is growth of,is habitat of,is in invasion phase,is mortality of,is natality of,is native range to,is part of,is prey of,is range of,is recruitment of,is similar to,is status of,license,license,license,license,located in,location of,occupies spatial region at some time,occurs in,output of,overlaps,part of,participates in,produced by,produces,quality of,role of,shows changes in species trait,spatially coextensive with,surrounded by,surrounds,title,TODO,license.1,license.2,license.3\".split(\",\")))\n",
    "    bio2def = dict(zip(inbio[\"Preferred Label\"], inbio[\"Definitions\"]))\n",
    "    mask_keys = inbio[\"Preferred Label\"]\n",
    "\n",
    "    DS_TRAIN_PATH = \"datasets/abstracts_all_labels_train.csv\"\n",
    "    DS_TEST_PATH = \"datasets/abstracts_all_labels_test.csv\"\n",
    "\n",
    "    train_dataset = load_set([DS_TRAIN_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"])\n",
    "    test_dataset = load_set([DS_TEST_PATH], unused_fields=[\"head\", \"body\", \"strlabels\"])\n",
    "\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"mcca\":\n",
    "    train_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-2][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "    test_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "        ]\n",
    "    })\n",
    "    tok_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "elif DATASET == \"translation_mcca\":\n",
    "    train_dataset = load_moses_set({\n",
    "        \"src\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-5][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-5][0-9]\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-5][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-5][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "    test_dataset = load_moses_set({\n",
    "        \"src\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x800[0-1]\",\n",
    "        ],\n",
    "        \"tgt\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x800[0-1]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x800[0-1]\",\n",
    "        ]\n",
    "    })\n",
    "    tok_dataset = load_moses_set({\n",
    "        \"text\": [\n",
    "            \"../datasets/multi_cc_aligned_en-de/en/x00[0-9][0-9]\",\n",
    "            \"../datasets/multi_cc_aligned_en-de/de/x00[0-9][0-9]\",\n",
    "        ]\n",
    "    })\n",
    "elif DATASET == \"aaabdon\":\n",
    "    train_ds = [f\"{CUSTOM_BASE_DS_PATH}/train/train_00[0-{CUSTOM_DS_TO_FILE}].csv\"]\n",
    "    test_ds = [f\"{CUSTOM_BASE_DS_PATH}/validation/validation_00[0-{CUSTOM_DS_TO_FILE}].csv\"]\n",
    "    train_dataset = load_set(train_ds)#.select(list(range(HF_TRAIN_ROWS)))\n",
    "    test_dataset = load_set(test_ds)#.select(list(range(HF_TEST_ROWS)))\n",
    "    tok_dataset = load_set([f\"{CUSTOM_BASE_DS_PATH}/train/train_*.csv\"])\n",
    "elif DATASET == \"oasst1\":\n",
    "    #train_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=f\"train[0:{HF_TRAIN_ROWS}]\")\n",
    "    #test_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=f\"validation[:{HF_TEST_ROWS}]\")\n",
    "    #tok_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")\n",
    "    \n",
    "    #train_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\", split=f\"train[0:{HF_TRAIN_ROWS}]\")  .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "    #test_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\", split=f\"test[0:{HF_TEST_ROWS}]\")     .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "    #tok_dataset = load_dataset(\"QingyiSi/Alpaca-CoT\")                                       .rename_column(\"instruction\", \"text\").rename_column(\"output\", \"target\")\n",
    "\n",
    "    train_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    test_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"validation\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    tok_dataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "    #train_dataset = load_from_disk(\"../datasets/oasst1/train\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TRAIN_ROWS)))\n",
    "    #test_dataset = load_from_disk(\"../datasets/oasst1/validation\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TEST_ROWS)))\n",
    "    #tok_dataset = load_from_disk(\"../datasets/oasst1/train\").filter(lambda e: e[\"lang\"] == \"en\")\n",
    "\n",
    "    #train_dataset = load_dataset(\"OpenAssistant/oasst2\", split=\"train\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TRAIN_ROWS)))\n",
    "    #test_dataset = load_dataset(\"OpenAssistant/oasst2\", split=\"validation\").filter(lambda e: e[\"lang\"] == \"en\").select(list(range(HF_TEST_ROWS)))\n",
    "\n",
    "if SHUFFLE_TRAIN_DATA:\n",
    "    print(\"shuffle\")\n",
    "    train_dataset = train_dataset.shuffle()\n",
    "if HF_TRAIN_ROWS > 0:\n",
    "    train_dataset =  train_dataset.select(list(range(HF_TRAIN_FROM, HF_TRAIN_ROWS)))\n",
    "if SHUFFLE_TEST_DATA:\n",
    "    print(\"shuffle\")\n",
    "    test_dataset = test_dataset.shuffle()\n",
    "if HF_TEST_ROWS > 0:\n",
    "    test_dataset = test_dataset.select(list(range(HF_TEST_FROM, HF_TEST_ROWS)))\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "print(tok_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.to_json(f\"{DATASET_JSON_PATH}/train.json\")\n",
    "#test_dataset.to_json(f\"{DATASET_JSON_PATH}/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "labels = [label for label in train_dataset.features.keys() if label not in [\"text\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer, BertWordPieceTokenizer, SentencePieceBPETokenizer, SentencePieceUnigramTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    #tok_dataset = load_dataset(\"wikitext\", name=\"wikitext-103-raw-v1\", split=\"train\")\n",
    "    #tok_dataset = load_dataset(\"glue\", name=\"sst2\", split=\"train\")\n",
    "    #tok_dataset = tok_dataset.rename_column(\"sentence\", \"text\")\n",
    "    \n",
    "    tokenizer = SentencePieceUnigramTokenizer()\n",
    "\n",
    "    tokenizer.train_from_iterator(\n",
    "        iterator=tok_dataset[\"text\"], \n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        #min_frequency=2,\n",
    "        show_progress=True,\n",
    "        #limit_alphabet=500,\n",
    "        special_tokens=[\n",
    "            \"<PAD>\", \n",
    "            \"<UNK>\", \n",
    "            \"<CLS>\", \n",
    "            \"<SEP>\", \n",
    "            \"<DOC>\",\n",
    "            \"<MASK>\"\n",
    "        ])\n",
    "\n",
    "    tokenizer = PreTrainedTokenizer(\n",
    "        tokenizer_object=tokenizer\n",
    "    )\n",
    "    tokenizer.save_model(SENTENCE_PIECE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(SENTENCE_PIECE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(tok_dataset), TRAIN_BATCH_SIZE):\n",
    "        yield tok_dataset[i : i + TRAIN_BATCH_SIZE][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"gpt2\", \n",
    "    #    vocab_size=VOCAB_SIZE - 2,\n",
    "    #    pad_token=\"<|pad|>\",\n",
    "    #    unk_token=\"<|unk|>\",\n",
    "    #    bos_token=\"<|doc|>\",\n",
    "    #    eos_token=\"<|udoc|>\",\n",
    "    )\n",
    "    #tokenizer.eos_token_id = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    old_tokenizer = transformers.AutoTokenizer.from_pretrained(\"AdithyaSK/LLama3Tokenizer\", token=ACCESS_TOKEN)\n",
    "    tokenizer = old_tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True,\n",
    "                                        strip_accents=True, lowercase=True)\n",
    "    #tokenizer = transformers.LlamaTokenizer()\n",
    "\n",
    "    tokenizer.train_from_iterator(iterator=tok_dataset[\"text\"], vocab_size=VOCAB_SIZE, min_frequency=2, special_tokens=[\n",
    "        \"[PAD]\", \n",
    "        \"[UNK]\", \n",
    "        \"[CLS]\", \n",
    "        \"[SEP]\", \n",
    "    #    \"[DOC]\",\n",
    "    #    \"[UDOC]\",\n",
    "        \"[MASK]\"\n",
    "    ])\n",
    "    tokenizer.save_model(WORDPIECE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    tokenizer = BertTokenizer.from_pretrained(WORDPIECE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if 0:\n",
    "    tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "    \n",
    "    tokenizer.train_from_iterator(iterator=tok_dataset[\"text\"], vocab_size=VOCAB_SIZE, min_frequency=2, length=MAX_POSITION_EMBEDDINGS, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<doc>\",\n",
    "        \"<mask>\",\n",
    "        \n",
    "        #\"<pad>\",\n",
    "        #\"<unk>\",\n",
    "        #\"<cls>\",\n",
    "        #\"<sep>\",\n",
    "        #\"<doc>\",\n",
    "        #\"<mask>\",\n",
    "    ])\n",
    "\n",
    "    # Save files to disk\n",
    "    tokenizer.save_model(BPE_TOKENIZER_DIR)\n",
    "    #assert False\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(BPE_TOKENIZER_DIR)\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(BPE_TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def encode_and_batch(dataset, tokenizer, max_position_embeddings, batch_size, shuffle=False):\n",
    "    if DATASET == \"inbio_mask\":\n",
    "        encoded = preprocess_for_key_masking(mask_keys, dataset, tokenizer, max_position_embeddings, remove_columns=dataset.column_names, to_mask=.15, chance_rand_token=.2, \n",
    "                                          group_texts=GROUP_TEXTS, pad_token=PAD_TOKEN, sparsify=SPARSIFY, prefix=PREFIX, switch_ii_decoder_ii=SWITCH_II_DECODER_II)\n",
    "    elif DATASET in (\"translation_mcca\"):\n",
    "        encoded = preprocess_for_translation(dataset, tokenizer, max_position_embeddings, source_lang=\"src\", target_lang=\"tgt\", prefix=PREFIX, num_proc=4, remove_columns=[\"src\", \"tgt\"], switch_ii_decoder_ii=SWITCH_II_DECODER_II)\n",
    "    elif DATASET in (\"wikitext_ppl\"):\n",
    "        encoded = preprocess_for_causallm(\n",
    "            dataset, \n",
    "            tokenizer, \n",
    "            block_size=MAX_POSITION_EMBEDDINGS, \n",
    "            remove_columns=dataset.column_names, \n",
    "            shift_right=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            doc_token_id=tokenizer.bos_token_id,\n",
    "            udoc_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    elif DATASET in (\"wikitext_mlm\"):\n",
    "        encoded = preprocess_for_maskedlm(\n",
    "            dataset, \n",
    "            tokenizer, \n",
    "            max_position_embeddings, \n",
    "            remove_columns=dataset.column_names, \n",
    "            to_mask=.15,\n",
    "            chance_rand_token=.2, \n",
    "            group_texts=GROUP_TEXTS, \n",
    "            mask_token=MASK_TOKEN, \n",
    "            pad_token=PAD_TOKEN, \n",
    "            sparsify=SPARSIFY, \n",
    "            prefix=PREFIX, \n",
    "            switch_ii_decoder_ii=False\n",
    "        )\n",
    "        \n",
    "    print(encoded)\n",
    "    batched = BatchBuffer(encoded, batch_size)\n",
    "    if shuffle:\n",
    "        batched.shuffle()\n",
    "    print(\"  finished\")\n",
    "    return batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Robert Boulter = \\n'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys:num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask']) \n",
      "dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   4%|▍         | 1000/25000 [00:00<00:06, 3806.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  20%|██        | 5000/25000 [00:00<00:01, 14705.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  36%|███▌      | 9000/25000 [00:00<00:00, 20178.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  52%|█████▏    | 13000/25000 [00:00<00:00, 23885.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])num keys:\n",
      " dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  68%|██████▊   | 17000/25000 [00:00<00:00, 26401.07 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  84%|████████▍ | 21000/25000 [00:00<00:00, 27891.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n",
      "num keys: dict_keys(['input_ids', 'group_mask', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 25000/25000 [00:01<00:00, 23079.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'group_mask', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
      "    num_rows: 3240\n",
      "})\n",
      "  finished\n",
      "Dataset({\n",
      "    features: ['input_ids', 'group_mask', 'attention_mask', 'decoder_input_ids', 'labels'],\n",
      "    num_rows: 557\n",
      "})\n",
      "  finished\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "train_loader_call = lambda: encode_and_batch(train_dataset, tokenizer, MAX_POSITION_EMBEDDINGS, TRAIN_BATCH_SIZE, True)\n",
    "test_loader_call = lambda: encode_and_batch(test_dataset, tokenizer, MAX_POSITION_EMBEDDINGS, TEST_BATCH_SIZE)\n",
    "\n",
    "if not REBATCH:\n",
    "    train_dataloader = train_loader_call()\n",
    "    test_dataloader = test_loader_call()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 796,\n",
       " 5199,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 796,\n",
       " 220,\n",
       " 198,\n",
       " 4,\n",
       " 4,\n",
       " 5199,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 318,\n",
       " 281,\n",
       " 3594,\n",
       " 2646,\n",
       " 837,\n",
       " 5581,\n",
       " 290,\n",
       " 21421,\n",
       " 8674,\n",
       " 764,\n",
       " 679,\n",
       " 550,\n",
       " 257,\n",
       " 8319,\n",
       " 2488,\n",
       " 12,\n",
       " 31,\n",
       " 20495,\n",
       " 2597,\n",
       " 319,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 383,\n",
       " 3941,\n",
       " 287,\n",
       " 4751,\n",
       " 764,\n",
       " 770,\n",
       " 373,\n",
       " 3940,\n",
       " 416,\n",
       " 257,\n",
       " 20495,\n",
       " 2597,\n",
       " 287,\n",
       " 262,\n",
       " 711,\n",
       " 2332,\n",
       " 684,\n",
       " 3194,\n",
       " 416,\n",
       " 11288,\n",
       " 37072,\n",
       " 837,\n",
       " 543,\n",
       " 373,\n",
       " 6157,\n",
       " 287,\n",
       " 5878,\n",
       " 379,\n",
       " 262,\n",
       " 8111,\n",
       " 3078,\n",
       " 15752,\n",
       " 764,\n",
       " 679,\n",
       " 550,\n",
       " 257,\n",
       " 8319,\n",
       " 2597,\n",
       " 287,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 8974,\n",
       " 1757,\n",
       " 1024,\n",
       " 276,\n",
       " 287,\n",
       " 6244,\n",
       " 764,\n",
       " 554,\n",
       " 5472,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 11406,\n",
       " 257,\n",
       " 2597,\n",
       " 355,\n",
       " 366,\n",
       " 13854,\n",
       " 366,\n",
       " 287,\n",
       " 262,\n",
       " 4471,\n",
       " 366,\n",
       " 29345,\n",
       " 705,\n",
       " 82,\n",
       " 8362,\n",
       " 366,\n",
       " 286,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 383,\n",
       " 5882,\n",
       " 31623,\n",
       " 2162,\n",
       " 339,\n",
       " 31636,\n",
       " 7848,\n",
       " 10544,\n",
       " 2940,\n",
       " 13535,\n",
       " 290,\n",
       " 20893,\n",
       " 12806,\n",
       " 72,\n",
       " 764,\n",
       " 679,\n",
       " 373,\n",
       " 3350,\n",
       " 287,\n",
       " 262,\n",
       " 5075,\n",
       " 21421,\n",
       " 32260,\n",
       " 286,\n",
       " 262,\n",
       " 14576,\n",
       " 39616,\n",
       " 711,\n",
       " 21673,\n",
       " 22384,\n",
       " 837,\n",
       " 543,\n",
       " 373,\n",
       " 6157,\n",
       " 379,\n",
       " 262,\n",
       " 25331,\n",
       " 15752,\n",
       " 287,\n",
       " 42125,\n",
       " 290,\n",
       " 262,\n",
       " 6065,\n",
       " 959,\n",
       " 24777,\n",
       " 19239,\n",
       " 287,\n",
       " 3576,\n",
       " 764,\n",
       " 679,\n",
       " 373,\n",
       " 7924,\n",
       " 416,\n",
       " 1757,\n",
       " 40928,\n",
       " 290,\n",
       " 31636,\n",
       " 7848,\n",
       " 3932,\n",
       " 854,\n",
       " 680,\n",
       " 707,\n",
       " 837,\n",
       " 24379,\n",
       " 1168,\n",
       " 7056,\n",
       " 837,\n",
       " 5850,\n",
       " 8758,\n",
       " 837,\n",
       " 28059,\n",
       " 13709,\n",
       " 411,\n",
       " 837,\n",
       " 35331,\n",
       " 36442,\n",
       " 290,\n",
       " 36401,\n",
       " 4789,\n",
       " 764,\n",
       " 220,\n",
       " 198,\n",
       " 4,\n",
       " 554,\n",
       " 4793,\n",
       " 837,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 31636,\n",
       " 7848,\n",
       " 854,\n",
       " 680,\n",
       " 707,\n",
       " 287,\n",
       " 262,\n",
       " 711,\n",
       " 47002,\n",
       " 3194,\n",
       " 416,\n",
       " 2940,\n",
       " 12552,\n",
       " 12639,\n",
       " 764,\n",
       " 679,\n",
       " 4120,\n",
       " 319,\n",
       " 257,\n",
       " 4793,\n",
       " 4471,\n",
       " 286,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 837,\n",
       " 28274,\n",
       " 837,\n",
       " 3940,\n",
       " 416,\n",
       " 257,\n",
       " 2597,\n",
       " 287,\n",
       " 262,\n",
       " 4343,\n",
       " 21421,\n",
       " 3227,\n",
       " 286,\n",
       " 1374,\n",
       " 284,\n",
       " 19739,\n",
       " 7924,\n",
       " 416,\n",
       " 22568,\n",
       " 494,\n",
       " 371,\n",
       " 49003,\n",
       " 764,\n",
       " 1374,\n",
       " 284,\n",
       " 19739,\n",
       " 373,\n",
       " 6157,\n",
       " 379,\n",
       " 5511,\n",
       " 15752,\n",
       " 287,\n",
       " 262,\n",
       " 3576,\n",
       " 48114,\n",
       " 286,\n",
       " 4345,\n",
       " 11056,\n",
       " 22947,\n",
       " 290,\n",
       " 28040,\n",
       " 2763,\n",
       " 764,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 31636,\n",
       " 287,\n",
       " 734,\n",
       " 7328,\n",
       " 287,\n",
       " 3648,\n",
       " 837,\n",
       " 47743,\n",
       " 3851,\n",
       " 13001,\n",
       " 416,\n",
       " 26479,\n",
       " 6342,\n",
       " 1004,\n",
       " 756,\n",
       " 72,\n",
       " 837,\n",
       " 290,\n",
       " 43823,\n",
       " 24265,\n",
       " 7924,\n",
       " 416,\n",
       " 440,\n",
       " 12810,\n",
       " 42603,\n",
       " 764,\n",
       " 554,\n",
       " 1737,\n",
       " 3648,\n",
       " 837,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 925,\n",
       " 257,\n",
       " 8319,\n",
       " 5585,\n",
       " 319,\n",
       " 257,\n",
       " 734,\n",
       " 2488,\n",
       " 12,\n",
       " 31,\n",
       " 636,\n",
       " 4471,\n",
       " 10389,\n",
       " 286,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 370,\n",
       " 868,\n",
       " 262,\n",
       " 5542,\n",
       " 837,\n",
       " 3940,\n",
       " 416,\n",
       " 281,\n",
       " 5585,\n",
       " 319,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 45998,\n",
       " 287,\n",
       " 3389,\n",
       " 3648,\n",
       " 764,\n",
       " 679,\n",
       " 550,\n",
       " 257,\n",
       " 24824,\n",
       " 2597,\n",
       " 287,\n",
       " 3478,\n",
       " 8640,\n",
       " 286,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 43508,\n",
       " 774,\n",
       " 287,\n",
       " 3050,\n",
       " 837,\n",
       " 355,\n",
       " 366,\n",
       " 39717,\n",
       " 261,\n",
       " 31942,\n",
       " 366,\n",
       " 764,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 31636,\n",
       " 287,\n",
       " 262,\n",
       " 2813,\n",
       " 2646,\n",
       " 12185,\n",
       " 30216,\n",
       " 7924,\n",
       " 416,\n",
       " 6342,\n",
       " 1004,\n",
       " 756,\n",
       " 72,\n",
       " 764,\n",
       " 220,\n",
       " 198,\n",
       " 4,\n",
       " 4,\n",
       " 796,\n",
       " 796,\n",
       " 32619,\n",
       " 796,\n",
       " 796,\n",
       " 220,\n",
       " 198,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 796,\n",
       " 796,\n",
       " 796,\n",
       " 4751,\n",
       " 784,\n",
       " 5075,\n",
       " 796,\n",
       " 796,\n",
       " 796,\n",
       " 220,\n",
       " 198,\n",
       " 4,\n",
       " 4,\n",
       " 554,\n",
       " 4751,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 550,\n",
       " 257,\n",
       " 8319,\n",
       " 2488,\n",
       " 12,\n",
       " 31,\n",
       " 20495,\n",
       " 2597,\n",
       " 319,\n",
       " 262,\n",
       " 5581,\n",
       " 2168,\n",
       " 383,\n",
       " 3941,\n",
       " 2162,\n",
       " 339,\n",
       " 19152,\n",
       " 366,\n",
       " 4746,\n",
       " 2547,\n",
       " 563,\n",
       " 366,\n",
       " 287,\n",
       " 262,\n",
       " 4471,\n",
       " 837,\n",
       " 366,\n",
       " 554,\n",
       " 19978,\n",
       " 22237,\n",
       " 366,\n",
       " 764,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 31636,\n",
       " 355,\n",
       " 366,\n",
       " 4746,\n",
       " 366,\n",
       " 287,\n",
       " 262,\n",
       " 711,\n",
       " 2332,\n",
       " 684,\n",
       " 3194,\n",
       " 416,\n",
       " 11288,\n",
       " 37072,\n",
       " 837,\n",
       " 543,\n",
       " 373,\n",
       " 6157,\n",
       " 287,\n",
       " 5878,\n",
       " 379,\n",
       " 262,\n",
       " 8111,\n",
       " 3078,\n",
       " 15752,\n",
       " 764,\n",
       " 317,\n",
       " 2423,\n",
       " 286,\n",
       " 347,\n",
       " 2852,\n",
       " 353,\n",
       " 705,\n",
       " 82,\n",
       " 2854,\n",
       " 287,\n",
       " 383,\n",
       " 13362,\n",
       " 319,\n",
       " 3502,\n",
       " 3417,\n",
       " 683,\n",
       " 355,\n",
       " 366,\n",
       " 33437,\n",
       " 39579,\n",
       " 366,\n",
       " 287,\n",
       " 262,\n",
       " 2597,\n",
       " 837,\n",
       " 290,\n",
       " 339,\n",
       " 2722,\n",
       " 4688,\n",
       " 8088,\n",
       " 287,\n",
       " 383,\n",
       " 18277,\n",
       " 837,\n",
       " 290,\n",
       " 31867,\n",
       " 8997,\n",
       " 764]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.ds[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#encoded_train_dataset = preprocess_for_maskedlm(dataset, tokenizer, MAX_POSITION_EMBEDDINGS, remove_columns=train_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#train_dataloader = BatchBuffer(encoded_dataset[\"train\"], BATCH_SIZE).shuffle()\n",
    "#test_dataloader = BatchBuffer(encoded_dataset[\"test\"], BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "#batch_schema = list(encoded_dataset[\"train\"].features.keys())\n",
    "#batch_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def count_item(inp, item):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for n in inp:\n",
    "        for r in n:\n",
    "            i = r\n",
    "            if not i < 4:\n",
    "                total += 1\n",
    "            if i == item:\n",
    "                count += 1\n",
    "            #if i != 0 and i != item:\n",
    "            #    print(i)\n",
    "    return f\"{count} / {total} ; {count/total}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked tokens [input_ids]: 0 / 1658880 ; 0.0\n",
      "masked tokens [labels]: 0 / 285184 ; 0.0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"masked tokens [input_ids]:\", count_item(train_dataloader.ds[\"input_ids\"], tokenizer.mask_token_id))\n",
    "print(\"masked tokens [labels]:\", count_item(test_dataloader.ds[\"labels\"], tokenizer.mask_token_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPS)\n",
    "\n",
    "total_steps = len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS\n",
    "warmup_steps = math.ceil(total_steps * 0.05)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warmup_steps,\n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'group_mask', 'attention_mask', 'decoder_input_ids', 'labels']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "train_dataloader.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerForCausalLM(\n",
       "  (model): TransformerModel(\n",
       "    (embeddings): Embedding(50257, 1024, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): SkipGateLayer(\n",
       "        (norm): RMSNorm()\n",
       "        (in_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (att): Attention(\n",
       "          (Wb_Q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (Wb_K): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (Wb_V): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (rope): RotaryEmbedding()\n",
       "        )\n",
       "        (post_norm): RMSNorm()\n",
       "        (post_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (glu): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=1536, bias=True)\n",
       "          (up_proj): Linear(in_features=1024, out_features=1536, bias=True)\n",
       "          (down_proj): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=5_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 25 ========\n",
      "\n",
      "Generated batch schema as ['input_ids', 'group_mask', 'attention_mask', 'decoder_input_ids', 'labels']\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass-multioutput and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_bern_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#train_dataloader,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#test_dataloader,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#batch_schema,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mREBATCH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mREBATCH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_train_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_call\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mREBATCH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_test_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader_call\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mREBATCH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVOCAB_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_hf_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIS_HF_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEST_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_save_core\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH_I\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIS_ENCODER_DECODER_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_lm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCAUSAL_LM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#forward_args=[\"input_ids\", \"decoder_input_ids\", \"group_mask\", \"labels\"],\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_lm_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43melectra_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlm_decode_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#mlm_decode_n=.0075,\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#mlm_decode_n=.1,\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlm_decode_max_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdump_coin_regions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneric_output_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#coin_region_lambda=lambda model: model.coin.core.regions\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_autoregressively\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/pushshift/cc-phoebe/model_training.py:1574\u001b[0m, in \u001b[0;36mtrain_bern_model\u001b[0;34m(model, optimizer, scheduler, epochs, device, loss_function, id2label, batch_schema, train_dataloader, test_dataloader, create_train_dataloader, create_test_dataloader, masked_lm_task, vocab_size, print_status, is_hf_model, checkpoint_path, train_batch_size, test_batch_size, only_save_core, one_label_only, mixed_lm_task, mixed_lm_loss_function, epoch_i, is_encoder_decoder_model, calc_metrics, electra_task, empty_cache, retain_graph, batch_hack_train, batch_hack_test, imitation_model, add_layers_on_stagnation, num_layers_to_add, add_layers_threshold, plot_k_topics, causal_lm, generic_output_class, forward_args, mlm_decode_n, tokenizer, mlm_decode_max_chars, mlm_decode_max_batch_output, dump_coin_regions, coin_region_lambda, check_run, chomsky_task, accuracy_mask, backprop_during_testing, per_class_f1, evaluate_autoregressively)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     mlm_use \u001b[38;5;241m=\u001b[39m (epoch_i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1572\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_lm_use(mlm_use)\n\u001b[0;32m-> 1574\u001b[0m c_train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_set_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_lm_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_hf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_label_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlm_use\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmixed_lm_task\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixed_lm_loss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalc_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m    \u001b[49m\u001b[43melectra_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mempty_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_hack_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimitation_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_lm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneric_output_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlm_decode_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlm_decode_max_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlm_decode_max_batch_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchomsky_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccuracy_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_class_f1\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1610\u001b[0m train_stats\u001b[38;5;241m.\u001b[39mappend(c_train_stats)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(test_dataloader, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/mnt/pushshift/cc-phoebe/model_training.py:1280\u001b[0m, in \u001b[0;36mtrain_set_epoch\u001b[0;34m(model, optimizer, scheduler, train_dataloader, batch_schema, device, loss_function, id2label, masked_lm_task, vocab_size, print_status, is_hf_model, batch_size, one_label_only, mixed_lm_task, mixed_lm_loss_function, is_encoder_decoder_model, calc_metrics, electra_task, empty_cache, retain_graph, batch_hack, imitation_model, causal_lm, generic_output_class, forward_args, mlm_decode_n, tokenizer, mlm_decode_max_chars, mlm_decode_max_batch_output, check_run, chomsky_task, accuracy_mask, per_class_f1)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     c_train_stats\u001b[38;5;241m.\u001b[39madd_score(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maux_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, aux_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m masked_lm_task \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m electra_task \u001b[38;5;129;01mand\u001b[39;00m calc_metrics:\n\u001b[0;32m-> 1280\u001b[0m     \u001b[43mc_train_stats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_label_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_label_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1281\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39mretain_graph)\n\u001b[1;32m   1283\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/mnt/pushshift/cc-phoebe/epoch_stats.py:116\u001b[0m, in \u001b[0;36mEpochStats.flat_metrics\u001b[0;34m(self, preds, labels, calc_scores_by_label, zd_val, one_label_only)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_score(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, acc)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_score(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_score(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhamming\u001b[39m\u001b[38;5;124m\"\u001b[39m, hamming_score(labels, y_pred))\n",
      "File \u001b[0;32m/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/mnt/pushshift/cc-phoebe/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:94\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     91\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     96\u001b[0m             type_true, type_pred\n\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    101\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and multiclass targets"
     ]
    }
   ],
   "source": [
    "stats = train_bern_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    EPOCHS,\n",
    "    #train_dataloader,\n",
    "    #test_dataloader,\n",
    "    #batch_schema,\n",
    "    device,\n",
    "    loss_function,\n",
    "    id2label,\n",
    "    train_dataloader=train_dataloader if not REBATCH else None,\n",
    "    test_dataloader=test_dataloader if not REBATCH else None,\n",
    "    create_train_dataloader=train_loader_call if REBATCH else None,\n",
    "    create_test_dataloader=test_loader_call if REBATCH else None,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    print_status=True,\n",
    "    is_hf_model=IS_HF_MODEL,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    only_save_core=False,\n",
    "    epoch_i=EPOCH_I,\n",
    "    is_encoder_decoder_model=IS_ENCODER_DECODER_MODEL,\n",
    "    causal_lm=CAUSAL_LM,\n",
    "\n",
    "    forward_args=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    #forward_args=[\"input_ids\", \"decoder_input_ids\", \"group_mask\", \"labels\"],\n",
    "\n",
    "    masked_lm_task=True,\n",
    "    electra_task=False,\n",
    "    mlm_decode_n=0,\n",
    "    #mlm_decode_n=.0075,\n",
    "    #mlm_decode_n=.1,\n",
    "    mlm_decode_max_chars=200,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    dump_coin_regions=False,\n",
    "    generic_output_class=True,\n",
    "    #coin_region_lambda=lambda model: model.coin.core.regions\n",
    "\n",
    "    evaluate_autoregressively=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stat_tuples(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
